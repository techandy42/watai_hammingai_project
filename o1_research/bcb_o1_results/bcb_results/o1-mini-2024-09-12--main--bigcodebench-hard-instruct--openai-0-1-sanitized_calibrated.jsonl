{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    Downloads all files from a specific directory on an FTP server using wget.\n\n    Args:\n        ftp_server (str): The FTP server address. Default is 'ftp.dlptest.com'.\n        ftp_user (str): The FTP server username. Default is 'dlpuser'.\n        ftp_password (str): The FTP server password. Default is 'rNrKYTX9g7z3RgJRmxWuGHbeu'.\n        ftp_dir (str): The directory path on the FTP server from which files need to be downloaded. Default is '/ftp/test'.\n\n    Returns:\n        List[str]: A list of filenames that were attempted to be downloaded from the FTP server.\n\n    Raises:\n        Exception: If there is a failure in connecting, logging in, or changing directory on the FTP server.\n    \"\"\"\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server, timeout=10)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    try:\n        # Login to the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    try:\n        # List all files in the directory\n        filenames = ftp.nlst()\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    ftp.quit()\n    \n    downloaded_files = []\n    for filename in filenames:\n        # Construct the FTP URL with credentials\n        url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\"\n        \n        try:\n            # Use wget to download the file\n            subprocess.run(['wget', url], check=True)\n            downloaded_files.append(filename)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download {filename}: {str(e)}\")\n    \n    return downloaded_files", "raw_solution": "```python\nimport subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    Downloads all files from a specific directory on an FTP server using wget.\n\n    Args:\n        ftp_server (str): The FTP server address. Default is 'ftp.dlptest.com'.\n        ftp_user (str): The FTP server username. Default is 'dlpuser'.\n        ftp_password (str): The FTP server password. Default is 'rNrKYTX9g7z3RgJRmxWuGHbeu'.\n        ftp_dir (str): The directory path on the FTP server from which files need to be downloaded. Default is '/ftp/test'.\n\n    Returns:\n        List[str]: A list of filenames that were attempted to be downloaded from the FTP server.\n\n    Raises:\n        Exception: If there is a failure in connecting, logging in, or changing directory on the FTP server.\n    \"\"\"\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server, timeout=10)\n    except Exception as e:\n        raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n    \n    try:\n        # Login to the FTP server\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n    \n    try:\n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    try:\n        # List all files in the directory\n        filenames = ftp.nlst()\n    except Exception as e:\n        ftp.quit()\n        raise Exception(f\"Failed to list files in directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    ftp.quit()\n    \n    downloaded_files = []\n    for filename in filenames:\n        # Construct the FTP URL with credentials\n        url = f\"ftp://{ftp_user}:{ftp_password}@{ftp_server}{ftp_dir}/{filename}\"\n        \n        try:\n            # Use wget to download the file\n            subprocess.run(['wget', url], check=True)\n            downloaded_files.append(filename)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to download {filename}: {str(e)}\")\n    \n    return downloaded_files\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        files = task_func()\n        print(\"Downloaded files:\", files)\n    except Exception as error:\n        print(error)\n```"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Executes a list of shell commands from a CSV file and saves their outputs.\n\n    Parameters:\n        commands_file_path (str): Path to the CSV file containing shell commands.\n        output_dir_path (str): Path to the directory where output files will be saved.\n\n    Returns:\n        list of str: List of paths to the output files created.\n\n    Raises:\n        FileNotFoundError: If the commands_file_path does not exist.\n    \"\"\"\n    if not os.path.isfile(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    # Ensure the output directory exists\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    output_files = []\n\n    with open(commands_file_path, newline='', encoding='utf-8') as csvfile:\n        reader = csv.reader(csvfile)\n        for index, row in enumerate(reader, start=1):\n            if not row:\n                # Skip empty rows\n                continue\n            command = row[0]\n            output_file_name = f\"command_{index}_output.txt\"\n            output_file_path = os.path.join(output_dir_path, output_file_name)\n            output_files.append(output_file_path)\n\n            try:\n                # Execute the command\n                result = subprocess.run(\n                    command,\n                    shell=True,\n                    capture_output=True,\n                    text=True\n                )\n\n                with open(output_file_path, 'w', encoding='utf-8') as outfile:\n                    if result.returncode == 0:\n                        outfile.write(result.stdout)\n                    else:\n                        error_message = (\n                            f\"Error executing command: {command}\\n\"\n                            f\"Exit Code: {result.returncode}\\n\"\n                            f\"Error Message:\\n{result.stderr}\"\n                        )\n                        outfile.write(error_message)\n            except Exception as e:\n                with open(output_file_path, 'w', encoding='utf-8') as outfile:\n                    outfile.write(f\"An unexpected error occurred while executing the command.\\nError: {str(e)}\")\n\n    return output_files", "raw_solution": "```python\nimport subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Executes a list of shell commands from a CSV file and saves their outputs.\n\n    Parameters:\n        commands_file_path (str): Path to the CSV file containing shell commands.\n        output_dir_path (str): Path to the directory where output files will be saved.\n\n    Returns:\n        list of str: List of paths to the output files created.\n\n    Raises:\n        FileNotFoundError: If the commands_file_path does not exist.\n    \"\"\"\n    if not os.path.isfile(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n\n    # Ensure the output directory exists\n    os.makedirs(output_dir_path, exist_ok=True)\n\n    output_files = []\n\n    with open(commands_file_path, newline='', encoding='utf-8') as csvfile:\n        reader = csv.reader(csvfile)\n        for index, row in enumerate(reader, start=1):\n            if not row:\n                # Skip empty rows\n                continue\n            command = row[0]\n            output_file_name = f\"command_{index}_output.txt\"\n            output_file_path = os.path.join(output_dir_path, output_file_name)\n            output_files.append(output_file_path)\n\n            try:\n                # Execute the command\n                result = subprocess.run(\n                    command,\n                    shell=True,\n                    capture_output=True,\n                    text=True\n                )\n\n                with open(output_file_path, 'w', encoding='utf-8') as outfile:\n                    if result.returncode == 0:\n                        outfile.write(result.stdout)\n                    else:\n                        error_message = (\n                            f\"Error executing command: {command}\\n\"\n                            f\"Exit Code: {result.returncode}\\n\"\n                            f\"Error Message:\\n{result.stderr}\"\n                        )\n                        outfile.write(error_message)\n            except Exception as e:\n                with open(output_file_path, 'w', encoding='utf-8') as outfile:\n                    outfile.write(f\"An unexpected error occurred while executing the command.\\nError: {str(e)}\")\n\n    return output_files\n```"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    Checks if a process is running. If not, starts it.\n    If it is running, restarts it.\n\n    Args:\n        process_name (str): The name of the process to check and manage.\n\n    Returns:\n        str: A message indicating the action taken.\n    \"\"\"\n    # Flag to check if the process is running\n    process_running = False\n    # List to hold all matching processes\n    matching_procs = []\n\n    # Iterate over all running processes\n    for proc in psutil.process_iter(['name']):\n        try:\n            if proc.info['name'] == process_name:\n                process_running = True\n                matching_procs.append(proc)\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            pass  # Ignore processes that no longer exist or can't be accessed\n\n    if not process_running:\n        try:\n            # Start the process\n            subprocess.Popen([process_name])\n            return f\"Process not found. Starting {process_name}.\"\n        except Exception as e:\n            return f\"Failed to start {process_name}. Error: {e}\"\n    else:\n        try:\n            # Terminate all matching processes\n            for proc in matching_procs:\n                proc.terminate()\n            # Wait for processes to terminate\n            gone, still_alive = psutil.wait_procs(matching_procs, timeout=5)\n            if still_alive:\n                # Force kill if not terminated\n                for proc in still_alive:\n                    proc.kill()\n            # Give some time before restarting\n            time.sleep(1)\n            # Restart the process\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n        except Exception as e:\n            return f\"Failed to restart {process_name}. Error: {e}\"", "raw_solution": "```python\nimport subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    Checks if a process is running. If not, starts it.\n    If it is running, restarts it.\n\n    Args:\n        process_name (str): The name of the process to check and manage.\n\n    Returns:\n        str: A message indicating the action taken.\n    \"\"\"\n    # Flag to check if the process is running\n    process_running = False\n    # List to hold all matching processes\n    matching_procs = []\n\n    # Iterate over all running processes\n    for proc in psutil.process_iter(['name']):\n        try:\n            if proc.info['name'] == process_name:\n                process_running = True\n                matching_procs.append(proc)\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            pass  # Ignore processes that no longer exist or can't be accessed\n\n    if not process_running:\n        try:\n            # Start the process\n            subprocess.Popen([process_name])\n            return f\"Process not found. Starting {process_name}.\"\n        except Exception as e:\n            return f\"Failed to start {process_name}. Error: {e}\"\n    else:\n        try:\n            # Terminate all matching processes\n            for proc in matching_procs:\n                proc.terminate()\n            # Wait for processes to terminate\n            gone, still_alive = psutil.wait_procs(matching_procs, timeout=5)\n            if still_alive:\n                # Force kill if not terminated\n                for proc in still_alive:\n                    proc.kill()\n            # Give some time before restarting\n            time.sleep(1)\n            # Restart the process\n            subprocess.Popen([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n        except Exception as e:\n            return f\"Failed to restart {process_name}. Error: {e}\"\n\n# Example usage:\nif __name__ == \"__main__\":\n    process = \"notepad.exe\"  # Replace with your process name\n    message = task_func(process)\n    print(message)\n```"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n\n    # Get list of files (exclude subdirectories)\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, 'files.zip')\n\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in files:\n            file_path = os.path.join(directory, file)\n            zipf.write(file_path, arcname=file)\n\n    return zip_path", "raw_solution": "```python\nimport os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n\n    # Get list of files (exclude subdirectories)\n    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\n    if not files:\n        return None\n\n    zip_path = os.path.join(directory, 'files.zip')\n\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in files:\n            file_path = os.path.join(directory, file)\n            zipf.write(file_path, arcname=file)\n\n    return zip_path\n```"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs using regex\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    text_no_urls = url_pattern.sub('', text)\n    \n    # Extract words after removing URLs\n    words = re.findall(r'\\w+', text_no_urls)\n    \n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n    \n    # Plot the word cloud\n    plt.figure(figsize=(15, 7.5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud", "raw_solution": "```python\nimport re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs using regex\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    text_no_urls = url_pattern.sub('', text)\n    \n    # Extract words after removing URLs\n    words = re.findall(r'\\w+', text_no_urls)\n    \n    if not words:\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n    \n    # Plot the word cloud\n    plt.figure(figsize=(15, 7.5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud\n\n# Example usage\nif __name__ == \"__main__\":\n    sample_text = \"\"\"\n    Visit our website at https://www.example.com for more information.\n    Check out our blog at http://blog.example.com and follow us on social media!\n    \"\"\"\n    \n    try:\n        wc = task_func(sample_text)\n    except ValueError as ve:\n        print(ve)\n```"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    \"\"\"\n    Trains a Random Forest classifier and plots feature importances.\n\n    Parameters:\n    - df (pd.DataFrame): The input dataframe.\n    - target_column (str): The name of the target column.\n\n    Returns:\n    - clf (RandomForestClassifier): The trained Random Forest classifier.\n    - ax (matplotlib.axes.Axes): The Axes object of the plotted feature importances.\n    \"\"\"\n    # Separate features and target\n    X = df.drop(columns=target_column)\n    y = df[target_column]\n\n    # Initialize and train the Random Forest classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n\n    # Retrieve feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n\n    # Create a DataFrame for feature importances\n    feature_importances = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    })\n\n    # Sort the DataFrame by importance in descending order\n    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\n    # Set the aesthetic style of the plots\n    sns.set_style(\"whitegrid\")\n\n    # Create the bar plot\n    plt.figure(figsize=(10, 8))\n    ax = sns.barplot(\n        x='Importance',\n        y='Feature',\n        data=feature_importances,\n        palette='viridis'\n    )\n\n    # Set plot labels and title\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n\n    # Adjust layout for better fit\n    plt.tight_layout()\n\n    # Display the plot\n    plt.show()\n\n    return clf, ax", "raw_solution": "```python\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_column):\n    \"\"\"\n    Trains a Random Forest classifier and plots feature importances.\n\n    Parameters:\n    - df (pd.DataFrame): The input dataframe.\n    - target_column (str): The name of the target column.\n\n    Returns:\n    - clf (RandomForestClassifier): The trained Random Forest classifier.\n    - ax (matplotlib.axes.Axes): The Axes object of the plotted feature importances.\n    \"\"\"\n    # Separate features and target\n    X = df.drop(columns=target_column)\n    y = df[target_column]\n\n    # Initialize and train the Random Forest classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n\n    # Retrieve feature importances\n    importances = clf.feature_importances_\n    feature_names = X.columns\n\n    # Create a DataFrame for feature importances\n    feature_importances = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importances\n    })\n\n    # Sort the DataFrame by importance in descending order\n    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\n    # Set the aesthetic style of the plots\n    sns.set_style(\"whitegrid\")\n\n    # Create the bar plot\n    plt.figure(figsize=(10, 8))\n    ax = sns.barplot(\n        x='Importance',\n        y='Feature',\n        data=feature_importances,\n        palette='viridis'\n    )\n\n    # Set plot labels and title\n    ax.set_xlabel('Feature Importance Score')\n    ax.set_ylabel('Features')\n    ax.set_title('Visualizing Important Features')\n\n    # Adjust layout for better fit\n    plt.tight_layout()\n\n    # Display the plot\n    plt.show()\n\n    return clf, ax\n```"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for, flash\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\nclass User(UserMixin):\n    def __init__(self, id, username, password_hash):\n        self.id = id\n        self.username = username\n        self.password_hash = password_hash\n\n    def verify_password(self, password):\n        return check_password_hash(self.password_hash, password)\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['SESSION_PROTECTION'] = 'strong'\n\n    login_manager.init_app(app)\n\n    # In-memory user storage\n    users = {}\n    # Create a test user\n    test_user = User(id='1', username='testuser', password_hash=generate_password_hash('testpassword'))\n    users[test_user.id] = test_user\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        return users.get(user_id)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        if current_user.is_authenticated:\n            return redirect(url_for('protected'))\n\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = next((u for u in users.values() if u.username == form.username.data), None)\n            if user and user.verify_password(form.password.data):\n                login_user(user)\n                flash('Logged in successfully.', 'success')\n                return redirect(url_for('protected'))\n            else:\n                flash('Invalid username or password.', 'danger')\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        flash('You have been logged out.', 'info')\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', name=current_user.username)\n\n    return app", "raw_solution": "```python\nfrom flask import Flask, render_template, redirect, url_for, flash\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\nlogin_manager.login_view = 'login'\n\nclass User(UserMixin):\n    def __init__(self, id, username, password_hash):\n        self.id = id\n        self.username = username\n        self.password_hash = password_hash\n\n    def verify_password(self, password):\n        return check_password_hash(self.password_hash, password)\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['SESSION_PROTECTION'] = 'strong'\n\n    login_manager.init_app(app)\n\n    # In-memory user storage\n    users = {}\n    # Create a test user\n    test_user = User(id='1', username='testuser', password_hash=generate_password_hash('testpassword'))\n    users[test_user.id] = test_user\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        return users.get(user_id)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        if current_user.is_authenticated:\n            return redirect(url_for('protected'))\n\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = next((u for u in users.values() if u.username == form.username.data), None)\n            if user and user.verify_password(form.password.data):\n                login_user(user)\n                flash('Logged in successfully.', 'success')\n                return redirect(url_for('protected'))\n            else:\n                flash('Invalid username or password.', 'danger')\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        flash('You have been logged out.', 'info')\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', name=current_user.username)\n\n    return app\n\nif __name__ == '__main__':\n    # Example usage\n    app = task_func(secret_key='your_secret_key', template_folder='templates')\n    app.run(debug=True)\n```\n\n**Flask:** A Flask application instance configured for user authentication."}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    - data (np.ndarray): The dataset as a 2D NumPy array.\n    - column (int): The index of the column to check for outliers.\n    - outlier_z_score (float): The Z-score threshold to identify outliers.\n\n    Returns:\n    - tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Ensure data is a NumPy array\n    data = np.array(data)\n    \n    # Original indices\n    original_indices = np.arange(data.shape[0])\n    \n    # Standardize the specified column\n    scaler = StandardScaler()\n    column_data = data[:, column].reshape(-1, 1)\n    standardized_column = scaler.fit_transform(column_data).flatten()\n    \n    # Calculate Z-scores\n    z_scores = stats.zscore(standardized_column)\n    \n    # Identify outliers\n    outlier_mask = np.abs(z_scores) > outlier_z_score\n    outlier_indices = original_indices[outlier_mask]\n    \n    # Remove outliers\n    data_without_outliers = data[~outlier_mask]\n    \n    # Plotting\n    plt.figure(figsize=(14, 6))\n    \n    # Plot with outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(original_indices, data[:, column], color='red', label='Outliers')\n    plt.scatter(original_indices[~outlier_mask], data[:, column][~outlier_mask], color='blue', label='Data Points')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(f'Column {column}')\n    plt.legend()\n    \n    # Plot without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(original_indices[~outlier_mask], data_without_outliers[:, column], color='green')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(f'Column {column}')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return (data, data_without_outliers, outlier_indices)", "raw_solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    It standardizes the column, calculates Z-scores, and removes data points where the Z-score exceeds a threshold.\n    The function also visualizes the data before and after outlier removal.\n\n    Parameters:\n    - data (np.ndarray): The dataset as a 2D NumPy array.\n    - column (int): The index of the column to check for outliers.\n    - outlier_z_score (float): The Z-score threshold to identify outliers.\n\n    Returns:\n    - tuple: A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Ensure data is a NumPy array\n    data = np.array(data)\n    \n    # Original indices\n    original_indices = np.arange(data.shape[0])\n    \n    # Standardize the specified column\n    scaler = StandardScaler()\n    column_data = data[:, column].reshape(-1, 1)\n    standardized_column = scaler.fit_transform(column_data).flatten()\n    \n    # Calculate Z-scores\n    z_scores = stats.zscore(standardized_column)\n    \n    # Identify outliers\n    outlier_mask = np.abs(z_scores) > outlier_z_score\n    outlier_indices = original_indices[outlier_mask]\n    \n    # Remove outliers\n    data_without_outliers = data[~outlier_mask]\n    \n    # Plotting\n    plt.figure(figsize=(14, 6))\n    \n    # Plot with outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(original_indices, data[:, column], color='red', label='Outliers')\n    plt.scatter(original_indices[~outlier_mask], data[:, column][~outlier_mask], color='blue', label='Data Points')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(f'Column {column}')\n    plt.legend()\n    \n    # Plot without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(original_indices[~outlier_mask], data_without_outliers[:, column], color='green')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(f'Column {column}')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return (data, data_without_outliers, outlier_indices)\n\nif __name__ == \"__main__\":\n    # Sample data generation\n    np.random.seed(0)\n    # Create a dataset with 100 data points and 2 features\n    data = np.random.randn(100, 2) * 10 + 50\n    \n    # Introduce outliers in column 1\n    outliers = np.array([[100, 200], [110, -50], [90, 300]])\n    data = np.vstack([data, outliers])\n    \n    # Specify the column to check for outliers (0 or 1)\n    column_to_check = 1\n    # Define Z-score threshold\n    z_threshold = 2.0\n    \n    # Call the function\n    original_data, cleaned_data, outlier_indices = task_func(data, column_to_check, z_threshold)\n    \n    # Output the results\n    print(f\"Original data shape: {original_data.shape}\")\n    print(f\"Data without outliers shape: {cleaned_data.shape}\")\n    print(f\"Indices of outliers: {outlier_indices}\")\n```"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\nimport numpy as np\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on the provided dataset and generate a scatter plot\n    visualizing the clusters and their centroids.\n\n    Parameters:\n        data (pd.DataFrame): The input data for clustering.\n        n_clusters (int): The number of clusters to form.\n\n    Returns:\n        tuple:\n            np.ndarray: An array of cluster labels assigned to each sample.\n            plt.Axes: An Axes object with the scatter plot showing the clusters and centroids.\n\n    Raises:\n        ValueError: If 'data' is not a pd.DataFrame.\n        ValueError: If 'n_clusters' is not an integer greater than 1.\n    \"\"\"\n    # Validate that 'data' is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"If 'data' is not a pd.DataFrame.\")\n\n    # Validate that 'n_clusters' is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"If 'n_clusters' is not an integer greater than 1.\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis', alpha=0.6, edgecolor='k')\n    \n    # Plot centroids\n    ax.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', label='Centroids')\n\n    # Set plot labels and title\n    ax.set_xlabel(data.columns[0])\n    ax.set_ylabel(data.columns[1])\n    ax.set_title('K-Means Clustering')\n    ax.legend()\n\n    return labels, ax", "raw_solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\nimport numpy as np\n\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on the provided dataset and generate a scatter plot\n    visualizing the clusters and their centroids.\n\n    Parameters:\n        data (pd.DataFrame): The input data for clustering.\n        n_clusters (int): The number of clusters to form.\n\n    Returns:\n        tuple:\n            np.ndarray: An array of cluster labels assigned to each sample.\n            plt.Axes: An Axes object with the scatter plot showing the clusters and centroids.\n\n    Raises:\n        ValueError: If 'data' is not a pd.DataFrame.\n        ValueError: If 'n_clusters' is not an integer greater than 1.\n    \"\"\"\n    # Validate that 'data' is a pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"If 'data' is not a pd.DataFrame.\")\n\n    # Validate that 'n_clusters' is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"If 'n_clusters' is not an integer greater than 1.\")\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis', alpha=0.6, edgecolor='k')\n    \n    # Plot centroids\n    ax.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', label='Centroids')\n\n    # Set plot labels and title\n    ax.set_xlabel(data.columns[0])\n    ax.set_ylabel(data.columns[1])\n    ax.set_title('K-Means Clustering')\n    ax.legend()\n\n    return labels, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    from sklearn.datasets import make_blobs\n\n    # Generate sample data\n    X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n    data_df = pd.DataFrame(X, columns=['Feature1', 'Feature2'])\n\n    # Perform clustering and get the plot\n    labels, plot_ax = task_func(data_df, n_clusters=4)\n\n    # Display the plot\n    plt.show()\n```"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the given dataset and generates a scatter plot\n    of the transformed data.\n\n    Parameters:\n    - data (array-like or DataFrame): The input data to perform PCA on.\n    - n_components (int, optional): The number of principal components to keep. Must be a positive integer. Default is 2.\n\n    Returns:\n    - df_pca (DataFrame): The transformed data with principal components.\n    - ax (matplotlib.axes.Axes): The matplotlib Axes object containing the scatter plot.\n\n    Raises:\n    - ValueError: If n_components is not a positive integer.\n    \"\"\"\n    # Validate n_components\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Create DataFrame with principal components\n    pc_columns = [f'PC{i+1}' for i in range(n_components)]\n    df_pca = pd.DataFrame(transformed_data, columns=pc_columns)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    if n_components >= 2:\n        ax.scatter(df_pca['PC1'], df_pca['PC2'], alpha=0.7)\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n        ax.set_title('PCA Scatter Plot (PC1 vs PC2)')\n    else:\n        ax.scatter(df_pca['PC1'], np.zeros_like(df_pca['PC1']), alpha=0.7)\n        ax.set_xlabel('PC1')\n        ax.set_yticks([])\n        ax.set_title('PCA Scatter Plot (PC1)')\n    \n    ax.grid(True)\n    plt.tight_layout()\n    plt.close(fig)  # Prevents the plot from displaying immediately\n    \n    return df_pca, ax", "raw_solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the given dataset and generates a scatter plot\n    of the transformed data.\n\n    Parameters:\n    - data (array-like or DataFrame): The input data to perform PCA on.\n    - n_components (int, optional): The number of principal components to keep. Must be a positive integer. Default is 2.\n\n    Returns:\n    - df_pca (DataFrame): The transformed data with principal components.\n    - ax (matplotlib.axes.Axes): The matplotlib Axes object containing the scatter plot.\n\n    Raises:\n    - ValueError: If n_components is not a positive integer.\n    \"\"\"\n    # Validate n_components\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n    \n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    \n    # Create DataFrame with principal components\n    pc_columns = [f'PC{i+1}' for i in range(n_components)]\n    df_pca = pd.DataFrame(transformed_data, columns=pc_columns)\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    if n_components >= 2:\n        ax.scatter(df_pca['PC1'], df_pca['PC2'], alpha=0.7)\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n        ax.set_title('PCA Scatter Plot (PC1 vs PC2)')\n    else:\n        ax.scatter(df_pca['PC1'], np.zeros_like(df_pca['PC1']), alpha=0.7)\n        ax.set_xlabel('PC1')\n        ax.set_yticks([])\n        ax.set_title('PCA Scatter Plot (PC1)')\n    \n    ax.grid(True)\n    plt.tight_layout()\n    plt.close(fig)  # Prevents the plot from displaying immediately\n    \n    return df_pca, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate sample data\n    np.random.seed(0)\n    sample_data = np.random.randn(100, 5)  # 100 samples, 5 features\n    \n    # Perform PCA\n    try:\n        transformed_df, scatter_ax = task_func(sample_data, n_components=2)\n        print(\"Transformed Data:\")\n        print(transformed_df.head())\n        \n        # Display the scatter plot\n        fig = scatter_ax.get_figure()\n        fig.show()\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Set global font to Arial\n    plt.rcParams[\"font.family\"] = \"Arial\"\n    \n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n    \n    # Create seaborn pair plot\n    pairplot = sns.pairplot(df, hue='species', markers=[\"o\", \"s\", \"D\"])\n    \n    # Set the title of the plot\n    pairplot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16)\n    \n    # Adjust the layout to make room for the title\n    pairplot.fig.subplots_adjust(top=0.95)\n    \n    # Return the matplotlib Figure object\n    return pairplot.fig", "raw_solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Set global font to Arial\n    plt.rcParams[\"font.family\"] = \"Arial\"\n    \n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n    \n    # Create seaborn pair plot\n    pairplot = sns.pairplot(df, hue='species', markers=[\"o\", \"s\", \"D\"])\n    \n    # Set the title of the plot\n    pairplot.fig.suptitle('Iris Dataset Pair Plot', fontsize=16)\n    \n    # Adjust the layout to make room for the title\n    pairplot.fig.subplots_adjust(top=0.95)\n    \n    # Return the matplotlib Figure object\n    return pairplot.fig\n```"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(seed=42):\n    try:\n        # Set seed for reproducibility\n        random.seed(seed)\n\n        # Generate date range for the past 30 days\n        end_date = datetime.today()\n        dates = pd.date_range(end=end_date, periods=30)\n\n        # Generate random values\n        values = [random.uniform(0, 100) for _ in range(30)]\n\n        # Create DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n\n        # Set Arial font for better readability\n        plt.rcParams[\"font.family\"] = \"Arial\"\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(df['Date'], df['Value'], marker='o', linestyle='-')\n\n        # Set labels and title\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.set_title('Random Time Series Data')\n\n        # Improve date formatting on the x-axis\n        fig.autofmt_xdate()\n\n        # Display the plot\n        plt.tight_layout()\n        plt.show()\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the data or plot: {e}\")", "raw_solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    try:\n        # Set seed for reproducibility\n        random.seed(seed)\n\n        # Generate date range for the past 30 days\n        end_date = datetime.today()\n        dates = pd.date_range(end=end_date, periods=30)\n\n        # Generate random values\n        values = [random.uniform(0, 100) for _ in range(30)]\n\n        # Create DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n\n        # Set Arial font for better readability\n        plt.rcParams[\"font.family\"] = \"Arial\"\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(df['Date'], df['Value'], marker='o', linestyle='-')\n\n        # Set labels and title\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        ax.set_title('Random Time Series Data')\n\n        # Improve date formatting on the x-axis\n        fig.autofmt_xdate()\n\n        # Display the plot\n        plt.tight_layout()\n        plt.show()\n\n        return ax\n\n    except Exception as e:\n        raise ValueError(f\"An error occurred while generating the data or plot: {e}\")\n```"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        np.random.seed(seed)\n        # Read the data from the URL\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        # Each sample is stored over two rows: first row has features, second row has target\n        data_values = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :]])\n        column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',\n                       'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n        df = pd.DataFrame(data_values, columns=column_names)\n        # Convert all columns to numeric\n        df = df.apply(pd.to_numeric)\n        # Compute correlation matrix\n        corr = df.corr()\n        # Generate the heatmap\n        plt.figure(figsize=(10,8))\n        ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n        plt.title('Boston Housing Dataset Correlation Heatmap')\n        plt.tight_layout()\n        # Save plot if save_path is specified\n        if save_path:\n            plt.savefig(save_path)\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")", "raw_solution": "```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42, save_path=None):\n    try:\n        np.random.seed(seed)\n        # Read the data from the URL\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        # Each sample is stored over two rows: first row has features, second row has target\n        data_values = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :]])\n        column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',\n                       'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n        df = pd.DataFrame(data_values, columns=column_names)\n        # Convert all columns to numeric\n        df = df.apply(pd.to_numeric)\n        # Compute correlation matrix\n        corr = df.corr()\n        # Generate the heatmap\n        plt.figure(figsize=(10,8))\n        ax = sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n        plt.title('Boston Housing Dataset Correlation Heatmap')\n        plt.tight_layout()\n        # Save plot if save_path is specified\n        if save_path:\n            plt.savefig(save_path)\n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n```"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes the 'value' column of a DataFrame into trend, seasonality, and residuals.\n\n    Parameters:\n    - df (pd.DataFrame): DataFrame containing a 'value' column with a datetime index.\n    - freq (str): Frequency string (e.g., 'D' for daily, 'M' for monthly).\n    - decomposition_model (str): Type of decomposition ('additive' or 'multiplicative').\n\n    Returns:\n    - tuple: (DecomposeResult object, list of matplotlib Axes objects)\n\n    Raises:\n    - ValueError: If input validations fail.\n    \"\"\"\n\n    # Validate that df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n\n    # Check that 'value' column exists\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n\n    # Check that 'value' column is numeric\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data.\")\n\n    # Validate 'freq' is a valid frequency string\n    try:\n        pd.tseries.frequencies.to_offset(freq)\n    except ValueError:\n        raise ValueError(\"'freq' is not a valid frequency string.\")\n\n    # Validate 'decomposition_model'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' must be either 'additive' or 'multiplicative'.\")\n\n    # Ensure the index is datetime\n    if not pd.api.types.is_datetime64_any_dtype(df.index):\n        raise ValueError(\"DataFrame index must be a datetime type.\")\n\n    # Set the frequency of the DataFrame\n    try:\n        df = df.asfreq(freq)\n    except ValueError as e:\n        raise ValueError(f\"Cannot set DataFrame index frequency to '{freq}': {e}\")\n\n    # Perform seasonal decomposition\n    try:\n        decomposition = seasonal_decompose(df['value'], model=decomposition_model)\n    except Exception as e:\n        raise ValueError(f\"Error in decomposition: {e}\")\n\n    # Plot the decomposition\n    fig = decomposition.plot()\n    plt.tight_layout()\n\n    return (decomposition, fig.axes)", "raw_solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes the 'value' column of a DataFrame into trend, seasonality, and residuals.\n\n    Parameters:\n    - df (pd.DataFrame): DataFrame containing a 'value' column with a datetime index.\n    - freq (str): Frequency string (e.g., 'D' for daily, 'M' for monthly).\n    - decomposition_model (str): Type of decomposition ('additive' or 'multiplicative').\n\n    Returns:\n    - tuple: (DecomposeResult object, list of matplotlib Axes objects)\n\n    Raises:\n    - ValueError: If input validations fail.\n    \"\"\"\n\n    # Validate that df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n\n    # Check that 'value' column exists\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n\n    # Check that 'value' column is numeric\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"'value' column must contain numeric data.\")\n\n    # Validate 'freq' is a valid frequency string\n    try:\n        pd.tseries.frequencies.to_offset(freq)\n    except ValueError:\n        raise ValueError(\"'freq' is not a valid frequency string.\")\n\n    # Validate 'decomposition_model'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"'decomposition_model' must be either 'additive' or 'multiplicative'.\")\n\n    # Ensure the index is datetime\n    if not pd.api.types.is_datetime64_any_dtype(df.index):\n        raise ValueError(\"DataFrame index must be a datetime type.\")\n\n    # Set the frequency of the DataFrame\n    try:\n        df = df.asfreq(freq)\n    except ValueError as e:\n        raise ValueError(f\"Cannot set DataFrame index frequency to '{freq}': {e}\")\n\n    # Perform seasonal decomposition\n    try:\n        decomposition = seasonal_decompose(df['value'], model=decomposition_model)\n    except Exception as e:\n        raise ValueError(f\"Error in decomposition: {e}\")\n\n    # Plot the decomposition\n    fig = decomposition.plot()\n    plt.tight_layout()\n\n    return (decomposition, fig.axes)\n```"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate a pandas Series of random dates within a specified date range.\n\n    Parameters:\n    - start_date (datetime): The start date of the range (inclusive).\n    - end_date (datetime): The end date of the range (inclusive).\n    - seed (int, optional): Seed for the random number generator. Default is 42.\n\n    Returns:\n    - pandas.Series: A Series containing random datetime objects within the specified range.\n\n    Raises:\n    - ValueError: If start_date or end_date is not a datetime instance,\n                  or if start_date is later than end_date.\n    \"\"\"\n    # Validate input types\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances.\")\n    \n    # Validate date order\n    if start_date > end_date:\n        raise ValueError(\"start_date must not be later than end_date.\")\n    \n    # Initialize random seed\n    random_seed(seed)\n    \n    # Calculate the total number of days in the range (inclusive)\n    delta = end_date - start_date\n    total_days = delta.days + 1  # Inclusive of end_date\n    \n    # Generate random dates\n    random_dates = []\n    for _ in range(total_days):\n        random_offset = randint(0, delta.days)\n        random_date = start_date + timedelta(days=random_offset)\n        random_dates.append(random_date)\n    \n    # Create a pandas Series\n    date_series = pd.Series(random_dates)\n    \n    return date_series", "raw_solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate a pandas Series of random dates within a specified date range.\n\n    Parameters:\n    - start_date (datetime): The start date of the range (inclusive).\n    - end_date (datetime): The end date of the range (inclusive).\n    - seed (int, optional): Seed for the random number generator. Default is 42.\n\n    Returns:\n    - pandas.Series: A Series containing random datetime objects within the specified range.\n\n    Raises:\n    - ValueError: If start_date or end_date is not a datetime instance,\n                  or if start_date is later than end_date.\n    \"\"\"\n    # Validate input types\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances.\")\n    \n    # Validate date order\n    if start_date > end_date:\n        raise ValueError(\"start_date must not be later than end_date.\")\n    \n    # Initialize random seed\n    random_seed(seed)\n    \n    # Calculate the total number of days in the range (inclusive)\n    delta = end_date - start_date\n    total_days = delta.days + 1  # Inclusive of end_date\n    \n    # Generate random dates\n    random_dates = []\n    for _ in range(total_days):\n        random_offset = randint(0, delta.days)\n        random_date = start_date + timedelta(days=random_offset)\n        random_dates.append(random_date)\n    \n    # Create a pandas Series\n    date_series = pd.Series(random_dates)\n    \n    return date_series\n\n# Example usage:\nif __name__ == \"__main__\":\n    start = datetime(2023, 1, 1)\n    end = datetime(2023, 1, 10)\n    series = task_func(start_date=start, end_date=end, seed=123)\n    print(series)\n```"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modifies the input list by adding the element '12', then concatenates a number of CSV files\n    from the specified directory into a single DataFrame. The number of files concatenated is\n    determined by the sum of the numbers in the list.\n\n    Parameters:\n    - my_list (list): The list of numbers.\n    - file_dir (str): Directory containing CSV files. Defaults to './data_files/'.\n    - file_ext (str): Extension of files to concatenate. Defaults to '.csv'.\n\n    Returns:\n    - pandas.DataFrame: Concatenated DataFrame from the selected CSV files.\n\n    Raises:\n    - TypeError: If 'my_list' is not a list.\n    - FileNotFoundError: If no CSV files are found in the specified directory.\n    - ValueError: If elements in 'my_list' cannot be converted to numbers.\n    \"\"\"\n    \n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list.\")\n    \n    # Add the element '12' to the list\n    my_list.append('12')\n    \n    # Calculate the sum of the numbers in the list\n    try:\n        total = sum(float(item) for item in my_list)\n    except ValueError:\n        raise ValueError(\"All elements in 'my_list' must be numbers or numeric strings.\")\n    \n    # Find all files with the given extension in the directory\n    search_pattern = os.path.join(file_dir, f'*{file_ext}')\n    files = glob.glob(search_pattern)\n    \n    if not files:\n        raise FileNotFoundError(f\"No files with extension '{file_ext}' found in directory '{file_dir}'.\")\n    \n    # Determine the number of files to concatenate based on the sum\n    num_files = int(total)\n    \n    if num_files <= 0:\n        raise ValueError(\"The sum of the list must be a positive integer to select files.\")\n    \n    # Check if there are enough files\n    if num_files > len(files):\n        raise FileNotFoundError(f\"Requested {num_files} files, but only found {len(files)} in '{file_dir}'.\")\n    \n    # Select the required number of files\n    selected_files = files[:num_files]\n    \n    # Read and concatenate the selected CSV files into a single DataFrame\n    try:\n        df_list = [pd.read_csv(f) for f in selected_files]\n        concatenated_df = pd.concat(df_list, ignore_index=True)\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred while reading or concatenating files: {e}\")\n    \n    return concatenated_df", "raw_solution": "```python\nimport pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modifies the input list by adding the element '12', then concatenates a number of CSV files\n    from the specified directory into a single DataFrame. The number of files concatenated is\n    determined by the sum of the numbers in the list.\n\n    Parameters:\n    - my_list (list): The list of numbers.\n    - file_dir (str): Directory containing CSV files. Defaults to './data_files/'.\n    - file_ext (str): Extension of files to concatenate. Defaults to '.csv'.\n\n    Returns:\n    - pandas.DataFrame: Concatenated DataFrame from the selected CSV files.\n\n    Raises:\n    - TypeError: If 'my_list' is not a list.\n    - FileNotFoundError: If no CSV files are found in the specified directory.\n    - ValueError: If elements in 'my_list' cannot be converted to numbers.\n    \"\"\"\n    \n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list.\")\n    \n    # Add the element '12' to the list\n    my_list.append('12')\n    \n    # Calculate the sum of the numbers in the list\n    try:\n        total = sum(float(item) for item in my_list)\n    except ValueError:\n        raise ValueError(\"All elements in 'my_list' must be numbers or numeric strings.\")\n    \n    # Find all files with the given extension in the directory\n    search_pattern = os.path.join(file_dir, f'*{file_ext}')\n    files = glob.glob(search_pattern)\n    \n    if not files:\n        raise FileNotFoundError(f\"No files with extension '{file_ext}' found in directory '{file_dir}'.\")\n    \n    # Determine the number of files to concatenate based on the sum\n    num_files = int(total)\n    \n    if num_files <= 0:\n        raise ValueError(\"The sum of the list must be a positive integer to select files.\")\n    \n    # Check if there are enough files\n    if num_files > len(files):\n        raise FileNotFoundError(f\"Requested {num_files} files, but only found {len(files)} in '{file_dir}'.\")\n    \n    # Select the required number of files\n    selected_files = files[:num_files]\n    \n    # Read and concatenate the selected CSV files into a single DataFrame\n    try:\n        df_list = [pd.read_csv(f) for f in selected_files]\n        concatenated_df = pd.concat(df_list, ignore_index=True)\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred while reading or concatenating files: {e}\")\n    \n    return concatenated_df\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_list = [1, 2, 3]\n    try:\n        result_df = task_func(sample_list, file_dir='./data_files/', file_ext='.csv')\n        print(result_df)\n    except Exception as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    Enhances 'my_list' by appending the number 12, generates a list of random integers based on the sum\n    of elements in 'my_list' (limited by 'size'), measures the time taken for this process, and plots\n    a histogram of the generated random numbers.\n\n    Parameters:\n        my_list (list): The input list to be enhanced and used for generating random numbers.\n        size (int, optional): The upper limit for the number of random integers to generate. Default is 100.\n        seed (int, optional): The seed value for the random number generator to ensure reproducibility. Default is 100.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: Time taken to generate the list of random numbers in seconds.\n            - matplotlib.axes.Axes: The Axes object containing the histogram plot.\n\n    Raises:\n        TypeError: If 'my_list' is not a list.\n        ValueError: If 'my_list' contains non-numeric elements.\n    \"\"\"\n    # Validate that my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list.\")\n\n    # Validate that all elements in my_list are int or float\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"'my_list' must contain only numeric (int or float) elements.\")\n\n    # Append the number 12 to my_list\n    my_list.append(12)\n\n    # Calculate the sum of elements in my_list\n    total_sum = sum(my_list)\n\n    # Determine the number of random integers to generate, limited by 'size'\n    random_size = min(total_sum, size)\n\n    # Set the random seed for reproducibility\n    random_seed(seed)\n\n    # Measure the time taken to generate the list of random numbers\n    start_time = time.time()\n    random_numbers = [randint(1, 100) for _ in range(random_size)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    # Create a histogram of the generated random numbers\n    fig, ax = plt.subplots()\n    bins = range(1, 102)  # Bins from 1 to 100 inclusive\n    ax.hist(random_numbers, bins=bins, edgecolor='black', align='left')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Generated Random Numbers')\n\n    return (time_taken, ax)", "raw_solution": "```python\nfrom random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    Enhances 'my_list' by appending the number 12, generates a list of random integers based on the sum\n    of elements in 'my_list' (limited by 'size'), measures the time taken for this process, and plots\n    a histogram of the generated random numbers.\n\n    Parameters:\n        my_list (list): The input list to be enhanced and used for generating random numbers.\n        size (int, optional): The upper limit for the number of random integers to generate. Default is 100.\n        seed (int, optional): The seed value for the random number generator to ensure reproducibility. Default is 100.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: Time taken to generate the list of random numbers in seconds.\n            - matplotlib.axes.Axes: The Axes object containing the histogram plot.\n\n    Raises:\n        TypeError: If 'my_list' is not a list.\n        ValueError: If 'my_list' contains non-numeric elements.\n    \"\"\"\n    # Validate that my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"'my_list' must be a list.\")\n\n    # Validate that all elements in my_list are int or float\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"'my_list' must contain only numeric (int or float) elements.\")\n\n    # Append the number 12 to my_list\n    my_list.append(12)\n\n    # Calculate the sum of elements in my_list\n    total_sum = sum(my_list)\n\n    # Determine the number of random integers to generate, limited by 'size'\n    random_size = min(total_sum, size)\n\n    # Set the random seed for reproducibility\n    random_seed(seed)\n\n    # Measure the time taken to generate the list of random numbers\n    start_time = time.time()\n    random_numbers = [randint(1, 100) for _ in range(random_size)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n\n    # Create a histogram of the generated random numbers\n    fig, ax = plt.subplots()\n    bins = range(1, 102)  # Bins from 1 to 100 inclusive\n    ax.hist(random_numbers, bins=bins, edgecolor='black', align='left')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Generated Random Numbers')\n\n    return (time_taken, ax)\n\nif __name__ == '__main__':\n    # Example usage of task_func\n    sample_list = [5, 10, 15, 20]\n    try:\n        elapsed_time, histogram_ax = task_func(sample_list, size=100, seed=42)\n        print(f\"Time taken to generate random numbers: {elapsed_time:.6f} seconds\")\n        plt.show()\n    except (TypeError, ValueError) as error:\n        print(f\"Error: {error}\")\n```"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrapes the first table from the specified web page URL and extracts the data into a Pandas DataFrame.\n    \n    Parameters:\n        url (str): The URL of the web page to scrape. Defaults to 'http://example.com'.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the scraped table data, with rows corresponding to table rows\n                      and columns named after the table headers, if available.\n    \n    Raises:\n        ConnectionError: If there is an issue connecting to the URL.\n        requests.HTTPError: If the HTTP request to the URL fails.\n        ValueError: If no table data is found on the page or if the page content cannot be parsed.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.ConnectionError as ce:\n        raise ConnectionError(\"Failed to connect to the URL\") from ce\n    except requests.exceptions.HTTPError as he:\n        raise requests.HTTPError(f\"HTTP request failed: {he}\") from he\n\n    try:\n        soup = BeautifulSoup(response.text, 'html.parser')\n        table = soup.find('table')\n        if not table:\n            raise ValueError(\"No table found on the page\")\n\n        # Extract headers if present\n        headers = []\n        header_row = table.find('tr')\n        if header_row and header_row.find_all('th'):\n            headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n            rows = table.find_all('tr')[1:]  # Skip header row\n        else:\n            rows = table.find_all('tr')\n\n        # Extract table data\n        data = []\n        for row in rows:\n            cells = row.find_all('td')\n            if not cells:\n                continue  # Skip rows without <td> elements\n            data.append([cell.get_text(strip=True) for cell in cells])\n\n        if not data:\n            raise ValueError(\"No table data found on the page\")\n\n        # Create DataFrame\n        if headers:\n            df = pd.DataFrame(data, columns=headers)\n        else:\n            df = pd.DataFrame(data)\n\n        return df\n\n    except Exception as e:\n        raise ValueError(f\"Failed to parse the table data: {e}\") from e", "raw_solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrapes the first table from the specified web page URL and extracts the data into a Pandas DataFrame.\n    \n    Parameters:\n        url (str): The URL of the web page to scrape. Defaults to 'http://example.com'.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the scraped table data, with rows corresponding to table rows\n                      and columns named after the table headers, if available.\n    \n    Raises:\n        ConnectionError: If there is an issue connecting to the URL.\n        requests.HTTPError: If the HTTP request to the URL fails.\n        ValueError: If no table data is found on the page or if the page content cannot be parsed.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.ConnectionError as ce:\n        raise ConnectionError(\"Failed to connect to the URL\") from ce\n    except requests.exceptions.HTTPError as he:\n        raise requests.HTTPError(f\"HTTP request failed: {he}\") from he\n\n    try:\n        soup = BeautifulSoup(response.text, 'html.parser')\n        table = soup.find('table')\n        if not table:\n            raise ValueError(\"No table found on the page\")\n\n        # Extract headers if present\n        headers = []\n        header_row = table.find('tr')\n        if header_row and header_row.find_all('th'):\n            headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n            rows = table.find_all('tr')[1:]  # Skip header row\n        else:\n            rows = table.find_all('tr')\n\n        # Extract table data\n        data = []\n        for row in rows:\n            cells = row.find_all('td')\n            if not cells:\n                continue  # Skip rows without <td> elements\n            data.append([cell.get_text(strip=True) for cell in cells])\n\n        if not data:\n            raise ValueError(\"No table data found on the page\")\n\n        # Create DataFrame\n        if headers:\n            df = pd.DataFrame(data, columns=headers)\n        else:\n            df = pd.DataFrame(data)\n\n        return df\n\n    except Exception as e:\n        raise ValueError(f\"Failed to parse the table data: {e}\") from e\n```"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draw histograms of numeric columns in a DataFrame and return the plots.\n\n    Parameters:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n\n    Raises:\n        ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns.\n    \"\"\"\n    # Validate input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n\n    # Select numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if not numeric_cols:\n        raise ValueError(\"DataFrame contains no numeric columns.\")\n\n    num_cols = len(numeric_cols)\n    \n    # Determine the number of subplot rows and columns\n    # Here, we'll use 3 columns per row for better visualization\n    cols_per_row = 3\n    rows = num_cols // cols_per_row + int(num_cols % cols_per_row > 0)\n    \n    # Create subplots\n    fig, axes = plt.subplots(rows, cols_per_row, figsize=(5 * cols_per_row, 4 * rows))\n    \n    # If there's only one row, axes may not be a 2D array\n    if rows == 1:\n        axes = np.expand_dims(axes, axis=0)\n    \n    axes = axes.flatten()\n    \n    axes_list = []\n    \n    for idx, col in enumerate(numeric_cols):\n        ax = axes[idx]\n        ax.hist(df[col].dropna(), bins=30, color='skyblue', edgecolor='black')\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n    \n    # Hide any unused subplots\n    for idx in range(len(numeric_cols), len(axes)):\n        axes[idx].set_visible(False)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return axes_list", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draw histograms of numeric columns in a DataFrame and return the plots.\n\n    Parameters:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n\n    Raises:\n        ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns.\n    \"\"\"\n    # Validate input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n\n    # Select numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if not numeric_cols:\n        raise ValueError(\"DataFrame contains no numeric columns.\")\n\n    num_cols = len(numeric_cols)\n    \n    # Determine the number of subplot rows and columns\n    # Here, we'll use 3 columns per row for better visualization\n    cols_per_row = 3\n    rows = num_cols // cols_per_row + int(num_cols % cols_per_row > 0)\n    \n    # Create subplots\n    fig, axes = plt.subplots(rows, cols_per_row, figsize=(5 * cols_per_row, 4 * rows))\n    \n    # If there's only one row, axes may not be a 2D array\n    if rows == 1:\n        axes = np.expand_dims(axes, axis=0)\n    \n    axes = axes.flatten()\n    \n    axes_list = []\n    \n    for idx, col in enumerate(numeric_cols):\n        ax = axes[idx]\n        ax.hist(df[col].dropna(), bins=30, color='skyblue', edgecolor='black')\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes_list.append(ax)\n    \n    # Hide any unused subplots\n    for idx in range(len(numeric_cols), len(axes)):\n        axes[idx].set_visible(False)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return axes_list\n```"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread, Lock\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans a specified IP address range and checks if a specified port is open on each IP.\n\n    Args:\n        ip_range (str): IP range in CIDR notation (e.g., '192.168.1.0/24').\n        port (int): The port number to check.\n\n    Returns:\n        dict: A dictionary mapping IP addresses to their port status (True if open, False otherwise).\n    \"\"\"\n    # Initialize the result dictionary\n    results = {}\n    # Lock to synchronize access to the results dictionary\n    lock = Lock()\n    \n    def scan_ip(ip):\n        \"\"\"\n        Attempts to connect to the specified IP and port to check if the port is open.\n\n        Args:\n            ip (str): The IP address to scan.\n        \"\"\"\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                s.settimeout(1)  # Set timeout to 1 second\n                result = s.connect_ex((str(ip), port))\n                is_open = result == 0\n        except socket.error:\n            is_open = False\n        \n        # Safely update the results dictionary\n        with lock:\n            results[str(ip)] = is_open\n\n    # Create a list to hold all thread objects\n    threads = []\n\n    # Parse the IP range\n    network = IPv4Network(ip_range, strict=False)\n\n    # Start a thread for each IP address in the range\n    for ip in network.hosts():\n        thread = Thread(target=scan_ip, args=(ip,))\n        thread.start()\n        threads.append(thread)\n    \n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    \n    return results", "raw_solution": "```python\nimport socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread, Lock\n\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans a specified IP address range and checks if a specified port is open on each IP.\n\n    Args:\n        ip_range (str): IP range in CIDR notation (e.g., '192.168.1.0/24').\n        port (int): The port number to check.\n\n    Returns:\n        dict: A dictionary mapping IP addresses to their port status (True if open, False otherwise).\n    \"\"\"\n    # Initialize the result dictionary\n    results = {}\n    # Lock to synchronize access to the results dictionary\n    lock = Lock()\n    \n    def scan_ip(ip):\n        \"\"\"\n        Attempts to connect to the specified IP and port to check if the port is open.\n\n        Args:\n            ip (str): The IP address to scan.\n        \"\"\"\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                s.settimeout(1)  # Set timeout to 1 second\n                result = s.connect_ex((str(ip), port))\n                is_open = result == 0\n        except socket.error:\n            is_open = False\n        \n        # Safely update the results dictionary\n        with lock:\n            results[str(ip)] = is_open\n\n    # Create a list to hold all thread objects\n    threads = []\n\n    # Parse the IP range\n    network = IPv4Network(ip_range, strict=False)\n\n    # Start a thread for each IP address in the range\n    for ip in network.hosts():\n        thread = Thread(target=scan_ip, args=(ip,))\n        thread.start()\n        threads.append(thread)\n    \n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    \n    return results\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the IP range and port to scan\n    ip_range = '192.168.1.0/28'  # Adjust the range as needed\n    port = 80  # Example port\n\n    # Call the scanning function\n    scan_results = task_func(ip_range, port)\n\n    # Print the results\n    for ip, status in scan_results.items():\n        status_str = 'Open' if status else 'Closed'\n        print(f\"Port {port} on {ip}: {status_str}\")\n```"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\nimport os\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information from a log file and stores it in a CSV file.\n\n    Parameters:\n        log_file (str): The path to the log file.\n\n    Returns:\n        str: The file path to the newly created CSV file.\n\n    Raises:\n        ValueError: If any timestamp is invalid or no valid log entries are found.\n    \"\"\"\n    log_pattern = re.compile(\n        r'^(?P<type>\\w+): \\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (?P<message>.*)$'\n    )\n    entries = []\n    invalid_timestamps = False\n\n    with open(log_file, 'r') as file:\n        for line_num, line in enumerate(file, start=1):\n            line = line.strip()\n            if not line:\n                continue  # Skip empty lines\n            match = log_pattern.match(line)\n            if match:\n                log_type = match.group('type')\n                timestamp_str = match.group('timestamp')\n                message = match.group('message')\n                try:\n                    # Validate timestamp\n                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n                    entries.append({\n                        'Type': log_type,\n                        'Timestamp': timestamp_str,\n                        'Message': message\n                    })\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp '{timestamp_str}' on line {line_num}.\")\n            else:\n                continue  # Skip lines that don't match the pattern\n\n    if not entries:\n        raise ValueError(\"No valid log entries found.\")\n\n    # Define the CSV file path\n    base_name = os.path.splitext(os.path.basename(log_file))[0]\n    csv_file = f\"{base_name}_structured.csv\"\n\n    # Create DataFrame and save to CSV\n    df = pd.DataFrame(entries)\n    df.to_csv(csv_file, index=False)\n\n    return os.path.abspath(csv_file)", "raw_solution": "```python\nimport re\nimport pandas as pd\nfrom datetime import datetime\nimport os\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information from a log file and stores it in a CSV file.\n\n    Parameters:\n        log_file (str): The path to the log file.\n\n    Returns:\n        str: The file path to the newly created CSV file.\n\n    Raises:\n        ValueError: If any timestamp is invalid or no valid log entries are found.\n    \"\"\"\n    log_pattern = re.compile(\n        r'^(?P<type>\\w+): \\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (?P<message>.*)$'\n    )\n    entries = []\n    invalid_timestamps = False\n\n    with open(log_file, 'r') as file:\n        for line_num, line in enumerate(file, start=1):\n            line = line.strip()\n            if not line:\n                continue  # Skip empty lines\n            match = log_pattern.match(line)\n            if match:\n                log_type = match.group('type')\n                timestamp_str = match.group('timestamp')\n                message = match.group('message')\n                try:\n                    # Validate timestamp\n                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n                    entries.append({\n                        'Type': log_type,\n                        'Timestamp': timestamp_str,\n                        'Message': message\n                    })\n                except ValueError:\n                    raise ValueError(f\"Invalid timestamp '{timestamp_str}' on line {line_num}.\")\n            else:\n                continue  # Skip lines that don't match the pattern\n\n    if not entries:\n        raise ValueError(\"No valid log entries found.\")\n\n    # Define the CSV file path\n    base_name = os.path.splitext(os.path.basename(log_file))[0]\n    csv_file = f\"{base_name}_structured.csv\"\n\n    # Create DataFrame and save to CSV\n    df = pd.DataFrame(entries)\n    df.to_csv(csv_file, index=False)\n\n    return os.path.abspath(csv_file)\n```"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a given text.\n    \n    Parameters:\n    - text (str): The input text to analyze.\n    - rwidth (float): Relative width of the bars as a fraction of the bin width (default is 0.8).\n    \n    Returns:\n    - matplotlib.axes.Axes: An Axes object containing the histogram of word lengths.\n    \"\"\"\n    # Use regex to extract words (alphanumeric characters and apostrophes)\n    words = re.findall(r\"\\b\\w+'\\w+|\\b\\w+\\b\", text)\n    \n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n    \n    # Create a figure and axis for the histogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    if word_lengths:\n        # Determine the range of word lengths\n        min_len = min(word_lengths)\n        max_len = max(word_lengths)\n        \n        # Define bins for each word length\n        bins = np.arange(min_len - 0.5, max_len + 1.5, 1)\n        \n        # Plot the histogram\n        ax.hist(word_lengths, bins=bins, rwidth=rwidth, color='skyblue', edgecolor='black')\n        \n        # Set x-axis ticks to be integers\n        ax.set_xticks(range(min_len, max_len + 1))\n        \n        # Set labels and title\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Word Lengths')\n    else:\n        # If no words, set labels and title without plotting\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Word Lengths (No data)')\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return ax", "raw_solution": "```python\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes and visualizes the distribution of word lengths in a given text.\n    \n    Parameters:\n    - text (str): The input text to analyze.\n    - rwidth (float): Relative width of the bars as a fraction of the bin width (default is 0.8).\n    \n    Returns:\n    - matplotlib.axes.Axes: An Axes object containing the histogram of word lengths.\n    \"\"\"\n    # Use regex to extract words (alphanumeric characters and apostrophes)\n    words = re.findall(r\"\\b\\w+'\\w+|\\b\\w+\\b\", text)\n    \n    # Calculate the length of each word\n    word_lengths = [len(word) for word in words]\n    \n    # Create a figure and axis for the histogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    if word_lengths:\n        # Determine the range of word lengths\n        min_len = min(word_lengths)\n        max_len = max(word_lengths)\n        \n        # Define bins for each word length\n        bins = np.arange(min_len - 0.5, max_len + 1.5, 1)\n        \n        # Plot the histogram\n        ax.hist(word_lengths, bins=bins, rwidth=rwidth, color='skyblue', edgecolor='black')\n        \n        # Set x-axis ticks to be integers\n        ax.set_xticks(range(min_len, max_len + 1))\n        \n        # Set labels and title\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Word Lengths')\n    else:\n        # If no words, set labels and title without plotting\n        ax.set_xlabel('Word Length')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Word Lengths (No data)')\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_text = \"\"\"\n    In computer programming, a self-contained script is a program that includes all the necessary components to execute independently.\n    It does not rely on external files or resources beyond standard libraries.\n    \"\"\"\n    task_func(sample_text)\n```"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import defaultdict\ndef task_func(df):\n    import nltk\n    # Ensure necessary NLTK data is downloaded\n    try:\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        nltk.download('punkt')\n\n    # Validate DataFrame\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    required_columns = {'Title', 'Content'}\n    if not required_columns.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'Title' and 'Content' columns.\")\n\n    # Define keywords\n    keywords = ['like', 'what']\n    \n    # Filter articles with titles containing the keywords (case-insensitive)\n    pattern = re.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', re.IGNORECASE)\n    filtered_df = df[df['Title'].str.contains(pattern, na=False)]\n\n    # Initialize frequency dictionary\n    freq_dict = defaultdict(int)\n\n    # Process each content\n    for content in filtered_df['Content']:\n        # Tokenize content into words\n        words = nltk.word_tokenize(content)\n        for word in words:\n            # Remove punctuation and convert to lowercase\n            word_clean = word.lower().strip(punctuation)\n            if word_clean:\n                freq_dict[word_clean] += 1\n\n    return dict(freq_dict)", "raw_solution": "```python\nimport re\nimport nltk\nfrom string import punctuation\nfrom collections import defaultdict\n\ndef task_func(df):\n    import nltk\n    # Ensure necessary NLTK data is downloaded\n    try:\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        nltk.download('punkt')\n\n    # Validate DataFrame\n    if df.empty:\n        raise ValueError(\"DataFrame is empty.\")\n    required_columns = {'Title', 'Content'}\n    if not required_columns.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'Title' and 'Content' columns.\")\n\n    # Define keywords\n    keywords = ['like', 'what']\n    \n    # Filter articles with titles containing the keywords (case-insensitive)\n    pattern = re.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', re.IGNORECASE)\n    filtered_df = df[df['Title'].str.contains(pattern, na=False)]\n\n    # Initialize frequency dictionary\n    freq_dict = defaultdict(int)\n\n    # Process each content\n    for content in filtered_df['Content']:\n        # Tokenize content into words\n        words = nltk.word_tokenize(content)\n        for word in words:\n            # Remove punctuation and convert to lowercase\n            word_clean = word.lower().strip(punctuation)\n            if word_clean:\n                freq_dict[word_clean] += 1\n\n    return dict(freq_dict)\n```"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = [\n    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n    'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n    'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n    'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n    'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n    'further', 'then', 'once'\n]\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Preprocesses text data by removing stopwords, numbers, and punctuation,\n    then vectorizes the text into a count-based numeric format.\n\n    Parameters:\n    - dataframe (pd.DataFrame): The input DataFrame containing the text data.\n    - text_column (str): The name of the column containing text to be processed.\n\n    Returns:\n    - pd.DataFrame: A DataFrame where each column represents a word and each row\n                    contains the count of each word in the corresponding text entry.\n    \"\"\"\n    \n    # Function to preprocess individual text entries\n    def preprocess(text):\n        # Convert to lowercase\n        text = text.lower()\n        # Remove numbers\n        text = re.sub(r'\\d+', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Tokenize text\n        tokens = text.split()\n        # Remove stopwords\n        tokens = [word for word in tokens if word not in STOPWORDS]\n        # Join tokens back to string\n        return ' '.join(tokens)\n    \n    # Apply preprocessing to the specified text column\n    dataframe['processed_text'] = dataframe[text_column].astype(str).apply(preprocess)\n    \n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the processed text data\n    X = vectorizer.fit_transform(dataframe['processed_text'])\n    \n    # Create a DataFrame with word counts\n    count_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return count_df", "raw_solution": "```python\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = [\n    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n    'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n    'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n    'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n    'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n    'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n    'further', 'then', 'once'\n]\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Preprocesses text data by removing stopwords, numbers, and punctuation,\n    then vectorizes the text into a count-based numeric format.\n\n    Parameters:\n    - dataframe (pd.DataFrame): The input DataFrame containing the text data.\n    - text_column (str): The name of the column containing text to be processed.\n\n    Returns:\n    - pd.DataFrame: A DataFrame where each column represents a word and each row\n                    contains the count of each word in the corresponding text entry.\n    \"\"\"\n    \n    # Function to preprocess individual text entries\n    def preprocess(text):\n        # Convert to lowercase\n        text = text.lower()\n        # Remove numbers\n        text = re.sub(r'\\d+', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Tokenize text\n        tokens = text.split()\n        # Remove stopwords\n        tokens = [word for word in tokens if word not in STOPWORDS]\n        # Join tokens back to string\n        return ' '.join(tokens)\n    \n    # Apply preprocessing to the specified text column\n    dataframe['processed_text'] = dataframe[text_column].astype(str).apply(preprocess)\n    \n    # Initialize CountVectorizer\n    vectorizer = CountVectorizer()\n    \n    # Fit and transform the processed text data\n    X = vectorizer.fit_transform(dataframe['processed_text'])\n    \n    # Create a DataFrame with word counts\n    count_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n    \n    return count_df\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    data = {\n        'id': [1, 2, 3],\n        'text': [\n            \"Hello, this is a sample text! It contains numbers like 123 and punctuation.\",\n            \"Another example: testing the vectorization process.\",\n            \"Data preprocessing is crucial for machine learning models.\"\n        ]\n    }\n    \n    df = pd.DataFrame(data)\n    \n    # Apply the task_func\n    word_counts = task_func(df, 'text')\n    \n    print(word_counts)\n```"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate that 'Lon' and 'Lat' keys are present\n    required_keys = ['Lon', 'Lat']\n    for key in required_keys:\n        if key not in dic:\n            raise ValueError(f\"Missing '{key}' key in the dictionary.\")\n        if not isinstance(dic[key], tuple):\n            raise ValueError(f\"The value for '{key}' must be a tuple.\")\n        if len(dic[key]) != 2:\n            raise ValueError(f\"The tuple for '{key}' must have exactly two elements.\")\n    \n    # Extract longitude and latitude ranges\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n    \n    # Generate random coordinates for each city\n    lons = np.random.uniform(lon_min, lon_max, size=len(cities))\n    lats = np.random.uniform(lat_min, lat_max, size=len(cities))\n    coordinates = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n    \n    # Create GeoDataFrame\n    gdf = gpd.GeoDataFrame({\n        'City': cities,\n        'Coordinates': coordinates\n    }, geometry='Coordinates')\n    \n    return gdf", "raw_solution": "```python\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Validate that 'Lon' and 'Lat' keys are present\n    required_keys = ['Lon', 'Lat']\n    for key in required_keys:\n        if key not in dic:\n            raise ValueError(f\"Missing '{key}' key in the dictionary.\")\n        if not isinstance(dic[key], tuple):\n            raise ValueError(f\"The value for '{key}' must be a tuple.\")\n        if len(dic[key]) != 2:\n            raise ValueError(f\"The tuple for '{key}' must have exactly two elements.\")\n    \n    # Extract longitude and latitude ranges\n    lon_min, lon_max = dic['Lon']\n    lat_min, lat_max = dic['Lat']\n    \n    # Generate random coordinates for each city\n    lons = np.random.uniform(lon_min, lon_max, size=len(cities))\n    lats = np.random.uniform(lat_min, lat_max, size=len(cities))\n    coordinates = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n    \n    # Create GeoDataFrame\n    gdf = gpd.GeoDataFrame({\n        'City': cities,\n        'Coordinates': coordinates\n    }, geometry='Coordinates')\n    \n    return gdf\n\n# Example usage\nif __name__ == \"__main__\":\n    cities = ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n    coordinate_ranges = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    \n    try:\n        geo_df = task_func(dic=coordinate_ranges, cities=cities)\n        print(geo_df)\n    except ValueError as e:\n        print(e)\n```"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n\n    Parameters:\n    - utc_datetime (datetime): The UTC datetime for the weather report.\n    - cities (list of str): List of city names.\n    - weather_conditions (list of str): List of possible weather conditions.\n    - timezones (dict): Mapping of city names to timezone strings.\n    - seed (int): Seed for random number generator.\n\n    Returns:\n    - pandas.DataFrame: DataFrame containing the weather report.\n\n    Raises:\n    - ValueError: If utc_datetime is not a datetime object or if parameters are not in expected format.\n    \"\"\"\n    # Validate utc_datetime\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object.\")\n    \n    # Validate cities\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings.\")\n    \n    # Validate weather_conditions\n    if not isinstance(weather_conditions, list) or not all(isinstance(cond, str) for cond in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings.\")\n    \n    # Validate timezones\n    if not isinstance(timezones, dict) or not all(isinstance(k, str) and isinstance(v, str) for k, v in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and string values.\")\n    \n    # Validate seed\n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer.\")\n    \n    # Initialize random seed\n    set_seed(seed)\n    \n    # Ensure utc_datetime is timezone-aware in UTC\n    if utc_datetime.tzinfo is None:\n        utc = pytz.UTC\n        utc_datetime = utc.localize(utc_datetime)\n    else:\n        utc_datetime = utc_datetime.astimezone(pytz.UTC)\n    \n    report_data = {\n        'City': [],\n        'Local Time': [],\n        'Weather Condition': []\n    }\n    \n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone for city '{city}' is not defined in timezones dictionary.\")\n        \n        timezone_str = timezones[city]\n        try:\n            timezone = pytz.timezone(timezone_str)\n        except pytz.UnknownTimeZoneError:\n            raise ValueError(f\"Unknown timezone '{timezone_str}' for city '{city}'.\")\n        \n        local_time = utc_datetime.astimezone(timezone)\n        timezone_abbr = local_time.tzname()\n        formatted_local_time = local_time.strftime(f\"%Y-%m-%d %H:%M:%S {timezone_abbr}\")\n        \n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        report_data['City'].append(city)\n        report_data['Local Time'].append(formatted_local_time)\n        report_data['Weather Condition'].append(weather_condition)\n    \n    df = pd.DataFrame(report_data)\n    return df", "raw_solution": "```python\nimport pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate a weather report for specified cities at a given UTC datetime.\n\n    Parameters:\n    - utc_datetime (datetime): The UTC datetime for the weather report.\n    - cities (list of str): List of city names.\n    - weather_conditions (list of str): List of possible weather conditions.\n    - timezones (dict): Mapping of city names to timezone strings.\n    - seed (int): Seed for random number generator.\n\n    Returns:\n    - pandas.DataFrame: DataFrame containing the weather report.\n\n    Raises:\n    - ValueError: If utc_datetime is not a datetime object or if parameters are not in expected format.\n    \"\"\"\n    # Validate utc_datetime\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object.\")\n    \n    # Validate cities\n    if not isinstance(cities, list) or not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings.\")\n    \n    # Validate weather_conditions\n    if not isinstance(weather_conditions, list) or not all(isinstance(cond, str) for cond in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings.\")\n    \n    # Validate timezones\n    if not isinstance(timezones, dict) or not all(isinstance(k, str) and isinstance(v, str) for k, v in timezones.items()):\n        raise ValueError(\"timezones must be a dictionary with string keys and string values.\")\n    \n    # Validate seed\n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer.\")\n    \n    # Initialize random seed\n    set_seed(seed)\n    \n    # Ensure utc_datetime is timezone-aware in UTC\n    if utc_datetime.tzinfo is None:\n        utc = pytz.UTC\n        utc_datetime = utc.localize(utc_datetime)\n    else:\n        utc_datetime = utc_datetime.astimezone(pytz.UTC)\n    \n    report_data = {\n        'City': [],\n        'Local Time': [],\n        'Weather Condition': []\n    }\n    \n    for city in cities:\n        if city not in timezones:\n            raise ValueError(f\"Timezone for city '{city}' is not defined in timezones dictionary.\")\n        \n        timezone_str = timezones[city]\n        try:\n            timezone = pytz.timezone(timezone_str)\n        except pytz.UnknownTimeZoneError:\n            raise ValueError(f\"Unknown timezone '{timezone_str}' for city '{city}'.\")\n        \n        local_time = utc_datetime.astimezone(timezone)\n        timezone_abbr = local_time.tzname()\n        formatted_local_time = local_time.strftime(f\"%Y-%m-%d %H:%M:%S {timezone_abbr}\")\n        \n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        \n        report_data['City'].append(city)\n        report_data['Local Time'].append(formatted_local_time)\n        report_data['Weather Condition'].append(weather_condition)\n    \n    df = pd.DataFrame(report_data)\n    return df\n```"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generates and plots a random walk based on a sequence of steps (-1 or 1).\n    \n    Parameters:\n    - elements (int): Number of steps in the random walk. Must be a positive integer.\n    - seed (int, optional): Seed for the random number generator. Default is 0.\n    \n    Returns:\n    - dict: Descriptive statistics of the random walk.\n    - matplotlib.axes.Axes: The Axes object with the plotted random walk.\n    \n    Raises:\n    - ValueError: If 'elements' is not a positive integer.\n    \"\"\"\n    # Validate input\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random steps: -1 or 1\n    steps = np.random.choice([-1, 1], size=elements)\n    \n    # Compute the random walk by cumulatively summing the steps\n    walk = np.cumsum(steps)\n    \n    # Create a plot of the random walk\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(walk, color='blue', linewidth=1)\n    ax.set_title('Random Walk', fontsize=14)\n    ax.set_xlabel('Step', fontsize=12)\n    ax.set_ylabel('Position', fontsize=12)\n    ax.grid(True)\n    \n    # Compute descriptive statistics\n    walk_series = pd.Series(walk)\n    descriptive = walk_series.describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    \n    stats = {\n        'count': int(descriptive['count']),\n        'mean': descriptive['mean'],\n        'std': descriptive['std'],\n        'min': descriptive['min'],\n        '5%': descriptive['5%'],\n        '25%': descriptive['25%'],\n        'median': descriptive['50%'],\n        '75%': descriptive['75%'],\n        '95%': descriptive['95%'],\n        'max': descriptive['max']\n    }\n    \n    return stats, ax", "raw_solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generates and plots a random walk based on a sequence of steps (-1 or 1).\n    \n    Parameters:\n    - elements (int): Number of steps in the random walk. Must be a positive integer.\n    - seed (int, optional): Seed for the random number generator. Default is 0.\n    \n    Returns:\n    - dict: Descriptive statistics of the random walk.\n    - matplotlib.axes.Axes: The Axes object with the plotted random walk.\n    \n    Raises:\n    - ValueError: If 'elements' is not a positive integer.\n    \"\"\"\n    # Validate input\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer.\")\n    \n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate random steps: -1 or 1\n    steps = np.random.choice([-1, 1], size=elements)\n    \n    # Compute the random walk by cumulatively summing the steps\n    walk = np.cumsum(steps)\n    \n    # Create a plot of the random walk\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(walk, color='blue', linewidth=1)\n    ax.set_title('Random Walk', fontsize=14)\n    ax.set_xlabel('Step', fontsize=12)\n    ax.set_ylabel('Position', fontsize=12)\n    ax.grid(True)\n    \n    # Compute descriptive statistics\n    walk_series = pd.Series(walk)\n    descriptive = walk_series.describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95])\n    \n    stats = {\n        'count': int(descriptive['count']),\n        'mean': descriptive['mean'],\n        'std': descriptive['std'],\n        'min': descriptive['min'],\n        '5%': descriptive['5%'],\n        '25%': descriptive['25%'],\n        'median': descriptive['50%'],\n        '75%': descriptive['75%'],\n        '95%': descriptive['95%'],\n        'max': descriptive['max']\n    }\n    \n    return stats, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    elements = 1000\n    stats, axes = task_func(elements)\n    print(\"Descriptive Statistics:\")\n    for key, value in stats.items():\n        print(f\"{key}: {value}\")\n    plt.show()\n```"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from the specified URL, extracts its contents to the given directory,\n    and returns a list of extracted filenames.\n\n    Parameters:\n        url (str): The URL to download the zip file from.\n        destination_directory (str): The directory to extract the zip contents into.\n        headers (dict, optional): Optional headers to include in the download request.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n\n    zip_path = os.path.join(destination_directory, 'downloaded.zip')\n\n    # Download the zip file\n    with requests.get(url, headers=headers, stream=True) as response:\n        response.raise_for_status()  # Raise an error for bad status codes\n        with open(zip_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:  # Filter out keep-alive chunks\n                    file.write(chunk)\n\n    extracted_files = []\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        # Get list of extracted file paths\n        for file in zip_ref.namelist():\n            extracted_file_path = os.path.join(destination_directory, file)\n            # If it's a file, add to the list\n            if os.path.isfile(extracted_file_path):\n                extracted_files.append(extracted_file_path)\n\n    # Remove the downloaded zip file\n    os.remove(zip_path)\n\n    return extracted_files", "raw_solution": "```python\nimport requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Downloads a zip file from the specified URL, extracts its contents to the given directory,\n    and returns a list of extracted filenames.\n\n    Parameters:\n        url (str): The URL to download the zip file from.\n        destination_directory (str): The directory to extract the zip contents into.\n        headers (dict, optional): Optional headers to include in the download request.\n\n    Returns:\n        list: A list of filenames of the extracted files.\n    \"\"\"\n    # Ensure the destination directory exists\n    os.makedirs(destination_directory, exist_ok=True)\n\n    zip_path = os.path.join(destination_directory, 'downloaded.zip')\n\n    # Download the zip file\n    with requests.get(url, headers=headers, stream=True) as response:\n        response.raise_for_status()  # Raise an error for bad status codes\n        with open(zip_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:  # Filter out keep-alive chunks\n                    file.write(chunk)\n\n    extracted_files = []\n\n    # Extract the zip file\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n        # Get list of extracted file paths\n        for file in zip_ref.namelist():\n            extracted_file_path = os.path.join(destination_directory, file)\n            # If it's a file, add to the list\n            if os.path.isfile(extracted_file_path):\n                extracted_files.append(extracted_file_path)\n\n    # Remove the downloaded zip file\n    os.remove(zip_path)\n\n    return extracted_files\n```"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generates a random RGB image and displays it.\n\n    Parameters:\n        seed (int): Seed for random number generator.\n        image_size (tuple): Size of the image in the format (height, width, channels).\n        range_low (int): Lower bound for random pixel values.\n        range_high (int): Upper bound for random pixel values.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axes object of the plot.\n        image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n        ValueError: If range_low is not less than range_high.\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high.\")\n\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    random.seed(seed)\n\n    # Generate random image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Create a matplotlib figure and axes\n    fig, ax = plt.subplots()\n    ax.axis('off')  # Hide the axes\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Show the plot\n    plt.show()\n\n    return ax, image", "raw_solution": "```python\nimport random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generates a random RGB image and displays it.\n\n    Parameters:\n        seed (int): Seed for random number generator.\n        image_size (tuple): Size of the image in the format (height, width, channels).\n        range_low (int): Lower bound for random pixel values.\n        range_high (int): Upper bound for random pixel values.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axes object of the plot.\n        image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n        ValueError: If range_low is not less than range_high.\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high.\")\n\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    random.seed(seed)\n\n    # Generate random image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Create a matplotlib figure and axes\n    fig, ax = plt.subplots()\n    ax.axis('off')  # Hide the axes\n\n    # Display the image\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Show the plot\n    plt.show()\n\n    return ax, image\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        ax, img = task_func()\n    except ValueError as ve:\n        print(f\"ValueError: {ve}\")\n\n    # Example that raises ValueError\n    try:\n        ax, img = task_func(range_low=300, range_high=100)\n    except ValueError as ve:\n        print(f\"ValueError: {ve}\")\n```"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from list L, normalizes it based on the SPL of the specified audio file,\n    and generates a spectrogram from the normalized matrix.\n\n    Parameters:\n        L (list): Input list of numerical values.\n        M (int): Number of rows for the matrix.\n        N (int): Number of columns for the matrix.\n        audio_file (str): Path to the audio file.\n\n    Returns:\n        tuple:\n            - normalized_matrix (numpy.ndarray): The normalized MxN matrix.\n            - fig (matplotlib.figure.Figure): The figure object for the generated spectrogram.\n\n    Raises:\n        FileNotFoundError: If the specified audio file does not exist.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file '{audio_file}' does not exist.\")\n\n    # Read audio data\n    data, samplerate = sf.read(audio_file)\n    \n    # If stereo, convert to mono by averaging channels\n    if data.ndim > 1:\n        data = np.mean(data, axis=1)\n    \n    # Calculate SPL\n    mean_square = np.mean(data**2)\n    if mean_square == 0:\n        raise ValueError(\"The audio file contains silence.\")\n    spl = 20 * np.log10(np.sqrt(mean_square))\n    \n    # Create MxN matrix from list L\n    total_elements = M * N\n    if len(L) < total_elements:\n        # Pad with zeros if L is shorter than M*N\n        padded_L = L + [0]*(total_elements - len(L))\n    else:\n        # Truncate L if it's longer than M*N\n        padded_L = L[:total_elements]\n    matrix = np.array(padded_L).reshape(M, N)\n    \n    # Normalize the matrix based on SPL\n    # Here, we scale the matrix so that its maximum amplitude corresponds to the SPL\n    max_val = np.max(np.abs(matrix))\n    if max_val == 0:\n        raise ValueError(\"The input list results in a matrix with all zero values.\")\n    normalized_matrix = (matrix / max_val) * spl\n    \n    # Generate spectrogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Create time and frequency axes\n    time = np.linspace(0, N, N)\n    # Assuming a typical audio frequency range from 20 Hz to 20 kHz\n    freq = np.linspace(20, 20000, M)\n    \n    # Use pcolormesh for better handling of the axes\n    X, Y = np.meshgrid(time, freq)\n    # Plot the normalized matrix; transpose to align axes correctly\n    c = ax.pcolormesh(X, Y, normalized_matrix, shading='auto', cmap='viridis')\n    \n    # Set logarithmic scale for frequency\n    ax.set_yscale('log')\n    ax.set_ylim([20, 20000])\n    \n    # Set labels\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency (Hz)')\n    ax.set_title('Spectrogram')\n    \n    # Add a colorbar to indicate amplitude in SPL\n    fig.colorbar(c, ax=ax, label='Amplitude (dB SPL)')\n    \n    plt.tight_layout()\n    \n    return normalized_matrix, fig", "raw_solution": "```python\n```python\nimport numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates an MxN matrix from list L, normalizes it based on the SPL of the specified audio file,\n    and generates a spectrogram from the normalized matrix.\n\n    Parameters:\n        L (list): Input list of numerical values.\n        M (int): Number of rows for the matrix.\n        N (int): Number of columns for the matrix.\n        audio_file (str): Path to the audio file.\n\n    Returns:\n        tuple:\n            - normalized_matrix (numpy.ndarray): The normalized MxN matrix.\n            - fig (matplotlib.figure.Figure): The figure object for the generated spectrogram.\n\n    Raises:\n        FileNotFoundError: If the specified audio file does not exist.\n    \"\"\"\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The audio file '{audio_file}' does not exist.\")\n\n    # Read audio data\n    data, samplerate = sf.read(audio_file)\n    \n    # If stereo, convert to mono by averaging channels\n    if data.ndim > 1:\n        data = np.mean(data, axis=1)\n    \n    # Calculate SPL\n    mean_square = np.mean(data**2)\n    if mean_square == 0:\n        raise ValueError(\"The audio file contains silence.\")\n    spl = 20 * np.log10(np.sqrt(mean_square))\n    \n    # Create MxN matrix from list L\n    total_elements = M * N\n    if len(L) < total_elements:\n        # Pad with zeros if L is shorter than M*N\n        padded_L = L + [0]*(total_elements - len(L))\n    else:\n        # Truncate L if it's longer than M*N\n        padded_L = L[:total_elements]\n    matrix = np.array(padded_L).reshape(M, N)\n    \n    # Normalize the matrix based on SPL\n    # Here, we scale the matrix so that its maximum amplitude corresponds to the SPL\n    max_val = np.max(np.abs(matrix))\n    if max_val == 0:\n        raise ValueError(\"The input list results in a matrix with all zero values.\")\n    normalized_matrix = (matrix / max_val) * spl\n    \n    # Generate spectrogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Create time and frequency axes\n    time = np.linspace(0, N, N)\n    # Assuming a typical audio frequency range from 20 Hz to 20 kHz\n    freq = np.linspace(20, 20000, M)\n    \n    # Use pcolormesh for better handling of the axes\n    X, Y = np.meshgrid(time, freq)\n    # Plot the normalized matrix; transpose to align axes correctly\n    c = ax.pcolormesh(X, Y, normalized_matrix, shading='auto', cmap='viridis')\n    \n    # Set logarithmic scale for frequency\n    ax.set_yscale('log')\n    ax.set_ylim([20, 20000])\n    \n    # Set labels\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency (Hz)')\n    ax.set_title('Spectrogram')\n    \n    # Add a colorbar to indicate amplitude in SPL\n    fig.colorbar(c, ax=ax, label='Amplitude (dB SPL)')\n    \n    plt.tight_layout()\n    \n    return normalized_matrix, fig\n```"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    \"\"\"\n    Extract numeric values from a list of tuples, compute basic statistics,\n    and generate a histogram with an overlaid probability density function (PDF).\n\n    Parameters:\n        original (list of tuples): The input list containing tuples with numeric and non-numeric values.\n\n    Returns:\n        np.ndarray: Array of extracted numeric values.\n        dict: Basic statistics including mean, standard deviation, minimum, and maximum.\n        matplotlib.axes.Axes: Axes object with the histogram and overlaid PDF.\n    \"\"\"\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([\n        item for tup in original for item in tup\n        if isinstance(item, (int, float, np.number))\n    ])\n\n    if numeric_values.size == 0:\n        raise ValueError(\"No numeric values found in the input data.\")\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std': np.std(numeric_values, ddof=1),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Create a matplotlib figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(numeric_values, bins='auto', density=True, alpha=0.6, color='skyblue', edgecolor='black')\n\n    # Define the range for the PDF\n    xmin, xmax = ax.get_xlim()\n    x_values = np.linspace(xmin, xmax, 1000)\n\n    # Calculate the PDF using the normal distribution with the computed mean and std\n    pdf_values = stats.norm.pdf(x_values, stats_dict['mean'], stats_dict['std'])\n\n    # Plot the PDF\n    ax.plot(x_values, pdf_values, 'r-', linewidth=2, label='Normal PDF')\n\n    # Add titles and labels\n    ax.set_title('Histogram with Overlaid Normal PDF')\n    ax.set_xlabel('Numeric Values')\n    ax.set_ylabel('Density')\n    ax.legend()\n\n    # Tight layout for better spacing\n    fig.tight_layout()\n\n    return numeric_values, stats_dict, ax", "raw_solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    \"\"\"\n    Extract numeric values from a list of tuples, compute basic statistics,\n    and generate a histogram with an overlaid probability density function (PDF).\n\n    Parameters:\n        original (list of tuples): The input list containing tuples with numeric and non-numeric values.\n\n    Returns:\n        np.ndarray: Array of extracted numeric values.\n        dict: Basic statistics including mean, standard deviation, minimum, and maximum.\n        matplotlib.axes.Axes: Axes object with the histogram and overlaid PDF.\n    \"\"\"\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([\n        item for tup in original for item in tup\n        if isinstance(item, (int, float, np.number))\n    ])\n\n    if numeric_values.size == 0:\n        raise ValueError(\"No numeric values found in the input data.\")\n\n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std': np.std(numeric_values, ddof=1),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n\n    # Create a matplotlib figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot the histogram\n    ax.hist(numeric_values, bins='auto', density=True, alpha=0.6, color='skyblue', edgecolor='black')\n\n    # Define the range for the PDF\n    xmin, xmax = ax.get_xlim()\n    x_values = np.linspace(xmin, xmax, 1000)\n\n    # Calculate the PDF using the normal distribution with the computed mean and std\n    pdf_values = stats.norm.pdf(x_values, stats_dict['mean'], stats_dict['std'])\n\n    # Plot the PDF\n    ax.plot(x_values, pdf_values, 'r-', linewidth=2, label='Normal PDF')\n\n    # Add titles and labels\n    ax.set_title('Histogram with Overlaid Normal PDF')\n    ax.set_xlabel('Numeric Values')\n    ax.set_ylabel('Density')\n    ax.legend()\n\n    # Tight layout for better spacing\n    fig.tight_layout()\n\n    return numeric_values, stats_dict, ax\n```"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    \"\"\"\n    Converts a list to a numpy array, normalizes it, and plots the original and normalized data.\n    \n    Parameters:\n        original (list or array-like): The original data list.\n        \n    Returns:\n        np.array: Numpy array of original data.\n        np.array: Numpy array of normalized data.\n        matplotlib.axes.Axes: Axes object with the plotted data.\n    \"\"\"\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Reshape the array for the scaler\n    original_array_reshaped = original_array.reshape(-1, 1)\n    \n    # Normalize the array using MinMaxScaler\n    scaler = preprocessing.MinMaxScaler()\n    normalized_array = scaler.fit_transform(original_array_reshaped).flatten()\n    \n    # Create a plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(original_array, label='Original Data', marker='o')\n    ax.plot(normalized_array, label='Normalized Data', marker='x')\n    \n    # Set title and labels\n    ax.set_title('Original vs. Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    \n    # Add legend\n    ax.legend()\n    \n    # Display the plot\n    plt.show()\n    \n    return original_array, normalized_array, ax", "raw_solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    \"\"\"\n    Converts a list to a numpy array, normalizes it, and plots the original and normalized data.\n    \n    Parameters:\n        original (list or array-like): The original data list.\n        \n    Returns:\n        np.array: Numpy array of original data.\n        np.array: Numpy array of normalized data.\n        matplotlib.axes.Axes: Axes object with the plotted data.\n    \"\"\"\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Reshape the array for the scaler\n    original_array_reshaped = original_array.reshape(-1, 1)\n    \n    # Normalize the array using MinMaxScaler\n    scaler = preprocessing.MinMaxScaler()\n    normalized_array = scaler.fit_transform(original_array_reshaped).flatten()\n    \n    # Create a plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(original_array, label='Original Data', marker='o')\n    ax.plot(normalized_array, label='Normalized Data', marker='x')\n    \n    # Set title and labels\n    ax.set_title('Original vs. Normalized Data')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    \n    # Add legend\n    ax.legend()\n    \n    # Display the plot\n    plt.show()\n    \n    return original_array, normalized_array, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    original_data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n    original_array, normalized_array, axes = task_func(original_data)\n    \n    print(\"Original Array:\", original_array)\n    print(\"Normalized Array:\", normalized_array)\n```"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    Processes the input dictionary by adding a new key, generating a signal,\n    performing FFT, and plotting the result.\n\n    Parameters:\n        data (dict): The input dictionary containing numerical values.\n        sample_rate (int, optional): The sampling rate of the signal. Defaults to 8000.\n\n    Returns:\n        tuple: A tuple containing:\n            - ndarray: The FFT of the signal.\n            - Axes: The plot of the FFT.\n    \"\"\"\n    # 1. Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # 2. Generate a signal based on the values in \"data\"\n    # Assuming the signal is a simple array of the dictionary's values\n    signal = np.array(list(data.values()), dtype=float)\n\n    # Number of sample points\n    N = len(signal)\n\n    # 3. Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    fft_magnitude = np.abs(fft_result) / N  # Normalize the amplitude\n\n    # Frequency bins\n    freqs = fftpack.fftfreq(N, d=1.0/sample_rate)\n\n    # Only take the positive half of the spectrum\n    positive_freqs = freqs[:N//2]\n    positive_magnitude = 2.0 * fft_magnitude[:N//2]\n\n    # 4. Plot the FFT of the signal\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(positive_freqs, positive_magnitude)\n    ax.set_title('FFT of the signal')\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n    ax.grid(True)\n\n    plt.show()\n\n    return (fft_result, ax)", "raw_solution": "```python\nimport numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    Processes the input dictionary by adding a new key, generating a signal,\n    performing FFT, and plotting the result.\n\n    Parameters:\n        data (dict): The input dictionary containing numerical values.\n        sample_rate (int, optional): The sampling rate of the signal. Defaults to 8000.\n\n    Returns:\n        tuple: A tuple containing:\n            - ndarray: The FFT of the signal.\n            - Axes: The plot of the FFT.\n    \"\"\"\n    # 1. Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n\n    # 2. Generate a signal based on the values in \"data\"\n    # Assuming the signal is a simple array of the dictionary's values\n    signal = np.array(list(data.values()), dtype=float)\n\n    # Number of sample points\n    N = len(signal)\n\n    # 3. Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    fft_magnitude = np.abs(fft_result) / N  # Normalize the amplitude\n\n    # Frequency bins\n    freqs = fftpack.fftfreq(N, d=1.0/sample_rate)\n\n    # Only take the positive half of the spectrum\n    positive_freqs = freqs[:N//2]\n    positive_magnitude = 2.0 * fft_magnitude[:N//2]\n\n    # 4. Plot the FFT of the signal\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(positive_freqs, positive_magnitude)\n    ax.set_title('FFT of the signal')\n    ax.set_xlabel('Frequency (Hz)')\n    ax.set_ylabel('Amplitude')\n    ax.grid(True)\n\n    plt.show()\n\n    return (fft_result, ax)\n\nif __name__ == \"__main__\":\n    # Example usage\n    data = {\n        'x': 0,\n        'y': 1,\n        'z': 0.5,\n        'w': 0.25\n    }\n\n    fft_output, fft_axes = task_func(data)\n    print(\"FFT Output:\", fft_output)\n```"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\ndef task_func():\n    class PostRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Check Content-Type header\n            content_type = self.headers.get('Content-Type')\n            if content_type != 'application/json':\n                self.send_response(400)\n                response = {\"status\": \"error\", \"message\": \"Content-Type header is not application/json\"}\n                self.send_header('Content-Type', 'application/json')\n                response_bytes = json.dumps(response).encode('utf-8')\n                self.send_header('Content-Length', str(len(response_bytes)))\n                self.end_headers()\n                self.wfile.write(response_bytes)\n                return\n\n            # Read and parse JSON data\n            content_length = int(self.headers.get('Content-Length', 0))\n            try:\n                post_data = self.rfile.read(content_length)\n                data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                response = {\"status\": \"error\", \"message\": \"Invalid JSON\"}\n                self.send_header('Content-Type', 'application/json')\n                response_bytes = json.dumps(response).encode('utf-8')\n                self.send_header('Content-Length', str(len(response_bytes)))\n                self.end_headers()\n                self.wfile.write(response_bytes)\n                return\n\n            # Check for 'data' key in JSON\n            if 'data' not in data:\n                self.send_response(400)\n                response = {\"status\": \"error\", \"message\": \"No data key in request\"}\n                self.send_header('Content-Type', 'application/json')\n                response_bytes = json.dumps(response).encode('utf-8')\n                self.send_header('Content-Length', str(len(response_bytes)))\n                self.end_headers()\n                self.wfile.write(response_bytes)\n                return\n\n            # If all checks pass, send success response\n            self.send_response(200)\n            response = SUCCESS_RESPONSE\n            self.send_header('Content-Type', 'application/json')\n            response_bytes = json.dumps(response).encode('utf-8')\n            self.send_header('Content-Length', str(len(response_bytes)))\n            self.end_headers()\n            self.wfile.write(response_bytes)\n\n        def log_message(self, format, *args):\n            # Override to prevent printing logs to stdout\n            return\n\n    # Set up and start the server\n    server_address = ('', 8080)  # Listen on all interfaces, port 8080\n    httpd = http.server.HTTPServer(server_address, PostRequestHandler)\n    print('Starting server on port 8080...')\n    try:\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        print('\\nServer is shutting down.')\n        httpd.server_close()", "raw_solution": "```python\nimport cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    class PostRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            # Check Content-Type header\n            content_type = self.headers.get('Content-Type')\n            if content_type != 'application/json':\n                self.send_response(400)\n                response = {\"status\": \"error\", \"message\": \"Content-Type header is not application/json\"}\n                self.send_header('Content-Type', 'application/json')\n                response_bytes = json.dumps(response).encode('utf-8')\n                self.send_header('Content-Length', str(len(response_bytes)))\n                self.end_headers()\n                self.wfile.write(response_bytes)\n                return\n\n            # Read and parse JSON data\n            content_length = int(self.headers.get('Content-Length', 0))\n            try:\n                post_data = self.rfile.read(content_length)\n                data = json.loads(post_data)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                response = {\"status\": \"error\", \"message\": \"Invalid JSON\"}\n                self.send_header('Content-Type', 'application/json')\n                response_bytes = json.dumps(response).encode('utf-8')\n                self.send_header('Content-Length', str(len(response_bytes)))\n                self.end_headers()\n                self.wfile.write(response_bytes)\n                return\n\n            # Check for 'data' key in JSON\n            if 'data' not in data:\n                self.send_response(400)\n                response = {\"status\": \"error\", \"message\": \"No data key in request\"}\n                self.send_header('Content-Type', 'application/json')\n                response_bytes = json.dumps(response).encode('utf-8')\n                self.send_header('Content-Length', str(len(response_bytes)))\n                self.end_headers()\n                self.wfile.write(response_bytes)\n                return\n\n            # If all checks pass, send success response\n            self.send_response(200)\n            response = SUCCESS_RESPONSE\n            self.send_header('Content-Type', 'application/json')\n            response_bytes = json.dumps(response).encode('utf-8')\n            self.send_header('Content-Length', str(len(response_bytes)))\n            self.end_headers()\n            self.wfile.write(response_bytes)\n\n        def log_message(self, format, *args):\n            # Override to prevent printing logs to stdout\n            return\n\n    # Set up and start the server\n    server_address = ('', 8080)  # Listen on all interfaces, port 8080\n    httpd = http.server.HTTPServer(server_address, PostRequestHandler)\n    print('Starting server on port 8080...')\n    try:\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        print('\\nServer is shutting down.')\n        httpd.server_close()\n\nif __name__ == '__main__':\n    task_func()\n```"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                # Determine the length of the incoming data\n                content_length = int(self.headers.get('Content-Length', 0))\n                content_type = self.headers.get('Content-Type', '')\n                \n                # Read and decode the incoming data\n                post_data = self.rfile.read(content_length)\n                try:\n                    email_data = json.loads(post_data)\n                except json.JSONDecodeError:\n                    self.send_response(400, 'Bad Request')\n                    response = b'Invalid JSON data.'\n                    self.send_header('Content-Type', 'text/plain')\n                    self.send_header('Content-Length', str(len(response)))\n                    self.end_headers()\n                    self.wfile.write(response)\n                    return\n\n                # Validate required fields\n                required_fields = ['subject', 'message', 'to']\n                if not all(field in email_data for field in required_fields):\n                    self.send_response(400, 'Bad Request')\n                    response = b'Missing one or more required fields: subject, message, to.'\n                    self.send_header('Content-Type', 'text/plain')\n                    self.send_header('Content-Length', str(len(response)))\n                    self.end_headers()\n                    self.wfile.write(response)\n                    return\n\n                subject = email_data['subject']\n                message = email_data['message']\n                to_address = email_data['to']\n\n                # Create the email message\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to_address\n\n                # Send the email\n                try:\n                    with smtplib.SMTP(smtp_server, smtp_port) as server:\n                        server.starttls()\n                        server.login(smtp_username, smtp_password)\n                        server.sendmail(smtp_username, [to_address], msg.as_string())\n                except smtplib.SMTPAuthenticationError:\n                    self.send_response(535, 'Authentication Failed')\n                    response = b'SMTP authentication failed.'\n                    self.send_header('Content-Type', 'text/plain')\n                    self.send_header('Content-Length', str(len(response)))\n                    self.end_headers()\n                    self.wfile.write(response)\n                    return\n                except smtplib.SMTPException as e:\n                    self.send_response(500, 'Internal Server Error')\n                    response = f'SMTP error occurred: {str(e)}'.encode()\n                    self.send_header('Content-Type', 'text/plain')\n                    self.send_header('Content-Length', str(len(response)))\n                    self.end_headers()\n                    self.wfile.write(response)\n                    return\n\n                # If email sent successfully\n                self.send_response(200, 'OK')\n                response = b'Email sent successfully.'\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', str(len(response)))\n                self.end_headers()\n                self.wfile.write(response)\n\n            except Exception as e:\n                self.send_response(500, 'Internal Server Error')\n                response = f'Unexpected error: {str(e)}'.encode()\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', str(len(response)))\n                self.end_headers()\n                self.wfile.write(response)\n\n        def log_message(self, format, *args):\n            # Override to prevent printing to stderr\n            return\n\n    # Set up and start the HTTP server\n    server_address = ('', 8080)\n    httpd = http.server.HTTPServer(server_address, EmailRequestHandler)\n    print(f\"Starting server on port {server_address[1]}...\")\n    try:\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        print(\"\\nShutting down server.\")\n        httpd.server_close()", "raw_solution": "```python\nimport cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailRequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                # Determine the length of the incoming data\n                content_length = int(self.headers.get('Content-Length', 0))\n                content_type = self.headers.get('Content-Type', '')\n                \n                # Read and decode the incoming data\n                post_data = self.rfile.read(content_length)\n                try:\n                    email_data = json.loads(post_data)\n                except json.JSONDecodeError:\n                    self.send_response(400, 'Bad Request')\n                    response = b'Invalid JSON data.'\n                    self.send_header('Content-Type', 'text/plain')\n                    self.send_header('Content-Length', str(len(response)))\n                    self.end_headers()\n                    self.wfile.write(response)\n                    return\n\n                # Validate required fields\n                required_fields = ['subject', 'message', 'to']\n                if not all(field in email_data for field in required_fields):\n                    self.send_response(400, 'Bad Request')\n                    response = b'Missing one or more required fields: subject, message, to.'\n                    self.send_header('Content-Type', 'text/plain')\n                    self.send_header('Content-Length', str(len(response)))\n                    self.end_headers()\n                    self.wfile.write(response)\n                    return\n\n                subject = email_data['subject']\n                message = email_data['message']\n                to_address = email_data['to']\n\n                # Create the email message\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to_address\n\n                # Send the email\n                try:\n                    with smtplib.SMTP(smtp_server, smtp_port) as server:\n                        server.starttls()\n                        server.login(smtp_username, smtp_password)\n                        server.sendmail(smtp_username, [to_address], msg.as_string())\n                except smtplib.SMTPAuthenticationError:\n                    self.send_response(535, 'Authentication Failed')\n                    response = b'SMTP authentication failed.'\n                    self.send_header('Content-Type', 'text/plain')\n                    self.send_header('Content-Length', str(len(response)))\n                    self.end_headers()\n                    self.wfile.write(response)\n                    return\n                except smtplib.SMTPException as e:\n                    self.send_response(500, 'Internal Server Error')\n                    response = f'SMTP error occurred: {str(e)}'.encode()\n                    self.send_header('Content-Type', 'text/plain')\n                    self.send_header('Content-Length', str(len(response)))\n                    self.end_headers()\n                    self.wfile.write(response)\n                    return\n\n                # If email sent successfully\n                self.send_response(200, 'OK')\n                response = b'Email sent successfully.'\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', str(len(response)))\n                self.end_headers()\n                self.wfile.write(response)\n\n            except Exception as e:\n                self.send_response(500, 'Internal Server Error')\n                response = f'Unexpected error: {str(e)}'.encode()\n                self.send_header('Content-Type', 'text/plain')\n                self.send_header('Content-Length', str(len(response)))\n                self.end_headers()\n                self.wfile.write(response)\n\n        def log_message(self, format, *args):\n            # Override to prevent printing to stderr\n            return\n\n    # Set up and start the HTTP server\n    server_address = ('', 8080)\n    httpd = http.server.HTTPServer(server_address, EmailRequestHandler)\n    print(f\"Starting server on port {server_address[1]}...\")\n    try:\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        print(\"\\nShutting down server.\")\n        httpd.server_close()\n\nif __name__ == '__main__':\n    # Replace these with your SMTP server details\n    SMTP_SERVER = 'smtp.example.com'\n    SMTP_PORT = 587\n    SMTP_USERNAME = 'your_username@example.com'\n    SMTP_PASSWORD = 'your_password'\n\n    task_func(SMTP_SERVER, SMTP_PORT, SMTP_USERNAME, SMTP_PASSWORD)\n```"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    word_counts = {}\n    total_words = 0\n\n    # List all files in the specified directory\n    for file in os.listdir(directory):\n        if file.endswith('.txt'):\n            file_path = os.path.join(directory, file)\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    text = f.read()\n                    words = text.split()\n                    count = len(words)\n                    word_counts[file] = count\n                    total_words += count\n            except Exception as e:\n                print(f\"Error reading {file_path}: {e}\")\n\n    # Write the word counts to the JSON file\n    try:\n        with open(filename, 'w', encoding='utf-8') as json_file:\n            json.dump(word_counts, json_file, indent=4)\n    except Exception as e:\n        print(f\"Error writing to {filename}: {e}\")\n\n    return total_words", "raw_solution": "```python\nfrom collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    word_counts = {}\n    total_words = 0\n\n    # List all files in the specified directory\n    for file in os.listdir(directory):\n        if file.endswith('.txt'):\n            file_path = os.path.join(directory, file)\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    text = f.read()\n                    words = text.split()\n                    count = len(words)\n                    word_counts[file] = count\n                    total_words += count\n            except Exception as e:\n                print(f\"Error reading {file_path}: {e}\")\n\n    # Write the word counts to the JSON file\n    try:\n        with open(filename, 'w', encoding='utf-8') as json_file:\n            json.dump(word_counts, json_file, indent=4)\n    except Exception as e:\n        print(f\"Error writing to {filename}: {e}\")\n\n    return total_words\n\n# Example usage:\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) != 3:\n        print(\"Usage: python script.py output.json /path/to/directory\")\n    else:\n        output_file = sys.argv[1]\n        directory_path = sys.argv[2]\n        total = task_func(output_file, directory_path)\n        print(total)\n```"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nCOLUMNS = ['Date', 'Value']\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n\n    Parameters:\n    - df (pd.DataFrame): Input DataFrame with columns specified in COLUMNS.\n    - plot (bool): If True, generates and returns a heatmap of the correlation matrix.\n\n    Returns:\n    - corr_df (pd.DataFrame): DataFrame containing the Pearson correlation coefficients.\n    - ax (matplotlib.axes.Axes, optional): Axes object of the heatmap plot if plot is True.\n\n    Raises:\n    - ValueError: If the input DataFrame is empty or the 'Value' column contains invalid data.\n    \"\"\"\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n\n    # Check if required columns exist\n    if not all(col in df.columns for col in COLUMNS):\n        raise ValueError(f\"Input DataFrame must contain columns: {COLUMNS}\")\n\n    # Check if 'Value' column contains lists\n    if not df['Value'].apply(lambda x: isinstance(x, list)).all():\n        raise ValueError(\"All entries in the 'Value' column must be lists.\")\n\n    # Split the lists in 'Value' column into separate columns\n    try:\n        # Determine the maximum list length\n        max_length = df['Value'].apply(len).max()\n        # Create new column names\n        value_columns = [f'Value_{i+1}' for i in range(max_length)]\n        # Expand the lists into a new DataFrame\n        value_expanded = pd.DataFrame(df['Value'].tolist(), columns=value_columns)\n    except Exception as e:\n        raise ValueError(f\"Error processing 'Value' column: {e}\")\n\n    # Calculate the Pearson correlation matrix\n    try:\n        corr_df = value_expanded.corr(method='pearson')\n    except Exception as e:\n        raise ValueError(f\"Error calculating correlation matrix: {e}\")\n\n    # If plot is requested, generate the heatmap\n    if plot:\n        try:\n            plt.figure(figsize=(10, 8))\n            ax = sns.heatmap(corr_df, annot=True, fmt=\".2f\", cmap='coolwarm', square=True,\n                             cbar_kws={'shrink': .5})\n            ax.set_title(\"Correlation Heatmap\", fontsize=16)\n            plt.tight_layout()\n            return corr_df, ax\n        except Exception as e:\n            raise ValueError(f\"Error generating heatmap: {e}\")\n\n    return corr_df", "raw_solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n\n    Parameters:\n    - df (pd.DataFrame): Input DataFrame with columns specified in COLUMNS.\n    - plot (bool): If True, generates and returns a heatmap of the correlation matrix.\n\n    Returns:\n    - corr_df (pd.DataFrame): DataFrame containing the Pearson correlation coefficients.\n    - ax (matplotlib.axes.Axes, optional): Axes object of the heatmap plot if plot is True.\n\n    Raises:\n    - ValueError: If the input DataFrame is empty or the 'Value' column contains invalid data.\n    \"\"\"\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n\n    # Check if required columns exist\n    if not all(col in df.columns for col in COLUMNS):\n        raise ValueError(f\"Input DataFrame must contain columns: {COLUMNS}\")\n\n    # Check if 'Value' column contains lists\n    if not df['Value'].apply(lambda x: isinstance(x, list)).all():\n        raise ValueError(\"All entries in the 'Value' column must be lists.\")\n\n    # Split the lists in 'Value' column into separate columns\n    try:\n        # Determine the maximum list length\n        max_length = df['Value'].apply(len).max()\n        # Create new column names\n        value_columns = [f'Value_{i+1}' for i in range(max_length)]\n        # Expand the lists into a new DataFrame\n        value_expanded = pd.DataFrame(df['Value'].tolist(), columns=value_columns)\n    except Exception as e:\n        raise ValueError(f\"Error processing 'Value' column: {e}\")\n\n    # Calculate the Pearson correlation matrix\n    try:\n        corr_df = value_expanded.corr(method='pearson')\n    except Exception as e:\n        raise ValueError(f\"Error calculating correlation matrix: {e}\")\n\n    # If plot is requested, generate the heatmap\n    if plot:\n        try:\n            plt.figure(figsize=(10, 8))\n            ax = sns.heatmap(corr_df, annot=True, fmt=\".2f\", cmap='coolwarm', square=True,\n                             cbar_kws={'shrink': .5})\n            ax.set_title(\"Correlation Heatmap\", fontsize=16)\n            plt.tight_layout()\n            return corr_df, ax\n        except Exception as e:\n            raise ValueError(f\"Error generating heatmap: {e}\")\n\n    return corr_df\n```"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    \"\"\"\n    Generates a report on students' grades in different subjects and calculates\n    the average grade for each student and each subject.\n\n    Parameters:\n    additional_fields (list): Optional list of additional field names to include.\n\n    Returns:\n    pd.DataFrame: DataFrame containing students' grades, their average grades,\n                  and the average grade per subject.\n    \"\"\"\n    # Initialize the data dictionary with subjects\n    data = {}\n    for subject in FIELDS:\n        # Assign random grades between 0 and 100 for each student\n        data[subject] = [random.randint(0, 100) for _ in STUDENTS]\n    \n    # Create the DataFrame with students as the index\n    df = pd.DataFrame(data, index=STUDENTS)\n    \n    # If there are additional fields, add them with random data\n    for field in additional_fields:\n        # For simplicity, assign random integers between 0 and 100\n        df[field] = [random.randint(0, 100) for _ in STUDENTS]\n    \n    # Calculate 'Average Grade' for each student\n    df['Average Grade'] = df[FIELDS].mean(axis=1)\n    \n    # Calculate the average for each subject\n    average_row = df[FIELDS + ['Average Grade']].mean()\n    \n    # If there are additional fields, calculate their averages\n    if additional_fields:\n        additional_averages = df[additional_fields].mean()\n        # Append additional averages to the average_row\n        average_row = average_row.append(additional_averages)\n    \n    # Name the average row\n    average_row.name = 'Average'\n    \n    # Append the average row to the DataFrame\n    df = df.append(average_row)\n    \n    return df", "raw_solution": "```python\nimport pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    \"\"\"\n    Generates a report on students' grades in different subjects and calculates\n    the average grade for each student and each subject.\n\n    Parameters:\n    additional_fields (list): Optional list of additional field names to include.\n\n    Returns:\n    pd.DataFrame: DataFrame containing students' grades, their average grades,\n                  and the average grade per subject.\n    \"\"\"\n    # Initialize the data dictionary with subjects\n    data = {}\n    for subject in FIELDS:\n        # Assign random grades between 0 and 100 for each student\n        data[subject] = [random.randint(0, 100) for _ in STUDENTS]\n    \n    # Create the DataFrame with students as the index\n    df = pd.DataFrame(data, index=STUDENTS)\n    \n    # If there are additional fields, add them with random data\n    for field in additional_fields:\n        # For simplicity, assign random integers between 0 and 100\n        df[field] = [random.randint(0, 100) for _ in STUDENTS]\n    \n    # Calculate 'Average Grade' for each student\n    df['Average Grade'] = df[FIELDS].mean(axis=1)\n    \n    # Calculate the average for each subject\n    average_row = df[FIELDS + ['Average Grade']].mean()\n    \n    # If there are additional fields, calculate their averages\n    if additional_fields:\n        additional_averages = df[additional_fields].mean()\n        # Append additional averages to the average_row\n        average_row = average_row.append(additional_averages)\n    \n    # Name the average row\n    average_row.name = 'Average'\n    \n    # Append the average row to the DataFrame\n    df = df.append(average_row)\n    \n    return df\n\n# Example usage\nif __name__ == '__main__':\n    # Generate report without additional fields\n    report = task_func()\n    print(\"Report without additional fields:\")\n    print(report)\n    \n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n    \n    # Generate report with additional fields\n    additional = ['Age', 'Attendance']\n    report_with_additional = task_func(additional_fields=additional)\n    print(\"Report with additional fields:\")\n    print(report_with_additional)\n```\n\nThis script performs the following steps:\n\n1. **Imports Necessary Libraries:**\n   - `pandas` for data manipulation.\n   - `mean` from `statistics` for calculating averages.\n   - `random` for generating random grades.\n\n2. **Defines Constants:**\n   - `FIELDS`: The list of subjects.\n   - `STUDENTS`: A list of 100 students named from `Student_1` to `Student_100`.\n\n3. **Defines the `task_func` Function:**\n   - **Generates Random Grades:** Assigns a random grade between 0 and 100 for each student in each subject.\n   - **Handles Additional Fields:** If `additional_fields` are provided, it adds these as extra columns with random data.\n   - **Calculates Average Grades:**\n     - **Per Student:** Computes the average grade across all subjects for each student and adds it as the 'Average Grade' column.\n     - **Per Subject:** Computes the average grade for each subject across all students.\n   - **Appends the Average Row:** Adds a new row named 'Average' that contains the average grades per subject and the overall average grade.\n\n4. **Example Usage:**\n   - Generates and prints the report without additional fields.\n   - Generates and prints the report with additional fields (`Age` and `Attendance`) for demonstration purposes.\n\n**Note:** The use of `df.append()` is deprecated in recent versions of pandas. If you're using a newer version, consider using `pd.concat()` instead. Here's how you can modify the append operation:\n\n```python\n    # Append the average row using pd.concat\n    average_row_df = pd.DataFrame(average_row).T\n    df = pd.concat([df, average_row_df])\n```\n\nReplace the `df.append(average_row)` line with the above code if you encounter any deprecation warnings."}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height (cm)', 'Weight (kg)']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    first_names = [\n        'James', 'Mary', 'John', 'Patricia', 'Robert', 'Jennifer', 'Michael',\n        'Linda', 'William', 'Elizabeth', 'David', 'Barbara', 'Richard', 'Susan',\n        'Joseph', 'Jessica', 'Thomas', 'Sarah', 'Charles', 'Karen', 'Christopher',\n        'Nancy', 'Daniel', 'Lisa', 'Matthew', 'Betty', 'Anthony', 'Margaret',\n        'Mark', 'Sandra', 'Donald', 'Ashley', 'Steven', 'Kimberly', 'Paul',\n        'Emily', 'Andrew', 'Donna', 'Joshua', 'Michelle'\n    ]\n\n    last_names = [\n        'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller',\n        'Davis', 'Rodriguez', 'Martinez', 'Hernandez', 'Lopez', 'Gonzalez',\n        'Wilson', 'Anderson', 'Thomas', 'Taylor', 'Moore', 'Jackson', 'Martin',\n        'Lee', 'Perez', 'Thompson', 'White', 'Harris', 'Sanchez', 'Clark',\n        'Ramirez', 'Lewis', 'Robinson', 'Walker', 'Young', 'Allen', 'King',\n        'Wright', 'Scott', 'Torres', 'Nguyen', 'Hill', 'Flores'\n    ]\n\n    data = []\n\n    for _ in range(PEOPLE_COUNT):\n        name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n        age = random.randint(18, 80)  # Age between 18 and 80\n        height = round(random.uniform(150.0, 200.0), 1)  # Height in cm\n        weight = round(random.uniform(50.0, 100.0), 1)   # Weight in kg\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = mean(person[1] for person in data)\n    avg_height = mean(person[2] for person in data)\n    avg_weight = mean(person[3] for person in data)\n\n    # Append averages to data\n    data.append(['Average', round(avg_age, 2), round(avg_height, 2), round(avg_weight, 2)])\n\n    # Write data to CSV\n    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return os.path.abspath(filename)", "raw_solution": "```python\nimport os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height (cm)', 'Weight (kg)']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    first_names = [\n        'James', 'Mary', 'John', 'Patricia', 'Robert', 'Jennifer', 'Michael',\n        'Linda', 'William', 'Elizabeth', 'David', 'Barbara', 'Richard', 'Susan',\n        'Joseph', 'Jessica', 'Thomas', 'Sarah', 'Charles', 'Karen', 'Christopher',\n        'Nancy', 'Daniel', 'Lisa', 'Matthew', 'Betty', 'Anthony', 'Margaret',\n        'Mark', 'Sandra', 'Donald', 'Ashley', 'Steven', 'Kimberly', 'Paul',\n        'Emily', 'Andrew', 'Donna', 'Joshua', 'Michelle'\n    ]\n\n    last_names = [\n        'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller',\n        'Davis', 'Rodriguez', 'Martinez', 'Hernandez', 'Lopez', 'Gonzalez',\n        'Wilson', 'Anderson', 'Thomas', 'Taylor', 'Moore', 'Jackson', 'Martin',\n        'Lee', 'Perez', 'Thompson', 'White', 'Harris', 'Sanchez', 'Clark',\n        'Ramirez', 'Lewis', 'Robinson', 'Walker', 'Young', 'Allen', 'King',\n        'Wright', 'Scott', 'Torres', 'Nguyen', 'Hill', 'Flores'\n    ]\n\n    data = []\n\n    for _ in range(PEOPLE_COUNT):\n        name = f\"{random.choice(first_names)} {random.choice(last_names)}\"\n        age = random.randint(18, 80)  # Age between 18 and 80\n        height = round(random.uniform(150.0, 200.0), 1)  # Height in cm\n        weight = round(random.uniform(50.0, 100.0), 1)   # Weight in kg\n        data.append([name, age, height, weight])\n\n    # Calculate averages\n    avg_age = mean(person[1] for person in data)\n    avg_height = mean(person[2] for person in data)\n    avg_weight = mean(person[3] for person in data)\n\n    # Append averages to data\n    data.append(['Average', round(avg_age, 2), round(avg_height, 2), round(avg_weight, 2)])\n\n    # Write data to CSV\n    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    return os.path.abspath(filename)\n\n# Example usage\nif __name__ == \"__main__\":\n    csv_file_path = task_func('simulated_people_data.csv')\n    print(f\"CSV file created at: {csv_file_path}\")\n```"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    \"\"\"\n    Organizes files in the specified directory based on the first text \n    not enclosed in square brackets. Moves the files to subdirectories \n    named after this text. If no matching text is found, the file is not moved.\n\n    Args:\n        directory (str): The path to the directory to organize.\n\n    Returns:\n        tuple:\n            str: The directory path with organized files.\n            dict: A dictionary where keys are the created subdirectories and \n                  values are lists of files moved to them.\n    \"\"\"\n    # Dictionary to hold subdirectory names and their corresponding files\n    organized_files = {}\n\n    # Regex pattern to find text not enclosed in square brackets\n    pattern = re.compile(r'\\[.*?\\]|([^/\\[\\]]+)')\n\n    # Ensure the provided directory exists\n    if not os.path.isdir(directory):\n        raise NotADirectoryError(f\"The provided path '{directory}' is not a directory.\")\n\n    # Iterate over all items in the directory\n    for item in os.listdir(directory):\n        item_path = os.path.join(directory, item)\n        \n        # Process only files\n        if os.path.isfile(item_path):\n            # Remove the file extension for processing\n            filename, ext = os.path.splitext(item)\n            \n            # Find all matches in the filename\n            matches = pattern.findall(filename)\n            \n            # Extract the first non-empty match that is not within brackets\n            target = None\n            for match in matches:\n                if match and not match.startswith('[') and not match.endswith(']'):\n                    target = match.strip()\n                    break\n            \n            # If a valid target is found, move the file\n            if target:\n                # Define the subdirectory path\n                subdirectory = os.path.join(directory, target)\n                \n                # Create the subdirectory if it doesn't exist\n                if not os.path.exists(subdirectory):\n                    os.makedirs(subdirectory)\n                \n                # Define the destination path\n                destination = os.path.join(subdirectory, item)\n                \n                try:\n                    # Move the file to the subdirectory\n                    shutil.move(item_path, destination)\n                    \n                    # Add the file to the organized_files dictionary\n                    if target in organized_files:\n                        organized_files[target].append(item)\n                    else:\n                        organized_files[target] = [item]\n                except Exception as e:\n                    print(f\"Error moving file '{item}': {e}\")\n\n    return (directory, organized_files)", "raw_solution": "```python\nimport re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    \"\"\"\n    Organizes files in the specified directory based on the first text \n    not enclosed in square brackets. Moves the files to subdirectories \n    named after this text. If no matching text is found, the file is not moved.\n\n    Args:\n        directory (str): The path to the directory to organize.\n\n    Returns:\n        tuple:\n            str: The directory path with organized files.\n            dict: A dictionary where keys are the created subdirectories and \n                  values are lists of files moved to them.\n    \"\"\"\n    # Dictionary to hold subdirectory names and their corresponding files\n    organized_files = {}\n\n    # Regex pattern to find text not enclosed in square brackets\n    pattern = re.compile(r'\\[.*?\\]|([^/\\[\\]]+)')\n\n    # Ensure the provided directory exists\n    if not os.path.isdir(directory):\n        raise NotADirectoryError(f\"The provided path '{directory}' is not a directory.\")\n\n    # Iterate over all items in the directory\n    for item in os.listdir(directory):\n        item_path = os.path.join(directory, item)\n        \n        # Process only files\n        if os.path.isfile(item_path):\n            # Remove the file extension for processing\n            filename, ext = os.path.splitext(item)\n            \n            # Find all matches in the filename\n            matches = pattern.findall(filename)\n            \n            # Extract the first non-empty match that is not within brackets\n            target = None\n            for match in matches:\n                if match and not match.startswith('[') and not match.endswith(']'):\n                    target = match.strip()\n                    break\n            \n            # If a valid target is found, move the file\n            if target:\n                # Define the subdirectory path\n                subdirectory = os.path.join(directory, target)\n                \n                # Create the subdirectory if it doesn't exist\n                if not os.path.exists(subdirectory):\n                    os.makedirs(subdirectory)\n                \n                # Define the destination path\n                destination = os.path.join(subdirectory, item)\n                \n                try:\n                    # Move the file to the subdirectory\n                    shutil.move(item_path, destination)\n                    \n                    # Add the file to the organized_files dictionary\n                    if target in organized_files:\n                        organized_files[target].append(item)\n                    else:\n                        organized_files[target] = [item]\n                except Exception as e:\n                    print(f\"Error moving file '{item}': {e}\")\n\n    return (directory, organized_files)\n\n# Example usage:\nif __name__ == \"__main__\":\n    dir_path = \"/path/to/your/directory\"\n    organized_path, files_dict = task_func(dir_path)\n    print(f\"Organized Directory: {organized_path}\")\n    for subdir, files in files_dict.items():\n        print(f\"Subdirectory '{subdir}':\")\n        for file in files:\n            print(f\"  - {file}\")\n```"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    \"\"\"\n    Runs each file in file_list as a subprocess concurrently and returns their exit codes.\n\n    Args:\n        file_list (list): List of file paths to execute.\n\n    Returns:\n        list: A list containing the exit codes of each subprocess.\n    \"\"\"\n    exit_codes = []\n    threads = []\n    exit_codes_lock = threading.Lock()\n\n    def run_subprocess(file):\n        try:\n            # Start the subprocess\n            process = subprocess.Popen([file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            # Wait for the subprocess to complete\n            stdout, stderr = process.communicate()\n            # Retrieve the exit code\n            exit_code = process.returncode\n        except Exception as e:\n            print(f\"Error running {file}: {e}\")\n            exit_code = -1  # Indicate failure\n        # Safely append the exit code\n        with exit_codes_lock:\n            exit_codes.append(exit_code)\n\n    # Create and start a thread for each file\n    for file in file_list:\n        thread = threading.Thread(target=run_subprocess, args=(file,))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes", "raw_solution": "```python\nimport subprocess\nimport time\nimport threading\n\ndef task_func(file_list):\n    \"\"\"\n    Runs each file in file_list as a subprocess concurrently and returns their exit codes.\n\n    Args:\n        file_list (list): List of file paths to execute.\n\n    Returns:\n        list: A list containing the exit codes of each subprocess.\n    \"\"\"\n    exit_codes = []\n    threads = []\n    exit_codes_lock = threading.Lock()\n\n    def run_subprocess(file):\n        try:\n            # Start the subprocess\n            process = subprocess.Popen([file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            # Wait for the subprocess to complete\n            stdout, stderr = process.communicate()\n            # Retrieve the exit code\n            exit_code = process.returncode\n        except Exception as e:\n            print(f\"Error running {file}: {e}\")\n            exit_code = -1  # Indicate failure\n        # Safely append the exit code\n        with exit_codes_lock:\n            exit_codes.append(exit_code)\n\n    # Create and start a thread for each file\n    for file in file_list:\n        thread = threading.Thread(target=run_subprocess, args=(file,))\n        thread.start()\n        threads.append(thread)\n\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    return exit_codes\n\nif __name__ == \"__main__\":\n    # Example usage:\n    # Ensure that 'script1.py', 'script2.py', and 'script3.py' exist and are executable.\n    files_to_run = ['script1.py', 'script2.py', 'script3.py']\n    exit_codes = task_func(files_to_run)\n    print(\"Exit codes:\", exit_codes)\n```\n\n## Explanation\n\n1. **Imports**:\n    - `subprocess`: To spawn new processes.\n    - `time`: For any time-related operations (not strictly necessary here but included as per your request).\n    - `threading`: To handle concurrent execution of subprocesses.\n\n2. **`task_func` Function**:\n    - **Arguments**:\n        - `file_list`: A list of file paths that you want to execute as subprocesses.\n    - **Process**:\n        - Initializes an empty list `exit_codes` to store the exit codes of each subprocess.\n        - Defines a nested function `run_subprocess` that handles the execution of a single file.\n            - Attempts to execute the file using `subprocess.Popen`.\n            - Waits for the process to complete using `communicate()`.\n            - Captures the exit code.\n            - If an error occurs during execution, it catches the exception and assigns an exit code of `-1`.\n            - Uses a lock `exit_codes_lock` to safely append the exit code to the shared `exit_codes` list.\n        - Iterates over each file in `file_list`, creates a new thread targeting the `run_subprocess` function, and starts the thread.\n        - After starting all threads, it waits (`join()`) for all of them to finish.\n    - **Return Value**:\n        - Returns the list `exit_codes` containing the exit codes of all subprocesses.\n\n3. **Example Usage**:\n    - In the `__main__` block, an example list `files_to_run` is defined.\n    - `task_func` is called with this list, and the resulting exit codes are printed.\n    - **Note**: Ensure that the files listed (`script1.py`, `script2.py`, `script3.py`) exist in the same directory and have the appropriate execute permissions. Alternatively, provide absolute or relative paths to executable files or scripts.\n\n## Usage Notes\n\n- **Executable Permissions**: Make sure that the files you intend to run have the necessary execute permissions. For Python scripts, you can add a shebang line (e.g., `#!/usr/bin/env python3`) at the top and make them executable using `chmod +x script.py`.\n  \n- **Error Handling**: The function assigns an exit code of `-1` if there's an error starting or running a subprocess. You can modify this behavior based on your requirements.\n\n- **Output Handling**: Currently, the subprocess's `stdout` and `stderr` are captured but not used. You can modify the `run_subprocess` function to process these outputs as needed.\n\n- **Concurrency Limits**: This implementation creates one thread per subprocess, which is suitable for a moderate number of files. For a very large number of subprocesses, you might want to implement concurrency limits to avoid overwhelming system resources."}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in the given directory.\n\n    Args:\n        directory_path (str): The path to the directory containing .bat files.\n\n    Returns:\n        list of tuples: Each tuple contains the file name and its exit code.\n                        The exit code is None if the file could not be executed.\n    \"\"\"\n    result = []\n    # Ensure the directory exists\n    if not os.path.isdir(directory_path):\n        print(f\"Directory does not exist: {directory_path}\")\n        return result\n\n    # Find all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n\n    for bat_file in bat_files:\n        file_name = os.path.basename(bat_file)\n        try:\n            # Execute the .bat file\n            completed_process = subprocess.run(\n                ['cmd.exe', '/c', bat_file],\n                capture_output=True,\n                text=True,\n                check=False\n            )\n            exit_code = completed_process.returncode\n        except Exception as e:\n            print(f\"Failed to execute {file_name}: {e}\")\n            exit_code = None\n\n        result.append((file_name, exit_code))\n\n    return result", "raw_solution": "```python\nimport subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Finds and runs all .bat files in the given directory.\n\n    Args:\n        directory_path (str): The path to the directory containing .bat files.\n\n    Returns:\n        list of tuples: Each tuple contains the file name and its exit code.\n                        The exit code is None if the file could not be executed.\n    \"\"\"\n    result = []\n    # Ensure the directory exists\n    if not os.path.isdir(directory_path):\n        print(f\"Directory does not exist: {directory_path}\")\n        return result\n\n    # Find all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n\n    for bat_file in bat_files:\n        file_name = os.path.basename(bat_file)\n        try:\n            # Execute the .bat file\n            completed_process = subprocess.run(\n                ['cmd.exe', '/c', bat_file],\n                capture_output=True,\n                text=True,\n                check=False\n            )\n            exit_code = completed_process.returncode\n        except Exception as e:\n            print(f\"Failed to execute {file_name}: {e}\")\n            exit_code = None\n\n        result.append((file_name, exit_code))\n\n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    directory = sys.argv[1] if len(sys.argv) > 1 else '.'\n    results = task_func(directory)\n    for file, code in results:\n        print(f\"File: {file}, Exit Code: {code}\")\n```"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    \"\"\"\n    Generates a matplotlib figure with a histogram (including a kernel density estimate)\n    and a box plot for the specified column in the DataFrame.\n    \n    Parameters:\n    - df (pd.DataFrame): The input DataFrame.\n    - col (str): The column name to visualize.\n    \n    Returns:\n    - matplotlib.figure.Figure: A figure object containing the histogram and box plot.\n    \n    Raises:\n    - ValueError: If df is not a DataFrame, is empty, or does not contain the specified column.\n    \"\"\"\n    # Validate input\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    if col not in df.columns:\n        raise ValueError(f\"The DataFrame does not contain the specified column: '{col}'.\")\n\n    # Create a matplotlib figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Histogram with KDE for numerical data\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axes[0], color='skyblue', edgecolor='black')\n        axes[0].set_title(f'Histogram and KDE of {col}')\n        axes[0].set_xlabel(col)\n        axes[0].set_ylabel('Frequency')\n    else:\n        # For non-numerical data, plot a count plot\n        sns.countplot(x=df[col], ax=axes[0], palette='pastel')\n        axes[0].set_title(f'Count Plot of {col}')\n        axes[0].set_xlabel(col)\n        axes[0].set_ylabel('Count')\n        axes[0].tick_params(axis='x', rotation=45)\n\n    # Box plot\n    sns.boxplot(y=df[col], ax=axes[1], color='lightgreen')\n    axes[1].set_title(f'Box Plot of {col}')\n    axes[1].set_ylabel(col)\n\n    plt.tight_layout()\n    return fig", "raw_solution": "```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    \"\"\"\n    Generates a matplotlib figure with a histogram (including a kernel density estimate)\n    and a box plot for the specified column in the DataFrame.\n    \n    Parameters:\n    - df (pd.DataFrame): The input DataFrame.\n    - col (str): The column name to visualize.\n    \n    Returns:\n    - matplotlib.figure.Figure: A figure object containing the histogram and box plot.\n    \n    Raises:\n    - ValueError: If df is not a DataFrame, is empty, or does not contain the specified column.\n    \"\"\"\n    # Validate input\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a pandas DataFrame.\")\n    if df.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n    if col not in df.columns:\n        raise ValueError(f\"The DataFrame does not contain the specified column: '{col}'.\")\n\n    # Create a matplotlib figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Histogram with KDE for numerical data\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axes[0], color='skyblue', edgecolor='black')\n        axes[0].set_title(f'Histogram and KDE of {col}')\n        axes[0].set_xlabel(col)\n        axes[0].set_ylabel('Frequency')\n    else:\n        # For non-numerical data, plot a count plot\n        sns.countplot(x=df[col], ax=axes[0], palette='pastel')\n        axes[0].set_title(f'Count Plot of {col}')\n        axes[0].set_xlabel(col)\n        axes[0].set_ylabel('Count')\n        axes[0].tick_params(axis='x', rotation=45)\n\n    # Box plot\n    sns.boxplot(y=df[col], ax=axes[1], color='lightgreen')\n    axes[1].set_title(f'Box Plot of {col}')\n    axes[1].set_ylabel(col)\n\n    plt.tight_layout()\n    return fig\n```"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Executes a Python script as a subprocess with the given arguments.\n\n    Parameters:\n        script_path (str): The path to the Python script to execute.\n        wait (bool): If True, wait for the process to complete and return the exit code.\n                     If False, start the process and return None.\n        *args: Additional arguments to pass to the script.\n\n    Returns:\n        int or None: The exit code of the subprocess if wait is True, otherwise None.\n\n    Raises:\n        ValueError: If the script does not exist.\n        subprocess.CalledProcessError: If the subprocess exits with a non-zero status.\n    \"\"\"\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    # Construct the command to execute the script with arguments\n    cmd = [sys.executable, script_path] + list(args)\n\n    try:\n        if wait:\n            # Execute the subprocess and wait for it to complete\n            result = subprocess.run(cmd, check=True)\n            return result.returncode\n        else:\n            # Start the subprocess without waiting\n            subprocess.Popen(cmd)\n            return None\n    except subprocess.CalledProcessError as e:\n        # Re-raise the exception if the subprocess fails\n        raise subprocess.CalledProcessError(\n            returncode=e.returncode,\n            cmd=e.cmd,\n            output=e.output,\n            stderr=e.stderr\n        )", "raw_solution": "```python\nimport subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Executes a Python script as a subprocess with the given arguments.\n\n    Parameters:\n        script_path (str): The path to the Python script to execute.\n        wait (bool): If True, wait for the process to complete and return the exit code.\n                     If False, start the process and return None.\n        *args: Additional arguments to pass to the script.\n\n    Returns:\n        int or None: The exit code of the subprocess if wait is True, otherwise None.\n\n    Raises:\n        ValueError: If the script does not exist.\n        subprocess.CalledProcessError: If the subprocess exits with a non-zero status.\n    \"\"\"\n    if not os.path.isfile(script_path):\n        raise ValueError(f\"Script does not exist: {script_path}\")\n\n    # Construct the command to execute the script with arguments\n    cmd = [sys.executable, script_path] + list(args)\n\n    try:\n        if wait:\n            # Execute the subprocess and wait for it to complete\n            result = subprocess.run(cmd, check=True)\n            return result.returncode\n        else:\n            # Start the subprocess without waiting\n            subprocess.Popen(cmd)\n            return None\n    except subprocess.CalledProcessError as e:\n        # Re-raise the exception if the subprocess fails\n        raise subprocess.CalledProcessError(\n            returncode=e.returncode,\n            cmd=e.cmd,\n            output=e.output,\n            stderr=e.stderr\n        )\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Replace 'example_script.py' with the path to your script\n    script = 'example_script.py'\n    \n    try:\n        # Run the script and wait for it to complete\n        return_code = task_func(script, wait=True, arg1='value1', arg2='value2')\n        print(f\"Script completed with return code: {return_code}\")\n    except ValueError as ve:\n        print(f\"Error: {ve}\")\n    except subprocess.CalledProcessError as cpe:\n        print(f\"Script failed with return code {cpe.returncode}\")\n```"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Loads data from an Excel file, calculates mean and standard deviation for each column,\n    and generates a bar chart.\n\n    Parameters:\n    - file_location (str): The path to the Excel file.\n    - sheet_name (str): The name of the sheet to load.\n\n    Returns:\n    - dict: A dictionary with mean and standard deviation of each column.\n    - matplotlib.figure.Figure: The figure object containing the bar chart.\n\n    Raises:\n    - FileNotFoundError: If the Excel file does not exist at the specified path.\n    - ValueError: If the specified sheet does not exist in the workbook.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(f\"The file '{file_location}' does not exist.\")\n    \n    try:\n        # Load the specified sheet from the Excel file\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as ve:\n        # Raised if the sheet_name does not exist\n        raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\") from ve\n    except Exception as e:\n        # Re-raise any other exceptions\n        raise e\n\n    # Ensure that the DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"The provided sheet is empty.\")\n    \n    # Select only numeric columns\n    numeric_df = df.select_dtypes(include=[np.number])\n    \n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns found in the sheet.\")\n    \n    # Calculate mean and standard deviation for each column\n    stats = {}\n    means = numeric_df.mean()\n    stds = numeric_df.std()\n    for column in numeric_df.columns:\n        stats[column] = {\n            'mean': means[column],\n            'std': stds[column]\n        }\n    \n    # Prepare data for plotting\n    columns = numeric_df.columns.tolist()\n    mean_values = means.values\n    std_values = stds.values\n\n    x = np.arange(len(columns))  # the label locations\n    width = 0.35  # the width of the bars\n\n    # Create the matplotlib figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot mean values\n    rects1 = ax.bar(x - width/2, mean_values, width, label='Mean')\n\n    # Plot standard deviation values\n    rects2 = ax.bar(x + width/2, std_values, width, label='Standard Deviation')\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(columns, rotation=45, ha='right')\n    ax.legend()\n\n    fig.tight_layout()\n\n    return stats, fig", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Loads data from an Excel file, calculates mean and standard deviation for each column,\n    and generates a bar chart.\n\n    Parameters:\n    - file_location (str): The path to the Excel file.\n    - sheet_name (str): The name of the sheet to load.\n\n    Returns:\n    - dict: A dictionary with mean and standard deviation of each column.\n    - matplotlib.figure.Figure: The figure object containing the bar chart.\n\n    Raises:\n    - FileNotFoundError: If the Excel file does not exist at the specified path.\n    - ValueError: If the specified sheet does not exist in the workbook.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.isfile(file_location):\n        raise FileNotFoundError(f\"The file '{file_location}' does not exist.\")\n    \n    try:\n        # Load the specified sheet from the Excel file\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except ValueError as ve:\n        # Raised if the sheet_name does not exist\n        raise ValueError(f\"The sheet '{sheet_name}' does not exist in the workbook.\") from ve\n    except Exception as e:\n        # Re-raise any other exceptions\n        raise e\n\n    # Ensure that the DataFrame is not empty\n    if df.empty:\n        raise ValueError(\"The provided sheet is empty.\")\n    \n    # Select only numeric columns\n    numeric_df = df.select_dtypes(include=[np.number])\n    \n    if numeric_df.empty:\n        raise ValueError(\"No numeric columns found in the sheet.\")\n    \n    # Calculate mean and standard deviation for each column\n    stats = {}\n    means = numeric_df.mean()\n    stds = numeric_df.std()\n    for column in numeric_df.columns:\n        stats[column] = {\n            'mean': means[column],\n            'std': stds[column]\n        }\n    \n    # Prepare data for plotting\n    columns = numeric_df.columns.tolist()\n    mean_values = means.values\n    std_values = stds.values\n\n    x = np.arange(len(columns))  # the label locations\n    width = 0.35  # the width of the bars\n\n    # Create the matplotlib figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot mean values\n    rects1 = ax.bar(x - width/2, mean_values, width, label='Mean')\n\n    # Plot standard deviation values\n    rects2 = ax.bar(x + width/2, std_values, width, label='Standard Deviation')\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(columns, rotation=45, ha='right')\n    ax.legend()\n\n    fig.tight_layout()\n\n    return stats, fig\n```"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n    \n    # Initialize a dictionary to count activities per day\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        day_name = activity.strftime('%A')\n        activity_counts[day_name] += 1\n    \n    # Define the days of the week in order\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    \n    # Get counts for each day, defaulting to 0 if no activities\n    counts = [activity_counts.get(day, 0) for day in days_of_week]\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days_of_week, counts, color='skyblue')\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    # Optional: Improve layout\n    plt.tight_layout()\n    \n    return ax", "raw_solution": "```python\nfrom datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects.\")\n    \n    # Initialize a dictionary to count activities per day\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        day_name = activity.strftime('%A')\n        activity_counts[day_name] += 1\n    \n    # Define the days of the week in order\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    \n    # Get counts for each day, defaulting to 0 if no activities\n    counts = [activity_counts.get(day, 0) for day in days_of_week]\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days_of_week, counts, color='skyblue')\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    # Optional: Improve layout\n    plt.tight_layout()\n    \n    return ax\n```"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the destination directory.\n\n    Args:\n        src_dir (str): The path to the source directory.\n        dest_dir (str): The path to the destination directory.\n        seed (int, optional): Seed for random number generator. Defaults to 100.\n\n    Returns:\n        str: The name of the file moved in the format 'filename.extension'.\n    \"\"\"\n    # Set the random seed for reproducibility\n    random.seed(seed)\n\n    # Check if source directory exists\n    if not os.path.isdir(src_dir):\n        raise NotADirectoryError(f\"Source directory does not exist: {src_dir}\")\n\n    # Check if destination directory exists; create it if it doesn't\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir, exist_ok=True)\n\n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n\n    if not files:\n        raise FileNotFoundError(f\"No files found in source directory: {src_dir}\")\n\n    # Select a random file\n    file_to_move = random.choice(files)\n\n    # Define full source and destination paths\n    src_path = os.path.join(src_dir, file_to_move)\n    dest_path = os.path.join(dest_dir, file_to_move)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    return file_to_move", "raw_solution": "```python\nimport os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    \"\"\"\n    Moves a random file from the source directory to the destination directory.\n\n    Args:\n        src_dir (str): The path to the source directory.\n        dest_dir (str): The path to the destination directory.\n        seed (int, optional): Seed for random number generator. Defaults to 100.\n\n    Returns:\n        str: The name of the file moved in the format 'filename.extension'.\n    \"\"\"\n    # Set the random seed for reproducibility\n    random.seed(seed)\n\n    # Check if source directory exists\n    if not os.path.isdir(src_dir):\n        raise NotADirectoryError(f\"Source directory does not exist: {src_dir}\")\n\n    # Check if destination directory exists; create it if it doesn't\n    if not os.path.isdir(dest_dir):\n        os.makedirs(dest_dir, exist_ok=True)\n\n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n\n    if not files:\n        raise FileNotFoundError(f\"No files found in source directory: {src_dir}\")\n\n    # Select a random file\n    file_to_move = random.choice(files)\n\n    # Define full source and destination paths\n    src_path = os.path.join(src_dir, file_to_move)\n    dest_path = os.path.join(dest_dir, file_to_move)\n\n    # Move the file\n    shutil.move(src_path, dest_path)\n\n    return file_to_move\n\n# Example usage:\nif __name__ == \"__main__\":\n    source_directory = \"path/to/source\"\n    destination_directory = \"path/to/destination\"\n    moved_file = task_func(source_directory, destination_directory)\n    print(f\"Moved file: {moved_file}\")\n```"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory\n    by prefixing them with a double backslash.\n\n    Args:\n        directory_path (str): Path to the directory containing .xlsx files.\n\n    Returns:\n        int: The number of Excel files processed.\n    \"\"\"\n    # Ensure the directory path ends with a separator\n    directory_path = os.path.abspath(directory_path)\n    if not os.path.isdir(directory_path):\n        print(f\"The directory {directory_path} does not exist.\")\n        return 0\n\n    # Find all .xlsx files in the directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    files_processed = 0\n\n    for file_path in excel_files:\n        try:\n            wb = load_workbook(filename=file_path)\n            modified = False\n\n            for sheet in wb.worksheets:\n                for row in sheet.iter_rows():\n                    for cell in row:\n                        if isinstance(cell.value, str):\n                            # Replace \" with \\\\\"\n                            new_value = re.sub(r'(\")', r'\\\\\\1', cell.value)\n                            if new_value != cell.value:\n                                cell.value = new_value\n                                modified = True\n\n            if modified:\n                wb.save(filename=file_path)\n            files_processed += 1\n\n        except Exception as e:\n            print(f\"Failed to process {file_path}: {e}\")\n\n    return files_processed", "raw_solution": "```python\nimport regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes in all Excel (.xlsx) files in the specified directory\n    by prefixing them with a double backslash.\n\n    Args:\n        directory_path (str): Path to the directory containing .xlsx files.\n\n    Returns:\n        int: The number of Excel files processed.\n    \"\"\"\n    # Ensure the directory path ends with a separator\n    directory_path = os.path.abspath(directory_path)\n    if not os.path.isdir(directory_path):\n        print(f\"The directory {directory_path} does not exist.\")\n        return 0\n\n    # Find all .xlsx files in the directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    files_processed = 0\n\n    for file_path in excel_files:\n        try:\n            wb = load_workbook(filename=file_path)\n            modified = False\n\n            for sheet in wb.worksheets:\n                for row in sheet.iter_rows():\n                    for cell in row:\n                        if isinstance(cell.value, str):\n                            # Replace \" with \\\\\"\n                            new_value = re.sub(r'(\")', r'\\\\\\1', cell.value)\n                            if new_value != cell.value:\n                                cell.value = new_value\n                                modified = True\n\n            if modified:\n                wb.save(filename=file_path)\n            files_processed += 1\n\n        except Exception as e:\n            print(f\"Failed to process {file_path}: {e}\")\n\n    return files_processed\n\nif __name__ == \"__main__\":\n    processed_count = task_func('./xlsx_files/')\n    print(f\"Number of Excel files processed: {processed_count}\")\n```"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Creates a plot of sine and cosine waves with the given frequency.\n\n    Parameters:\n        frequency (float): The frequency of the sine and cosine waves. Must be non-negative.\n        sample_size (int, optional): The number of samples to generate. Must be positive. Defaults to 10000.\n\n    Returns:\n        fig (matplotlib.figure.Figure): The figure object containing the plot.\n        ax (matplotlib.axes.Axes): The axes object of the plot.\n\n    Raises:\n        ValueError: If the frequency is negative.\n        ValueError: If the sample size is non-positive.\n    \"\"\"\n    # Input validation\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    # Define the sampling rate\n    sampling_rate = 1000  # in Hz\n    duration = sample_size / sampling_rate  # in seconds\n\n    # Generate time values\n    t = np.linspace(0, duration, sample_size, endpoint=False)\n\n    # Calculate sine and cosine values\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave', linestyle='--')\n\n    # Customize the plot\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency} Hz')\n    ax.set_xlabel('Time (s)')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    ax.grid(True)\n\n    plt.tight_layout()\n\n    return fig, ax", "raw_solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Creates a plot of sine and cosine waves with the given frequency.\n\n    Parameters:\n        frequency (float): The frequency of the sine and cosine waves. Must be non-negative.\n        sample_size (int, optional): The number of samples to generate. Must be positive. Defaults to 10000.\n\n    Returns:\n        fig (matplotlib.figure.Figure): The figure object containing the plot.\n        ax (matplotlib.axes.Axes): The axes object of the plot.\n\n    Raises:\n        ValueError: If the frequency is negative.\n        ValueError: If the sample size is non-positive.\n    \"\"\"\n    # Input validation\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    # Define the sampling rate\n    sampling_rate = 1000  # in Hz\n    duration = sample_size / sampling_rate  # in seconds\n\n    # Generate time values\n    t = np.linspace(0, duration, sample_size, endpoint=False)\n\n    # Calculate sine and cosine values\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n\n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave', linestyle='--')\n\n    # Customize the plot\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency} Hz')\n    ax.set_xlabel('Time (s)')\n    ax.set_ylabel('Amplitude')\n    ax.legend()\n    ax.grid(True)\n\n    plt.tight_layout()\n\n    return fig, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        freq = 5  # Frequency in Hz\n        samples = 1000  # Number of samples\n        fig, ax = task_func(freq, samples)\n        plt.show()\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    \"\"\"\n    Initializes a Flask-Mail instance for sending emails using the specified app_name.\n    Email server details are retrieved from environment variables with default values if not set.\n\n    Args:\n        app_name (str): The name of the Flask application.\n\n    Returns:\n        tuple: A tuple containing the Flask-Mail instance and the app's mail configurations.\n    \"\"\"\n    app = Flask(app_name)\n\n    # Retrieve mail server configurations from environment variables or use defaults\n    mail_server = os.environ.get('MAIL_SERVER', 'localhost')\n    \n    # Ensure MAIL_PORT is an integer\n    try:\n        mail_port = int(os.environ.get('MAIL_PORT', 25))\n    except ValueError:\n        mail_port = 25\n\n    # Convert MAIL_USE_TLS to boolean\n    mail_use_tls_str = os.environ.get('MAIL_USE_TLS', 'False').lower()\n    mail_use_tls = mail_use_tls_str in ['true', '1', 't', 'yes']\n\n    mail_username = os.environ.get('MAIL_USERNAME')\n    mail_password = os.environ.get('MAIL_PASSWORD')\n\n    # Configure Flask app with mail settings\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Prepare mail configurations for output\n    mail_configurations = {\n        'MAIL_SERVER': app.config['MAIL_SERVER'],\n        'MAIL_PORT': app.config['MAIL_PORT'],\n        'MAIL_USE_TLS': app.config['MAIL_USE_TLS'],\n        'MAIL_USERNAME': app.config['MAIL_USERNAME'],\n        'MAIL_PASSWORD': app.config['MAIL_PASSWORD'],\n    }\n\n    return (mail, mail_configurations)", "raw_solution": "```python\nfrom flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    Initializes a Flask-Mail instance for sending emails using the specified app_name.\n    Email server details are retrieved from environment variables with default values if not set.\n\n    Args:\n        app_name (str): The name of the Flask application.\n\n    Returns:\n        tuple: A tuple containing the Flask-Mail instance and the app's mail configurations.\n    \"\"\"\n    app = Flask(app_name)\n\n    # Retrieve mail server configurations from environment variables or use defaults\n    mail_server = os.environ.get('MAIL_SERVER', 'localhost')\n    \n    # Ensure MAIL_PORT is an integer\n    try:\n        mail_port = int(os.environ.get('MAIL_PORT', 25))\n    except ValueError:\n        mail_port = 25\n\n    # Convert MAIL_USE_TLS to boolean\n    mail_use_tls_str = os.environ.get('MAIL_USE_TLS', 'False').lower()\n    mail_use_tls = mail_use_tls_str in ['true', '1', 't', 'yes']\n\n    mail_username = os.environ.get('MAIL_USERNAME')\n    mail_password = os.environ.get('MAIL_PASSWORD')\n\n    # Configure Flask app with mail settings\n    app.config.update(\n        MAIL_SERVER=mail_server,\n        MAIL_PORT=mail_port,\n        MAIL_USE_TLS=mail_use_tls,\n        MAIL_USERNAME=mail_username,\n        MAIL_PASSWORD=mail_password\n    )\n\n    # Initialize Flask-Mail\n    mail = Mail(app)\n\n    # Prepare mail configurations for output\n    mail_configurations = {\n        'MAIL_SERVER': app.config['MAIL_SERVER'],\n        'MAIL_PORT': app.config['MAIL_PORT'],\n        'MAIL_USE_TLS': app.config['MAIL_USE_TLS'],\n        'MAIL_USERNAME': app.config['MAIL_USERNAME'],\n        'MAIL_PASSWORD': app.config['MAIL_PASSWORD'],\n    }\n\n    return (mail, mail_configurations)\n\n# Example usage:\nif __name__ == \"__main__\":\n    app_name = \"my_flask_app\"\n    mail_instance, configs = task_func(app_name)\n    print(\"Mail Configurations:\")\n    for key, value in configs.items():\n        print(f\"{key}: {value}\")\n```"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file '{full_path}' does not exist.\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise e\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' was not found in the Excel file.\")\n    \n    # Extract the data from the specified column, excluding NaN values\n    data = df[column_name].dropna()\n    \n    # Calculate mean, median, and standard deviation\n    mean = data.mean()\n    median = data.median()\n    std_dev = data.std()\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'std_dev': std_dev\n    }", "raw_solution": "```python\nimport os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full path to the Excel file\n    full_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(full_path):\n        raise FileNotFoundError(f\"The file '{full_path}' does not exist.\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(full_path)\n    except Exception as e:\n        raise e\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' was not found in the Excel file.\")\n    \n    # Extract the data from the specified column, excluding NaN values\n    data = df[column_name].dropna()\n    \n    # Calculate mean, median, and standard deviation\n    mean = data.mean()\n    median = data.median()\n    std_dev = data.std()\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean,\n        'median': median,\n        'std_dev': std_dev\n    }\n```"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    # Split the data into training (75%) and test (25%) sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(4, input_dim=2, activation='sigmoid'))  # Hidden layer with 4 units\n    model.add(Dense(1, activation='sigmoid'))              # Output layer\n    \n    # Compile the model with binary cross-entropy loss and SGD optimizer\n    optimizer = SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    \n    # Fit the model to the training data with validation on the test set\n    history = model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), verbose=0)\n    \n    # Plot training and validation loss over epochs\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n    \n    return model, ax", "raw_solution": "```python\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    # Split the data into training (75%) and test (25%) sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Construct the Sequential model\n    model = Sequential()\n    model.add(Dense(4, input_dim=2, activation='sigmoid'))  # Hidden layer with 4 units\n    model.add(Dense(1, activation='sigmoid'))              # Output layer\n    \n    # Compile the model with binary cross-entropy loss and SGD optimizer\n    optimizer = SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    \n    # Fit the model to the training data with validation on the test set\n    history = model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), verbose=0)\n    \n    # Plot training and validation loss over epochs\n    fig, ax = plt.subplots()\n    ax.plot(history.history['loss'], label='Train')\n    ax.plot(history.history['val_loss'], label='Test')\n    ax.set_title('Model loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.legend()\n    \n    return model, ax\n```"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Define the Keras Sequential model with one hidden layer using sigmoid activation\n    model = keras.models.Sequential([\n        keras.layers.Dense(16, activation='sigmoid', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    # Compile the model with binary cross-entropy loss and SGD optimizer with a specified learning rate\n    optimizer = keras.optimizers.SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    # Fit the model to the training data in non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n    \n    # Predict probabilities on the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n    \n    # Compute ROC curve and AUC score\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot the ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')\n    ax.plot([0, 1], [0, 1], color='red', linestyle='--')\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.legend(loc='lower right')\n    plt.close(fig)  # Prevents the plot from displaying immediately\n    \n    return model, ax", "raw_solution": "```python\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and test sets (70% training, 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \n    # Define the Keras Sequential model with one hidden layer using sigmoid activation\n    model = keras.models.Sequential([\n        keras.layers.Dense(16, activation='sigmoid', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    # Compile the model with binary cross-entropy loss and SGD optimizer with a specified learning rate\n    optimizer = keras.optimizers.SGD(learning_rate=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    # Fit the model to the training data in non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n    \n    # Predict probabilities on the test set\n    Y_pred_prob = model.predict(X_test).ravel()\n    \n    # Compute ROC curve and AUC score\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n    \n    # Plot the ROC curve\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')\n    ax.plot([0, 1], [0, 1], color='red', linestyle='--')\n    ax.set_title('ROC curve')\n    ax.set_xlabel('False positive rate')\n    ax.set_ylabel('True positive rate')\n    ax.legend(loc='lower right')\n    plt.close(fig)  # Prevents the plot from displaying immediately\n    \n    return model, ax\n```"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads an RGB image, applies K-means clustering to segment the image into 'n_clusters' regions,\n    saves each region as a separate image, and returns numpy arrays of the original and segmented images.\n\n    Parameters:\n        image_path (str): Path to the input RGB image.\n        n_clusters (int): Number of clusters to form.\n        random_seed (int): Random seed for KMeans initialization.\n\n    Returns:\n        tuple: A tuple containing two numpy arrays.\n               - The first array is the original RGB image.\n               - The second array is the segmented image with each pixel's color replaced by its cluster centroid color.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist at the specified path.\n        ValueError: If 'n_clusters' is not a positive integer.\n    \"\"\"\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file was not found at the specified path: {image_path}\")\n\n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(f\"'n_clusters' must be a positive integer. Received: {n_clusters}\")\n\n    # Read the image using OpenCV\n    image_bgr = cv2.imread(image_path)\n    if image_bgr is None:\n        raise FileNotFoundError(f\"Failed to read the image. Please ensure the file is a valid image: {image_path}\")\n\n    # Convert the image from BGR to RGB\n    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n\n    # Store the original image\n    original_image = image_rgb.copy()\n\n    # Get image dimensions\n    height, width, channels = image_rgb.shape\n\n    # Reshape the image to a 2D array of pixels\n    pixel_data = image_rgb.reshape((-1, 3))\n\n    # Convert to float type for KMeans\n    pixel_data = np.float32(pixel_data)\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_data)\n\n    # Get cluster centers and labels\n    centers = kmeans.cluster_centers_.astype('uint8')\n    labels = kmeans.labels_\n\n    # Create the segmented image by replacing each pixel with its cluster center\n    segmented_pixels = centers[labels]\n    segmented_image = segmented_pixels.reshape((height, width, 3))\n\n    # Save each region as a separate image\n    for cluster_idx in range(n_clusters):\n        # Create a mask for the current cluster\n        mask = (labels == cluster_idx)\n        mask = mask.reshape((height, width))\n\n        # Initialize an empty image with zeros (black)\n        region_image = np.zeros_like(image_rgb)\n\n        # Assign the original pixel colors to the region image where the mask is True\n        region_image[mask] = image_rgb[mask]\n\n        # Convert the region image from RGB to BGR for saving with OpenCV\n        region_bgr = cv2.cvtColor(region_image, cv2.COLOR_RGB2BGR)\n\n        # Define the filename for the region image\n        region_filename = f'region_{cluster_idx + 1}.png'\n\n        # Save the region image\n        cv2.imwrite(region_filename, region_bgr)\n\n    return original_image, segmented_image", "raw_solution": "```python\n```python\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads an RGB image, applies K-means clustering to segment the image into 'n_clusters' regions,\n    saves each region as a separate image, and returns numpy arrays of the original and segmented images.\n\n    Parameters:\n        image_path (str): Path to the input RGB image.\n        n_clusters (int): Number of clusters to form.\n        random_seed (int): Random seed for KMeans initialization.\n\n    Returns:\n        tuple: A tuple containing two numpy arrays.\n               - The first array is the original RGB image.\n               - The second array is the segmented image with each pixel's color replaced by its cluster centroid color.\n\n    Raises:\n        FileNotFoundError: If the image file does not exist at the specified path.\n        ValueError: If 'n_clusters' is not a positive integer.\n    \"\"\"\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"The image file was not found at the specified path: {image_path}\")\n\n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(f\"'n_clusters' must be a positive integer. Received: {n_clusters}\")\n\n    # Read the image using OpenCV\n    image_bgr = cv2.imread(image_path)\n    if image_bgr is None:\n        raise FileNotFoundError(f\"Failed to read the image. Please ensure the file is a valid image: {image_path}\")\n\n    # Convert the image from BGR to RGB\n    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n\n    # Store the original image\n    original_image = image_rgb.copy()\n\n    # Get image dimensions\n    height, width, channels = image_rgb.shape\n\n    # Reshape the image to a 2D array of pixels\n    pixel_data = image_rgb.reshape((-1, 3))\n\n    # Convert to float type for KMeans\n    pixel_data = np.float32(pixel_data)\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_data)\n\n    # Get cluster centers and labels\n    centers = kmeans.cluster_centers_.astype('uint8')\n    labels = kmeans.labels_\n\n    # Create the segmented image by replacing each pixel with its cluster center\n    segmented_pixels = centers[labels]\n    segmented_image = segmented_pixels.reshape((height, width, 3))\n\n    # Save each region as a separate image\n    for cluster_idx in range(n_clusters):\n        # Create a mask for the current cluster\n        mask = (labels == cluster_idx)\n        mask = mask.reshape((height, width))\n\n        # Initialize an empty image with zeros (black)\n        region_image = np.zeros_like(image_rgb)\n\n        # Assign the original pixel colors to the region image where the mask is True\n        region_image[mask] = image_rgb[mask]\n\n        # Convert the region image from RGB to BGR for saving with OpenCV\n        region_bgr = cv2.cvtColor(region_image, cv2.COLOR_RGB2BGR)\n\n        # Define the filename for the region image\n        region_filename = f'region_{cluster_idx + 1}.png'\n\n        # Save the region image\n        cv2.imwrite(region_filename, region_bgr)\n\n    return original_image, segmented_image\n```"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of matrix P and 3D tensor T, flatten the result,\n    apply KMeans clustering to the flattened data, and visualize the clustering.\n\n    Parameters:\n        P (np.ndarray): 2D matrix of shape (m, n).\n        T (np.ndarray): 3D tensor of shape (n, d, e).\n        n_clusters (int): Number of clusters for KMeans. Default is 3.\n        random_state (int): Random state for KMeans. Default is 0.\n        n_init (int): Number of time the k-means algorithm will be run with different centroid seeds. Default is 10.\n\n    Returns:\n        cluster_result (np.ndarray): The result of KMeans clustering.\n        ax (matplotlib.axes.Axes): The visualization of the KMeans clustering.\n    \"\"\"\n    # Check dimensions for matrix multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(f\"Incompatible dimensions: P has shape {P.shape} and T has shape {T.shape}.\")\n\n    # Matrix-Tensor multiplication: result shape (m, d, e)\n    product = np.tensordot(P, T, axes=([1], [0]))\n    \n    # Flatten the result to 2D for KMeans: (m*d*e, 1)\n    flattened = product.reshape(-1, 1)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened)\n\n    # Visualization\n    fig, ax = plt.subplots(figsize=(8, 6))\n    scatter = ax.scatter(range(len(flattened)), flattened.flatten(), c=cluster_result, cmap='viridis', alpha=0.6)\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Sample Index')\n    ax.set_ylabel('Value')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    plt.tight_layout()\n\n    return cluster_result, ax", "raw_solution": "```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of matrix P and 3D tensor T, flatten the result,\n    apply KMeans clustering to the flattened data, and visualize the clustering.\n\n    Parameters:\n        P (np.ndarray): 2D matrix of shape (m, n).\n        T (np.ndarray): 3D tensor of shape (n, d, e).\n        n_clusters (int): Number of clusters for KMeans. Default is 3.\n        random_state (int): Random state for KMeans. Default is 0.\n        n_init (int): Number of time the k-means algorithm will be run with different centroid seeds. Default is 10.\n\n    Returns:\n        cluster_result (np.ndarray): The result of KMeans clustering.\n        ax (matplotlib.axes.Axes): The visualization of the KMeans clustering.\n    \"\"\"\n    # Check dimensions for matrix multiplication\n    if P.shape[1] != T.shape[0]:\n        raise ValueError(f\"Incompatible dimensions: P has shape {P.shape} and T has shape {T.shape}.\")\n\n    # Matrix-Tensor multiplication: result shape (m, d, e)\n    product = np.tensordot(P, T, axes=([1], [0]))\n    \n    # Flatten the result to 2D for KMeans: (m*d*e, 1)\n    flattened = product.reshape(-1, 1)\n\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened)\n\n    # Visualization\n    fig, ax = plt.subplots(figsize=(8, 6))\n    scatter = ax.scatter(range(len(flattened)), flattened.flatten(), c=cluster_result, cmap='viridis', alpha=0.6)\n    ax.set_title('KMeans Clustering Visualization')\n    ax.set_xlabel('Sample Index')\n    ax.set_ylabel('Value')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    plt.tight_layout()\n\n    return cluster_result, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example dimensions\n    m, n, d, e = 5, 4, 3, 2\n\n    # Generate random matrix P and tensor T\n    np.random.seed(0)\n    P = np.random.rand(m, n)\n    T = np.random.rand(n, d, e)\n\n    # Call the task function\n    clusters, axis = task_func(P, T, n_clusters=3, random_state=42, n_init=15)\n\n    # Show the plot\n    plt.show()\n\n    # Print the cluster results\n    print(\"Cluster Results:\")\n    print(clusters)\n```"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a set of 2D points, apply jittering,\n    and plot the diagram.\n\n    Parameters:\n    ----------\n    points : array-like\n        An (n, 2) array of points in 2D space.\n    seed : int, optional\n        Seed for the random number generator used in jittering (default is 0).\n\n    Returns:\n    -------\n    tuple\n        A tuple containing:\n        - vor (Voronoi): The Voronoi diagram of the input points.\n        - ax (Axes): The matplotlib axes with the plotted Voronoi diagram.\n\n    Raises:\n    ------\n    TypeError\n        If the input is not an array-like structure.\n    ValueError\n        If the input does not have the correct shape or contains non-numeric values.\n    \"\"\"\n    # Input validation\n    if not isinstance(points, (list, tuple, np.ndarray)):\n        raise TypeError(\"Input 'points' must be a list, tuple, or numpy array.\")\n    \n    points = np.asarray(points)\n    \n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input 'points' must be a 2D array with shape (n, 2).\")\n    \n    if not np.issubdtype(points.dtype, np.number):\n        raise ValueError(\"All coordinates in 'points' must be numeric.\")\n    \n    # Set random seed for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # Apply jittering\n    jitter_magnitude = 1e-5  # Small jitter to avoid duplicate points\n    jitter = rng.uniform(-jitter_magnitude, jitter_magnitude, points.shape)\n    jittered_points = points + jitter\n    \n    # Compute Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Plot Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange',\n                    line_width=2, line_alpha=0.6, point_size=2)\n    \n    # Plot the original points\n    ax.plot(jittered_points[:, 0], jittered_points[:, 1], 'b.', markersize=5)\n    \n    # Set plot aesthetics\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_aspect('equal')\n    \n    plt.tight_layout()\n    \n    return vor, ax", "raw_solution": "```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a set of 2D points, apply jittering,\n    and plot the diagram.\n\n    Parameters:\n    ----------\n    points : array-like\n        An (n, 2) array of points in 2D space.\n    seed : int, optional\n        Seed for the random number generator used in jittering (default is 0).\n\n    Returns:\n    -------\n    tuple\n        A tuple containing:\n        - vor (Voronoi): The Voronoi diagram of the input points.\n        - ax (Axes): The matplotlib axes with the plotted Voronoi diagram.\n\n    Raises:\n    ------\n    TypeError\n        If the input is not an array-like structure.\n    ValueError\n        If the input does not have the correct shape or contains non-numeric values.\n    \"\"\"\n    # Input validation\n    if not isinstance(points, (list, tuple, np.ndarray)):\n        raise TypeError(\"Input 'points' must be a list, tuple, or numpy array.\")\n    \n    points = np.asarray(points)\n    \n    if points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input 'points' must be a 2D array with shape (n, 2).\")\n    \n    if not np.issubdtype(points.dtype, np.number):\n        raise ValueError(\"All coordinates in 'points' must be numeric.\")\n    \n    # Set random seed for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # Apply jittering\n    jitter_magnitude = 1e-5  # Small jitter to avoid duplicate points\n    jitter = rng.uniform(-jitter_magnitude, jitter_magnitude, points.shape)\n    jittered_points = points + jitter\n    \n    # Compute Voronoi diagram\n    vor = Voronoi(jittered_points)\n    \n    # Plot Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange',\n                    line_width=2, line_alpha=0.6, point_size=2)\n    \n    # Plot the original points\n    ax.plot(jittered_points[:, 0], jittered_points[:, 1], 'b.', markersize=5)\n    \n    # Set plot aesthetics\n    ax.set_title('Voronoi Diagram')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_aspect('equal')\n    \n    plt.tight_layout()\n    \n    return vor, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate random points\n    np.random.seed(42)\n    sample_points = np.random.rand(15, 2) * 100\n    \n    # Compute and plot Voronoi diagram\n    voronoi, axes = task_func(sample_points, seed=42)\n    \n    # Display the plot\n    plt.show()\n```"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with the specified extension from src_dir to dest_dir.\n    \n    Parameters:\n        src_dir (str): Source directory path.\n        dest_dir (str): Destination directory path.\n        ext (str): File extension to filter by (e.g., '.txt').\n    \n    Returns:\n        list: A list of full paths of files that were successfully moved.\n    \n    Raises:\n        FileNotFoundError: If either the source or destination directory does not exist.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n\n    # Check if destination directory exists\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n\n    # Create the search pattern for the specified extension\n    pattern = os.path.join(src_dir, f\"*{ext}\")\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n\n    moved_files = []\n\n    for file_path in files:\n        file_name = os.path.basename(file_path)\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            shutil.move(file_path, dest_file_path)\n            moved_files.append(os.path.abspath(dest_file_path))\n        else:\n            # File exists in destination; do not move\n            continue\n\n    return moved_files", "raw_solution": "```python\nimport os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with the specified extension from src_dir to dest_dir.\n    \n    Parameters:\n        src_dir (str): Source directory path.\n        dest_dir (str): Destination directory path.\n        ext (str): File extension to filter by (e.g., '.txt').\n    \n    Returns:\n        list: A list of full paths of files that were successfully moved.\n    \n    Raises:\n        FileNotFoundError: If either the source or destination directory does not exist.\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n\n    # Check if destination directory exists\n    if not os.path.isdir(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n\n    # Create the search pattern for the specified extension\n    pattern = os.path.join(src_dir, f\"*{ext}\")\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n\n    moved_files = []\n\n    for file_path in files:\n        file_name = os.path.basename(file_path)\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            shutil.move(file_path, dest_file_path)\n            moved_files.append(os.path.abspath(dest_file_path))\n        else:\n            # File exists in destination; do not move\n            continue\n\n    return moved_files\n\n# Example usage:\nif __name__ == \"__main__\":\n    source_directory = \"/path/to/source\"\n    destination_directory = \"/path/to/destination\"\n    file_extension = \".txt\"\n\n    try:\n        moved = task_func(source_directory, destination_directory, file_extension)\n        if moved:\n            print(\"Successfully moved files:\")\n            for file in moved:\n                print(file)\n        else:\n            print(\"No files were moved.\")\n    except FileNotFoundError as e:\n        print(e)\n```"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    if not isinstance(data, dict) or not data:\n        return pd.DataFrame()\n\n    normalized_data = {}\n\n    for key, value in data.items():\n        if isinstance(value, (int, float)):\n            normalized_data[key] = float(value) * 2\n        elif isinstance(value, list):\n            doubled_list = []\n            for item in value:\n                if isinstance(item, (int, float)):\n                    doubled_list.append(float(item) * 2)\n                elif isinstance(item, str):\n                    # Find all numbers in the string\n                    numbers = re.findall(r'-?\\d+\\.?\\d*', item)\n                    if numbers:\n                        new_item = item\n                        for num in numbers:\n                            try:\n                                doubled_num = float(num) * 2\n                                # Replace only the first occurrence\n                                new_item = re.sub(re.escape(num), str(doubled_num), new_item, count=1)\n                            except ValueError:\n                                pass\n                        doubled_list.append(new_item)\n                    else:\n                        doubled_list.append(item)\n                else:\n                    doubled_list.append(item)\n            normalized_data[key] = doubled_list\n        elif isinstance(value, str):\n            # Check if the entire string is a number\n            if re.fullmatch(r'-?\\d+\\.?\\d*', value.strip()):\n                try:\n                    normalized_data[key] = float(value) * 2\n                except ValueError:\n                    normalized_data[key] = value\n            else:\n                # Find and double numbers within the string\n                numbers = re.findall(r'-?\\d+\\.?\\d*', value)\n                if numbers:\n                    new_value = value\n                    for num in numbers:\n                        try:\n                            doubled_num = float(num) * 2\n                            new_value = re.sub(re.escape(num), str(doubled_num), new_value, count=1)\n                        except ValueError:\n                            pass\n                    normalized_data[key] = new_value\n                else:\n                    normalized_data[key] = value\n        else:\n            normalized_data[key] = value\n\n    if not normalized_data:\n        return pd.DataFrame()\n\n    return pd.DataFrame([normalized_data])", "raw_solution": "```python\nimport json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    try:\n        data = json.loads(json_str)\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n\n    if not isinstance(data, dict) or not data:\n        return pd.DataFrame()\n\n    normalized_data = {}\n\n    for key, value in data.items():\n        if isinstance(value, (int, float)):\n            normalized_data[key] = float(value) * 2\n        elif isinstance(value, list):\n            doubled_list = []\n            for item in value:\n                if isinstance(item, (int, float)):\n                    doubled_list.append(float(item) * 2)\n                elif isinstance(item, str):\n                    # Find all numbers in the string\n                    numbers = re.findall(r'-?\\d+\\.?\\d*', item)\n                    if numbers:\n                        new_item = item\n                        for num in numbers:\n                            try:\n                                doubled_num = float(num) * 2\n                                # Replace only the first occurrence\n                                new_item = re.sub(re.escape(num), str(doubled_num), new_item, count=1)\n                            except ValueError:\n                                pass\n                        doubled_list.append(new_item)\n                    else:\n                        doubled_list.append(item)\n                else:\n                    doubled_list.append(item)\n            normalized_data[key] = doubled_list\n        elif isinstance(value, str):\n            # Check if the entire string is a number\n            if re.fullmatch(r'-?\\d+\\.?\\d*', value.strip()):\n                try:\n                    normalized_data[key] = float(value) * 2\n                except ValueError:\n                    normalized_data[key] = value\n            else:\n                # Find and double numbers within the string\n                numbers = re.findall(r'-?\\d+\\.?\\d*', value)\n                if numbers:\n                    new_value = value\n                    for num in numbers:\n                        try:\n                            doubled_num = float(num) * 2\n                            new_value = re.sub(re.escape(num), str(doubled_num), new_value, count=1)\n                        except ValueError:\n                            pass\n                    normalized_data[key] = new_value\n                else:\n                    normalized_data[key] = value\n        else:\n            normalized_data[key] = value\n\n    if not normalized_data:\n        return pd.DataFrame()\n\n    return pd.DataFrame([normalized_data])\n\n# Example usage\nif __name__ == '__main__':\n    json_input = '''\n    {\n        \"integer\": 10,\n        \"float\": 5.5,\n        \"list_numbers\": [1, \"2\", \"three\", 4.0],\n        \"string_number\": \"7\",\n        \"string_mixed\": \"Value is 3 and 4\",\n        \"non_numeric\": \"hello\",\n        \"empty_list\": [],\n        \"nested_dict\": {\"a\": 1}\n    }\n    '''\n    df = task_func(json_input)\n    print(\"DataFrame:\")\n    print(df)\n```\n\n# Sample Output\n\nWhen you run the above script, it will output:\n\n```\nDataFrame:\n   integer  float list_numbers string_number      string_mixed non_numeric empty_list  \\\n0     20.0   11.0  [2.0, 4.0, three, 8.0]           14     Value is 6 and 8      hello         []   \n\n                                  nested_dict  \n0  {'a': 1}  \n```\n\n**Explanation of the Output:**\n\n- **integer**: The original value `10` is doubled to `20.0`.\n- **float**: The original value `5.5` is doubled to `11.0`.\n- **list_numbers**: \n  - `1` becomes `2.0`\n  - `\"2\"` (a string) is interpreted as `2` and doubled to `4.0`\n  - `\"three\"` is a non-numeric string and remains unchanged\n  - `4.0` becomes `8.0`\n- **string_number**: The string `\"7\"` is interpreted as a number and doubled to `14.0`.\n- **string_mixed**: The numbers `3` and `4` within the string are doubled to `6` and `8`, resulting in `\"Value is 6 and 8\"`.\n- **non_numeric**: The string `\"hello\"` does not contain any numbers and remains unchanged.\n- **empty_list**: An empty list remains as is.\n- **nested_dict**: Nested dictionaries are not processed and remain unchanged.\n\nThis function effectively processes the input JSON string, doubling numerical values while preserving non-numeric data structures."}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"Script path '{script_path}' does not exist.\")\n\n    accumulated_memory = 0\n    cpu_usage_percent = 0.0\n\n    try:\n        # Start the subprocess\n        process = subprocess.Popen(['bash', script_path],\n                                   stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE)\n        ps_process = psutil.Process(process.pid)\n        start_time = time.time()\n\n        # Get initial CPU times and memory\n        initial_cpu_times = ps_process.cpu_times()\n        start_cpu = initial_cpu_times.user + initial_cpu_times.system\n\n        previous_memory = ps_process.memory_info().rss\n\n        while True:\n            if not ps_process.is_running() or ps_process.status() == psutil.STATUS_ZOMBIE:\n                break\n\n            current_time = time.time()\n            elapsed_time = current_time - start_time\n\n            if timeout is not None and elapsed_time > timeout:\n                process.terminate()\n                break\n\n            try:\n                # Update CPU times\n                current_cpu_times = ps_process.cpu_times()\n                current_cpu = current_cpu_times.user + current_cpu_times.system\n\n                # Update memory usage\n                current_memory = ps_process.memory_info().rss\n                if current_memory > previous_memory:\n                    accumulated_memory += current_memory - previous_memory\n                    previous_memory = current_memory\n\n            except (psutil.NoSuchProcess, psutil.ZombieProcess):\n                break\n\n            time.sleep(0.1)  # Sampling interval\n\n        # Calculate CPU usage\n        end_time = time.time()\n        wall_time = end_time - start_time if (end_time - start_time) > 0 else 1e-6\n\n        try:\n            end_cpu_times = ps_process.cpu_times()\n            end_cpu = end_cpu_times.user + end_cpu_times.system\n            total_cpu = end_cpu - start_cpu\n            cpu_usage_percent = (total_cpu / wall_time) * 100\n        except (psutil.NoSuchProcess, psutil.ZombieProcess):\n            cpu_usage_percent = 0.0\n\n        return {\n            'CPU Usage': cpu_usage_percent,\n            'Memory Usage': accumulated_memory\n        }\n\n    except Exception as e:\n        # Handle unexpected exceptions\n        return {\n            'CPU Usage': cpu_usage_percent,\n            'Memory Usage': accumulated_memory\n        }", "raw_solution": "```python\nimport subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"Script path '{script_path}' does not exist.\")\n\n    accumulated_memory = 0\n    cpu_usage_percent = 0.0\n\n    try:\n        # Start the subprocess\n        process = subprocess.Popen(['bash', script_path],\n                                   stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE)\n        ps_process = psutil.Process(process.pid)\n        start_time = time.time()\n\n        # Get initial CPU times and memory\n        initial_cpu_times = ps_process.cpu_times()\n        start_cpu = initial_cpu_times.user + initial_cpu_times.system\n\n        previous_memory = ps_process.memory_info().rss\n\n        while True:\n            if not ps_process.is_running() or ps_process.status() == psutil.STATUS_ZOMBIE:\n                break\n\n            current_time = time.time()\n            elapsed_time = current_time - start_time\n\n            if timeout is not None and elapsed_time > timeout:\n                process.terminate()\n                break\n\n            try:\n                # Update CPU times\n                current_cpu_times = ps_process.cpu_times()\n                current_cpu = current_cpu_times.user + current_cpu_times.system\n\n                # Update memory usage\n                current_memory = ps_process.memory_info().rss\n                if current_memory > previous_memory:\n                    accumulated_memory += current_memory - previous_memory\n                    previous_memory = current_memory\n\n            except (psutil.NoSuchProcess, psutil.ZombieProcess):\n                break\n\n            time.sleep(0.1)  # Sampling interval\n\n        # Calculate CPU usage\n        end_time = time.time()\n        wall_time = end_time - start_time if (end_time - start_time) > 0 else 1e-6\n\n        try:\n            end_cpu_times = ps_process.cpu_times()\n            end_cpu = end_cpu_times.user + end_cpu_times.system\n            total_cpu = end_cpu - start_cpu\n            cpu_usage_percent = (total_cpu / wall_time) * 100\n        except (psutil.NoSuchProcess, psutil.ZombieProcess):\n            cpu_usage_percent = 0.0\n\n        return {\n            'CPU Usage': cpu_usage_percent,\n            'Memory Usage': accumulated_memory\n        }\n\n    except Exception as e:\n        # Handle unexpected exceptions\n        return {\n            'CPU Usage': cpu_usage_percent,\n            'Memory Usage': accumulated_memory\n        }\n```"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random x and y values\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    if N >= len(CATEGORIES):\n        # Ensure each category appears at least once\n        categories = list(CATEGORIES)\n        remaining = N - len(CATEGORIES)\n        if remaining > 0:\n            categories += list(np.random.choice(CATEGORIES, size=remaining, replace=True))\n        np.random.shuffle(categories)\n    else:\n        # Sample without replacement if N < number of categories\n        categories = list(np.random.choice(CATEGORIES, size=N, replace=False))\n    \n    # Create the DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Assign a unique color to each category\n    categories_unique = df['category'].unique()\n    colors = plt.cm.tab10.colors  # A colormap with multiple distinct colors\n    color_map = {category: colors[i % len(colors)] for i, category in enumerate(categories_unique)}\n    \n    for category in categories_unique:\n        subset = df[df['category'] == category]\n        ax.scatter(subset['x'], subset['y'], label=category, color=color_map[category], alpha=0.7, edgecolors='w', s=100)\n    \n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot of x vs y Colored by Category')\n    ax.legend(title='Category')\n    ax.grid(True)\n    \n    plt.close(fig)  # Prevents the plot from displaying immediately\n    \n    return df, ax", "raw_solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    np.random.seed(seed)\n    \n    # Generate random x and y values\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    if N >= len(CATEGORIES):\n        # Ensure each category appears at least once\n        categories = list(CATEGORIES)\n        remaining = N - len(CATEGORIES)\n        if remaining > 0:\n            categories += list(np.random.choice(CATEGORIES, size=remaining, replace=True))\n        np.random.shuffle(categories)\n    else:\n        # Sample without replacement if N < number of categories\n        categories = list(np.random.choice(CATEGORIES, size=N, replace=False))\n    \n    # Create the DataFrame\n    df = pd.DataFrame({\n        'x': x,\n        'y': y,\n        'category': categories\n    })\n    \n    # Create scatter plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Assign a unique color to each category\n    categories_unique = df['category'].unique()\n    colors = plt.cm.tab10.colors  # A colormap with multiple distinct colors\n    color_map = {category: colors[i % len(colors)] for i, category in enumerate(categories_unique)}\n    \n    for category in categories_unique:\n        subset = df[df['category'] == category]\n        ax.scatter(subset['x'], subset['y'], label=category, color=color_map[category], alpha=0.7, edgecolors='w', s=100)\n    \n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot of x vs y Colored by Category')\n    ax.legend(title='Category')\n    ax.grid(True)\n    \n    plt.close(fig)  # Prevents the plot from displaying immediately\n    \n    return df, ax\n```"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generates a time series with a linear trend and plots it.\n\n    Parameters:\n    - start_time (str or datetime): The start time of the series.\n    - end_time (str or datetime): The end time of the series.\n    - step (str): Frequency string (e.g., 'D' for daily, 'H' for hourly).\n    - trend (float): The slope of the linear trend.\n    - seed (int): Seed for the random number generator.\n\n    Returns:\n    - ax (matplotlib.pyplot.Axes): The Axes object of the generated plot.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate datetime index\n    time_index = pd.date_range(start=start_time, end=end_time, freq=step)\n    n_points = len(time_index)\n    \n    # Generate random values from a normal distribution\n    random_values = np.random.normal(loc=0, scale=1, size=n_points)\n    \n    # Create a linear trend\n    linear_trend = trend * np.arange(n_points)\n    \n    # Combine random values with the linear trend\n    values = random_values + linear_trend\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Time': time_index, 'Value': values})\n    \n    # Plot the time series\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'], marker='o', linestyle='-')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n    ax.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return ax", "raw_solution": "```python\nfrom datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generates a time series with a linear trend and plots it.\n\n    Parameters:\n    - start_time (str or datetime): The start time of the series.\n    - end_time (str or datetime): The end time of the series.\n    - step (str): Frequency string (e.g., 'D' for daily, 'H' for hourly).\n    - trend (float): The slope of the linear trend.\n    - seed (int): Seed for the random number generator.\n\n    Returns:\n    - ax (matplotlib.pyplot.Axes): The Axes object of the generated plot.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate datetime index\n    time_index = pd.date_range(start=start_time, end=end_time, freq=step)\n    n_points = len(time_index)\n    \n    # Generate random values from a normal distribution\n    random_values = np.random.normal(loc=0, scale=1, size=n_points)\n    \n    # Create a linear trend\n    linear_trend = trend * np.arange(n_points)\n    \n    # Combine random values with the linear trend\n    values = random_values + linear_trend\n    \n    # Create a DataFrame\n    df = pd.DataFrame({'Time': time_index, 'Value': values})\n    \n    # Plot the time series\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Time'], df['Value'], marker='o', linestyle='-')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Time Series with Linear Trend')\n    ax.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return ax\n```"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for specified products from a given epoch time up to the current date.\n\n    Parameters:\n        epoch_milliseconds (int): The start time in milliseconds since epoch.\n        random_seed (int, optional): Seed for the random number generator. Defaults to 0.\n        products (list of str, optional): List of product names. Defaults to [\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"].\n\n    Returns:\n        pd.DataFrame: DataFrame containing 'Product', 'Date', and 'Sales' columns.\n    \"\"\"\n    # Input validation\n    if not isinstance(epoch_milliseconds, int):\n        raise TypeError(\"epoch_milliseconds must be an integer representing milliseconds since epoch.\")\n    \n    if epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a positive integer.\")\n    \n    if not isinstance(products, list) or len(products) != 5 or not all(isinstance(p, str) for p in products):\n        raise ValueError(\"products must be a list of five string elements.\")\n    \n    # Convert epoch milliseconds to datetime.date\n    start_datetime = datetime.utcfromtimestamp(epoch_milliseconds / 1000.0)\n    start_date = start_datetime.date()\n    \n    # Get current date\n    current_date = datetime.utcnow().date()\n    \n    if start_date > current_date:\n        raise ValueError(\"The start date must be on or before the current date.\")\n    \n    # Generate list of dates from start_date to current_date\n    delta_days = (current_date - start_date).days + 1  # Include current_date\n    date_list = [start_date + timedelta(days=i) for i in range(delta_days)]\n    \n    # Initialize random seed\n    random.seed(random_seed)\n    \n    # Generate sales data\n    data = []\n    for date in date_list:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append({\n                'Product': product,\n                'Date': datetime.combine(date, datetime.min.time()),\n                'Sales': sales\n            })\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    return df", "raw_solution": "```python\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generates sales data for specified products from a given epoch time up to the current date.\n\n    Parameters:\n        epoch_milliseconds (int): The start time in milliseconds since epoch.\n        random_seed (int, optional): Seed for the random number generator. Defaults to 0.\n        products (list of str, optional): List of product names. Defaults to [\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"].\n\n    Returns:\n        pd.DataFrame: DataFrame containing 'Product', 'Date', and 'Sales' columns.\n    \"\"\"\n    # Input validation\n    if not isinstance(epoch_milliseconds, int):\n        raise TypeError(\"epoch_milliseconds must be an integer representing milliseconds since epoch.\")\n    \n    if epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a positive integer.\")\n    \n    if not isinstance(products, list) or len(products) != 5 or not all(isinstance(p, str) for p in products):\n        raise ValueError(\"products must be a list of five string elements.\")\n    \n    # Convert epoch milliseconds to datetime.date\n    start_datetime = datetime.utcfromtimestamp(epoch_milliseconds / 1000.0)\n    start_date = start_datetime.date()\n    \n    # Get current date\n    current_date = datetime.utcnow().date()\n    \n    if start_date > current_date:\n        raise ValueError(\"The start date must be on or before the current date.\")\n    \n    # Generate list of dates from start_date to current_date\n    delta_days = (current_date - start_date).days + 1  # Include current_date\n    date_list = [start_date + timedelta(days=i) for i in range(delta_days)]\n    \n    # Initialize random seed\n    random.seed(random_seed)\n    \n    # Generate sales data\n    data = []\n    for date in date_list:\n        for product in products:\n            sales = random.randint(10, 50)\n            data.append({\n                'Product': product,\n                'Date': datetime.combine(date, datetime.min.time()),\n                'Sales': sales\n            })\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    return df\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example epoch time: January 1, 2023 in milliseconds\n    example_epoch = int(datetime(2023, 1, 1).timestamp() * 1000)\n    sales_df = task_func(example_epoch)\n    print(sales_df.head())\n```"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\nimport json\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Convert a JSON string to an Excel file.\n\n    Parameters:\n        json_str (str, bytes, bytearray): The JSON data as a string, bytes, or bytearray.\n        filename (str): The name of the Excel file to create.\n        sheet_name (str): The name of the sheet in the Excel file. Defaults to \"sheet1\".\n\n    Returns:\n        str: The absolute path of the created Excel file.\n\n    Raises:\n        TypeError: If `json_str` is not a string, bytes, or bytearray.\n        ValueError: If `json_str` is not valid JSON or does not represent a JSON array.\n        Exception: For general errors related to file writing.\n    \"\"\"\n    # Validate the type of json_str\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray.\")\n\n    try:\n        # Decode bytes or bytearray to string if necessary\n        if isinstance(json_str, (bytes, bytearray)):\n            json_str = json_str.decode('utf-8')\n\n        # Parse the JSON string\n        data = json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Invalid JSON string.\") from e\n\n    # Ensure the JSON data is a list (JSON array)\n    if not isinstance(data, list):\n        raise ValueError(\"JSON data must be an array.\")\n\n    try:\n        # Create a DataFrame from the JSON data\n        if data:\n            df = pd.DataFrame(data)\n        else:\n            # Create an empty DataFrame if the JSON array is empty\n            df = pd.DataFrame()\n\n        # Write the DataFrame to an Excel file using xlwt engine\n        with pd.ExcelWriter(filename, engine='xlwt') as writer:\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(\"Failed to write Excel file.\") from e\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)", "raw_solution": "```python\nimport xlwt\nimport os\nimport pandas as pd\nimport json\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Convert a JSON string to an Excel file.\n\n    Parameters:\n        json_str (str, bytes, bytearray): The JSON data as a string, bytes, or bytearray.\n        filename (str): The name of the Excel file to create.\n        sheet_name (str): The name of the sheet in the Excel file. Defaults to \"sheet1\".\n\n    Returns:\n        str: The absolute path of the created Excel file.\n\n    Raises:\n        TypeError: If `json_str` is not a string, bytes, or bytearray.\n        ValueError: If `json_str` is not valid JSON or does not represent a JSON array.\n        Exception: For general errors related to file writing.\n    \"\"\"\n    # Validate the type of json_str\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray.\")\n\n    try:\n        # Decode bytes or bytearray to string if necessary\n        if isinstance(json_str, (bytes, bytearray)):\n            json_str = json_str.decode('utf-8')\n\n        # Parse the JSON string\n        data = json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Invalid JSON string.\") from e\n\n    # Ensure the JSON data is a list (JSON array)\n    if not isinstance(data, list):\n        raise ValueError(\"JSON data must be an array.\")\n\n    try:\n        # Create a DataFrame from the JSON data\n        if data:\n            df = pd.DataFrame(data)\n        else:\n            # Create an empty DataFrame if the JSON array is empty\n            df = pd.DataFrame()\n\n        # Write the DataFrame to an Excel file using xlwt engine\n        with pd.ExcelWriter(filename, engine='xlwt') as writer:\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n    except Exception as e:\n        raise Exception(\"Failed to write Excel file.\") from e\n\n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example JSON strings\n    valid_json = '[{\"Name\": \"Alice\", \"Age\": 30}, {\"Name\": \"Bob\", \"Age\": 25}]'\n    empty_json = '[]'\n    invalid_json = '{\"Name\": \"Charlie\", \"Age\": 40}'  # Not a JSON array\n\n    # Define filenames\n    valid_filename = 'valid_output.xls'\n    empty_filename = 'empty_output.xls'\n    invalid_filename = 'invalid_output.xls'\n\n    # Test with valid JSON\n    try:\n        path = task_func(valid_json, valid_filename)\n        print(f\"Excel file created at: {path}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    # Test with empty JSON array\n    try:\n        path = task_func(empty_json, empty_filename)\n        print(f\"Excel file created at: {path}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    # Test with invalid JSON (not a JSON array)\n    try:\n        path = task_func(invalid_json, invalid_filename)\n        print(f\"Excel file created at: {path}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Generates a graph of daily activity durations for a specified number of days in the past\n    using randomly generated data for activities.\n\n    Parameters:\n    - days_in_past (int): Number of past days to generate data for.\n    - random_seed (int): Seed for random number generator to ensure reproducibility.\n\n    Returns:\n    - tuple:\n        - ax (matplotlib.pyplot.Axes): The axes object of the generated plot.\n        - df (pd.DataFrame): The DataFrame containing the generated activity data.\n    \"\"\"\n    \n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate a list of dates\n    today = datetime.today()\n    date_list = [(today - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)][::-1]\n    \n    # Generate random activity durations\n    data = []\n    for date in date_list:\n        for activity in activities:\n            duration = random.randint(0, 120)  # Duration between 0 and 120 minutes\n            data.append({\n                'Date': date,\n                'Activity': activity,\n                'Duration': duration\n            })\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n    \n    # Initialize the matplotlib figure\n    plt.figure(figsize=(12, 6))\n    \n    # Create a Seaborn lineplot\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    \n    # Enhance the plot with titles and labels\n    ax.set_title('Daily Activity Durations')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Duration (minutes)')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent clipping of tick-labels\n    plt.tight_layout()\n    \n    return ax, df", "raw_solution": "```python\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Generates a graph of daily activity durations for a specified number of days in the past\n    using randomly generated data for activities.\n\n    Parameters:\n    - days_in_past (int): Number of past days to generate data for.\n    - random_seed (int): Seed for random number generator to ensure reproducibility.\n\n    Returns:\n    - tuple:\n        - ax (matplotlib.pyplot.Axes): The axes object of the generated plot.\n        - df (pd.DataFrame): The DataFrame containing the generated activity data.\n    \"\"\"\n    \n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Generate a list of dates\n    today = datetime.today()\n    date_list = [(today - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_in_past)][::-1]\n    \n    # Generate random activity durations\n    data = []\n    for date in date_list:\n        for activity in activities:\n            duration = random.randint(0, 120)  # Duration between 0 and 120 minutes\n            data.append({\n                'Date': date,\n                'Activity': activity,\n                'Duration': duration\n            })\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data)\n    \n    # Initialize the matplotlib figure\n    plt.figure(figsize=(12, 6))\n    \n    # Create a Seaborn lineplot\n    ax = sns.lineplot(data=df, x='Date', y='Duration', hue='Activity', marker='o')\n    \n    # Enhance the plot with titles and labels\n    ax.set_title('Daily Activity Durations')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Duration (minutes)')\n    \n    # Rotate x-axis labels for better readability\n    plt.xticks(rotation=45)\n    \n    # Adjust layout to prevent clipping of tick-labels\n    plt.tight_layout()\n    \n    return ax, df\n\n# Example usage:\n# ax, df = task_func(days_in_past=14, random_seed=42)\n# plt.show()\n```"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Generate a DataFrame containing random stock prices for specified number of past days.\n\n    Parameters:\n    - days_in_past (int): Number of days in the past to generate data for.\n    - stock_names (list of str): List of stock ticker symbols.\n    - random_seed (int): Seed for the random number generator for reproducibility.\n\n    Returns:\n    - pd.DataFrame: DataFrame with dates as index and stock prices as float values in [0.0, 1.0).\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate a list of dates for the past 'days_in_past' days\n    end_date = datetime.today()\n    date_list = [end_date - timedelta(days=x) for x in range(days_in_past)]\n    date_list.reverse()  # Optional: to have the oldest date first\n    \n    # Generate random stock prices in [0.0, 1.0) for each stock and each day\n    price_data = np.random.rand(days_in_past, len(stock_names))\n    \n    # Create the DataFrame\n    df = pd.DataFrame(data=price_data, index=date_list, columns=stock_names)\n    \n    # Optionally, format the index as dates\n    df.index = pd.to_datetime(df.index).date\n    \n    return df", "raw_solution": "```python\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Generate a DataFrame containing random stock prices for specified number of past days.\n\n    Parameters:\n    - days_in_past (int): Number of days in the past to generate data for.\n    - stock_names (list of str): List of stock ticker symbols.\n    - random_seed (int): Seed for the random number generator for reproducibility.\n\n    Returns:\n    - pd.DataFrame: DataFrame with dates as index and stock prices as float values in [0.0, 1.0).\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Generate a list of dates for the past 'days_in_past' days\n    end_date = datetime.today()\n    date_list = [end_date - timedelta(days=x) for x in range(days_in_past)]\n    date_list.reverse()  # Optional: to have the oldest date first\n    \n    # Generate random stock prices in [0.0, 1.0) for each stock and each day\n    price_data = np.random.rand(days_in_past, len(stock_names))\n    \n    # Create the DataFrame\n    df = pd.DataFrame(data=price_data, index=date_list, columns=stock_names)\n    \n    # Optionally, format the index as dates\n    df.index = pd.to_datetime(df.index).date\n    \n    return df\n\n# Example usage:\nif __name__ == \"__main__\":\n    df_prices = task_func(days_in_past=10)\n    print(df_prices)\n```"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compare two CSV files and create a difference report.\n\n    Parameters:\n        file_path1 (str): Path to the first CSV file.\n        file_path2 (str): Path to the second CSV file.\n        delimiter (str): Delimiter used in the CSV files.\n        quotechar (str): Quote character used in the CSV files.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the differences with columns:\n                          'Line Number', 'Status', 'Content'.\n\n    Raises:\n        FileNotFoundError: If either of the files cannot be found.\n        ValueError: If either of the files is empty.\n        Exception: For other IO related errors.\n    \"\"\"\n    try:\n        with open(file_path1, 'r', newline='', encoding='utf-8') as f1:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            lines1 = [delimiter.join(row) for row in reader1]\n        \n        with open(file_path2, 'r', newline='', encoding='utf-8') as f2:\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            lines2 = [delimiter.join(row) for row in reader2]\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e.filename}\") from e\n    except Exception as e:\n        raise Exception(f\"An error occurred while reading the files: {e}\") from e\n\n    if not lines1 or not lines2:\n        raise ValueError(\"One of the files is empty.\")\n\n    diff = list(ndiff(lines1, lines2))\n    data = []\n    line_num = 1\n\n    for d in diff:\n        status = d[0]\n        content = d[2:].strip()\n        if status == ' ':\n            data.append({'Line Number': line_num, 'Status': ' ', 'Content': content})\n            line_num += 1\n        elif status == '-':\n            data.append({'Line Number': line_num, 'Status': '-', 'Content': content})\n            line_num += 1\n        elif status == '+':\n            data.append({'Line Number': line_num, 'Status': '+', 'Content': content})\n            # Do not increment line_num for additions\n\n    df = pd.DataFrame(data, columns=['Line Number', 'Status', 'Content'])\n    return df", "raw_solution": "```python\nimport pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compare two CSV files and create a difference report.\n\n    Parameters:\n        file_path1 (str): Path to the first CSV file.\n        file_path2 (str): Path to the second CSV file.\n        delimiter (str): Delimiter used in the CSV files.\n        quotechar (str): Quote character used in the CSV files.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the differences with columns:\n                          'Line Number', 'Status', 'Content'.\n\n    Raises:\n        FileNotFoundError: If either of the files cannot be found.\n        ValueError: If either of the files is empty.\n        Exception: For other IO related errors.\n    \"\"\"\n    try:\n        with open(file_path1, 'r', newline='', encoding='utf-8') as f1:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            lines1 = [delimiter.join(row) for row in reader1]\n        \n        with open(file_path2, 'r', newline='', encoding='utf-8') as f2:\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            lines2 = [delimiter.join(row) for row in reader2]\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f\"File not found: {e.filename}\") from e\n    except Exception as e:\n        raise Exception(f\"An error occurred while reading the files: {e}\") from e\n\n    if not lines1 or not lines2:\n        raise ValueError(\"One of the files is empty.\")\n\n    diff = list(ndiff(lines1, lines2))\n    data = []\n    line_num = 1\n\n    for d in diff:\n        status = d[0]\n        content = d[2:].strip()\n        if status == ' ':\n            data.append({'Line Number': line_num, 'Status': ' ', 'Content': content})\n            line_num += 1\n        elif status == '-':\n            data.append({'Line Number': line_num, 'Status': '-', 'Content': content})\n            line_num += 1\n        elif status == '+':\n            data.append({'Line Number': line_num, 'Status': '+', 'Content': content})\n            # Do not increment line_num for additions\n\n    df = pd.DataFrame(data, columns=['Line Number', 'Status', 'Content'])\n    return df\n\n# Example usage:\n# try:\n#     difference_df = task_func('file1.csv', 'file2.csv')\n#     print(difference_df)\n# except Exception as e:\n#     print(f\"Error: {e}\")\n```"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyzes employee data and calculates statistics for the specified column.\n    Also visualizes the data with a pie chart using the Age column as labels.\n\n    Parameters:\n        column (str): The column name to calculate statistics for.\n        data (list of dict): List of employee data dictionaries.\n\n    Returns:\n        tuple: A tuple containing:\n            - dict: A dictionary with 'sum', 'mean', 'min', and 'max' of the column.\n            - Axes: The matplotlib Axes object of the pie chart.\n    \"\"\"\n    if not data:\n        # If data list is empty\n        stats = {\n            'sum': 0,\n            'mean': np.nan,\n            'min': np.nan,\n            'max': np.nan\n        }\n        fig, ax = plt.subplots()\n        # Create an empty pie chart\n        ax.pie([], labels=[], autopct='%1.1f%%')\n    else:\n        # Convert data list to DataFrame\n        df = pd.DataFrame(data)\n        \n        if column not in df.columns:\n            raise ValueError(f\"Column '{column}' does not exist in the data.\")\n        \n        # Calculate statistics\n        stats = {\n            'sum': df[column].sum(),\n            'mean': df[column].mean(),\n            'min': df[column].min(),\n            'max': df[column].max()\n        }\n        \n        if 'Age' not in df.columns:\n            raise ValueError(\"Column 'Age' does not exist in the data for labeling the pie chart.\")\n        \n        # Prepare labels and sizes for pie chart\n        labels = df['Age'].astype(str)\n        sizes = df[column]\n        \n        fig, ax = plt.subplots()\n        # Create pie chart\n        ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n        ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return stats, ax", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    \"\"\"\n    Analyzes employee data and calculates statistics for the specified column.\n    Also visualizes the data with a pie chart using the Age column as labels.\n\n    Parameters:\n        column (str): The column name to calculate statistics for.\n        data (list of dict): List of employee data dictionaries.\n\n    Returns:\n        tuple: A tuple containing:\n            - dict: A dictionary with 'sum', 'mean', 'min', and 'max' of the column.\n            - Axes: The matplotlib Axes object of the pie chart.\n    \"\"\"\n    if not data:\n        # If data list is empty\n        stats = {\n            'sum': 0,\n            'mean': np.nan,\n            'min': np.nan,\n            'max': np.nan\n        }\n        fig, ax = plt.subplots()\n        # Create an empty pie chart\n        ax.pie([], labels=[], autopct='%1.1f%%')\n    else:\n        # Convert data list to DataFrame\n        df = pd.DataFrame(data)\n        \n        if column not in df.columns:\n            raise ValueError(f\"Column '{column}' does not exist in the data.\")\n        \n        # Calculate statistics\n        stats = {\n            'sum': df[column].sum(),\n            'mean': df[column].mean(),\n            'min': df[column].min(),\n            'max': df[column].max()\n        }\n        \n        if 'Age' not in df.columns:\n            raise ValueError(\"Column 'Age' does not exist in the data for labeling the pie chart.\")\n        \n        # Prepare labels and sizes for pie chart\n        labels = df['Age'].astype(str)\n        sizes = df[column]\n        \n        fig, ax = plt.subplots()\n        # Create pie chart\n        ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n        ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\n    return stats, ax\n\nif __name__ == \"__main__\":\n    # Example employee data\n    employee_data = [\n        {'Name': 'Alice', 'Age': 30, 'Salary': 70000},\n        {'Name': 'Bob', 'Age': 24, 'Salary': 50000},\n        {'Name': 'Charlie', 'Age': 29, 'Salary': 60000},\n        {'Name': 'Diana', 'Age': 35, 'Salary': 80000},\n        {'Name': 'Ethan', 'Age': 40, 'Salary': 90000}\n    ]\n    \n    # Specify the column to analyze\n    column_to_analyze = 'Salary'\n    \n    # Call the function\n    statistics, pie_ax = task_func(column_to_analyze, employee_data)\n    \n    # Output the statistics\n    print(f\"Statistics for column '{column_to_analyze}':\")\n    print(statistics)\n    \n    # Display the pie chart\n    plt.title(f\"Distribution of {column_to_analyze} by Age\")\n    plt.show()\n    \n    # Example with empty data\n    empty_data = []\n    statistics_empty, pie_ax_empty = task_func(column_to_analyze, empty_data)\n    \n    print(f\"\\nStatistics for column '{column_to_analyze}' with empty data:\")\n    print(statistics_empty)\n    \n    # Display the empty pie chart\n    plt.title(f\"Distribution of {column_to_analyze} by Age (Empty Data)\")\n    plt.show()\n```"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    \"\"\"\n    Analyzes fitness data, calculates statistics, and plots a line chart for a specified column.\n\n    Parameters:\n    - column (str): The column name to analyze.\n    - data (list of dict): The fitness data where each dictionary contains keys like 'Date', 'steps', \n                           'calories_burned', and 'distance_walked'.\n\n    Returns:\n    - tuple: \n        - dict: Contains the sum, mean, min, and max of the specified column.\n        - matplotlib.axes.Axes: The Axes object of the plotted line chart.\n    \n    Raises:\n    - KeyError: If the specified column or required columns are not present in the data.\n    - ValueError: If the data list is empty or contains negative values in 'steps', \n                  'calories_burned', or 'distance_walked'.\n    \"\"\"\n    # Validate that data is not empty\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Define the required numeric columns\n    required_columns = ['steps', 'calories_burned', 'distance_walked']\n    \n    # Check for presence and non-negative values in required columns\n    for col in required_columns:\n        if col not in df.columns:\n            raise KeyError(f\"Required column '{col}' is missing from data.\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must contain numeric values.\")\n        if (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in column '{col}'.\")\n\n    # Check if the specified column exists\n    if column not in df.columns:\n        raise KeyError(f\"Specified column '{column}' is not valid.\")\n    \n    # Calculate statistics\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    # Ensure 'Date' column exists for plotting\n    if 'Date' not in df.columns:\n        raise KeyError(\"Required column 'Date' is missing from data.\")\n    \n    # Convert 'Date' column to datetime\n    try:\n        df['Date'] = pd.to_datetime(df['Date'])\n    except Exception as e:\n        raise ValueError(\"Error converting 'Date' column to datetime.\") from e\n    \n    # Sort the DataFrame by Date\n    df = df.sort_values('Date')\n\n    # Plot the line chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Date'], df[column], marker='o', linestyle='-')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column.capitalize())\n    ax.set_title(f\"Line Chart of {column.capitalize()}\")\n    ax.grid(True)\n    plt.tight_layout()\n    \n    return (stats, ax)", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    \"\"\"\n    Analyzes fitness data, calculates statistics, and plots a line chart for a specified column.\n\n    Parameters:\n    - column (str): The column name to analyze.\n    - data (list of dict): The fitness data where each dictionary contains keys like 'Date', 'steps', \n                           'calories_burned', and 'distance_walked'.\n\n    Returns:\n    - tuple: \n        - dict: Contains the sum, mean, min, and max of the specified column.\n        - matplotlib.axes.Axes: The Axes object of the plotted line chart.\n    \n    Raises:\n    - KeyError: If the specified column or required columns are not present in the data.\n    - ValueError: If the data list is empty or contains negative values in 'steps', \n                  'calories_burned', or 'distance_walked'.\n    \"\"\"\n    # Validate that data is not empty\n    if not data:\n        raise ValueError(\"The data list is empty.\")\n    \n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n\n    # Define the required numeric columns\n    required_columns = ['steps', 'calories_burned', 'distance_walked']\n    \n    # Check for presence and non-negative values in required columns\n    for col in required_columns:\n        if col not in df.columns:\n            raise KeyError(f\"Required column '{col}' is missing from data.\")\n        if not pd.api.types.is_numeric_dtype(df[col]):\n            raise ValueError(f\"Column '{col}' must contain numeric values.\")\n        if (df[col] < 0).any():\n            raise ValueError(f\"Negative values found in column '{col}'.\")\n\n    # Check if the specified column exists\n    if column not in df.columns:\n        raise KeyError(f\"Specified column '{column}' is not valid.\")\n    \n    # Calculate statistics\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    # Ensure 'Date' column exists for plotting\n    if 'Date' not in df.columns:\n        raise KeyError(\"Required column 'Date' is missing from data.\")\n    \n    # Convert 'Date' column to datetime\n    try:\n        df['Date'] = pd.to_datetime(df['Date'])\n    except Exception as e:\n        raise ValueError(\"Error converting 'Date' column to datetime.\") from e\n    \n    # Sort the DataFrame by Date\n    df = df.sort_values('Date')\n\n    # Plot the line chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df['Date'], df[column], marker='o', linestyle='-')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column.capitalize())\n    ax.set_title(f\"Line Chart of {column.capitalize()}\")\n    ax.grid(True)\n    plt.tight_layout()\n    \n    return (stats, ax)\n```"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median\n    for each key (ignoring non-numeric or missing values), and returns the results\n    as a sorted Pandas DataFrame.\n\n    Parameters:\n        input_file (str): Path to the JSON file containing a list of dictionaries.\n\n    Returns:\n        pd.DataFrame: A DataFrame indexed by the variable names (keys) from the\n                      input data, containing columns 'mean' and 'median', sorted\n                      by the index.\n    \"\"\"\n    # Initialize a defaultdict to store numeric values for each key\n    data_dict = defaultdict(list)\n\n    # Read the JSON file\n    try:\n        with open(input_file, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {input_file} does not exist.\")\n    except json.JSONDecodeError:\n        raise ValueError(f\"The file {input_file} is not a valid JSON file.\")\n\n    # Iterate over each dictionary in the list\n    for entry in data:\n        if isinstance(entry, dict):\n            for key, value in entry.items():\n                # Check if the value is numeric (int or float)\n                if isinstance(value, (int, float)):\n                    data_dict[key].append(value)\n        else:\n            raise ValueError(\"Each item in the JSON list should be a dictionary.\")\n\n    # Prepare a dictionary to hold mean and median for each key\n    stats = {}\n\n    for key, values in data_dict.items():\n        if values:\n            mean_val = np.mean(values)\n            median_val = np.median(values)\n            stats[key] = {'mean': mean_val, 'median': median_val}\n\n    # Create a DataFrame from the stats dictionary\n    df = pd.DataFrame.from_dict(stats, orient='index')\n\n    # Sort the DataFrame by the index (variable names)\n    df.sort_index(inplace=True)\n\n    return df", "raw_solution": "```python\nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median\n    for each key (ignoring non-numeric or missing values), and returns the results\n    as a sorted Pandas DataFrame.\n\n    Parameters:\n        input_file (str): Path to the JSON file containing a list of dictionaries.\n\n    Returns:\n        pd.DataFrame: A DataFrame indexed by the variable names (keys) from the\n                      input data, containing columns 'mean' and 'median', sorted\n                      by the index.\n    \"\"\"\n    # Initialize a defaultdict to store numeric values for each key\n    data_dict = defaultdict(list)\n\n    # Read the JSON file\n    try:\n        with open(input_file, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {input_file} does not exist.\")\n    except json.JSONDecodeError:\n        raise ValueError(f\"The file {input_file} is not a valid JSON file.\")\n\n    # Iterate over each dictionary in the list\n    for entry in data:\n        if isinstance(entry, dict):\n            for key, value in entry.items():\n                # Check if the value is numeric (int or float)\n                if isinstance(value, (int, float)):\n                    data_dict[key].append(value)\n        else:\n            raise ValueError(\"Each item in the JSON list should be a dictionary.\")\n\n    # Prepare a dictionary to hold mean and median for each key\n    stats = {}\n\n    for key, values in data_dict.items():\n        if values:\n            mean_val = np.mean(values)\n            median_val = np.median(values)\n            stats[key] = {'mean': mean_val, 'median': median_val}\n\n    # Create a DataFrame from the stats dictionary\n    df = pd.DataFrame.from_dict(stats, orient='index')\n\n    # Sort the DataFrame by the index (variable names)\n    df.sort_index(inplace=True)\n\n    return df\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Assuming 'data.json' is in the same directory and properly formatted\n    result_df = task_func(\"data.json\")\n    print(result_df)\n```"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.lower().endswith('.csv'):\n        raise ValueError(\"The file must have a .csv extension.\")\n    \n    # Read the CSV file and count duplicate rows\n    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n        reader = csv.reader(csvfile)\n        rows = list(reader)\n    \n    # Use Counter to count occurrences of each row as tuples\n    row_counts = Counter(tuple(row) for row in rows)\n    \n    # Filter out rows that occur more than once (duplicates)\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n    \n    if not duplicates:\n        print(\"No duplicate rows found.\")\n        return duplicates, None\n    \n    # Convert duplicate rows to a pandas DataFrame\n    # Assuming the first row is the header\n    header = rows[0]\n    duplicate_rows = list(duplicates.keys())\n    counts = list(duplicates.values())\n    \n    # Create DataFrame\n    df_duplicates = pd.DataFrame(duplicate_rows, columns=header)\n    df_duplicates['Count'] = counts\n    \n    # Create a string representation of each duplicate row for plotting\n    df_duplicates['Row'] = df_duplicates.apply(lambda row: ', '.join(row.drop('Count').astype(str)), axis=1)\n    \n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(df_duplicates['Row'], df_duplicates['Count'], color='skyblue')\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n    ax.set_title('Duplicate Rows Count')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    \n    return duplicates, ax", "raw_solution": "```python\nimport csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.lower().endswith('.csv'):\n        raise ValueError(\"The file must have a .csv extension.\")\n    \n    # Read the CSV file and count duplicate rows\n    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n        reader = csv.reader(csvfile)\n        rows = list(reader)\n    \n    # Use Counter to count occurrences of each row as tuples\n    row_counts = Counter(tuple(row) for row in rows)\n    \n    # Filter out rows that occur more than once (duplicates)\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n    \n    if not duplicates:\n        print(\"No duplicate rows found.\")\n        return duplicates, None\n    \n    # Convert duplicate rows to a pandas DataFrame\n    # Assuming the first row is the header\n    header = rows[0]\n    duplicate_rows = list(duplicates.keys())\n    counts = list(duplicates.values())\n    \n    # Create DataFrame\n    df_duplicates = pd.DataFrame(duplicate_rows, columns=header)\n    df_duplicates['Count'] = counts\n    \n    # Create a string representation of each duplicate row for plotting\n    df_duplicates['Row'] = df_duplicates.apply(lambda row: ', '.join(row.drop('Count').astype(str)), axis=1)\n    \n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(df_duplicates['Row'], df_duplicates['Count'], color='skyblue')\n    ax.set_xlabel('Duplicate Rows')\n    ax.set_ylabel('Count')\n    ax.set_title('Duplicate Rows Count')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    \n    return duplicates, ax\n```"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    Identifies duplicate entries in a DataFrame and records the age distribution for the duplicate names.\n    \n    Parameters:\n        df (pd.DataFrame): DataFrame containing 'name' and 'age' columns.\n        \n    Returns:\n        Tuple[Counter, plt.Axes or None]: \n            - Counter object with the age distribution among duplicate names.\n            - Matplotlib Axes object with the histogram plot, or None if there are no duplicates.\n            \n    Raises:\n        ValueError: If the DataFrame is empty or if any age is negative.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    if 'age' not in df.columns or 'name' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'name' and 'age' columns.\")\n    \n    # Ensure 'age' is numeric\n    if not pd.api.types.is_numeric_dtype(df['age']):\n        raise ValueError(\"'age' column must be numeric.\")\n    \n    # Check for negative ages\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age values must not be negative.\")\n    \n    # Round down float ages to nearest integer\n    df = df.copy()\n    df['age'] = df['age'].floordiv(1).astype(int)\n    \n    # Identify duplicate names\n    duplicate_names = df['name'][df['name'].duplicated(keep=False)]\n    duplicates_df = df[df['name'].isin(duplicate_names)]\n    \n    if duplicates_df.empty:\n        return Counter(), None\n    \n    # Record age distribution\n    age_counts = Counter(duplicates_df['age'])\n    \n    # Calculate bins\n    min_age = duplicates_df['age'].min()\n    max_age = duplicates_df['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)\n    \n    # Plot histogram\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(data=duplicates_df, x='age', bins=bins, kde=False)\n    ax.set_title('Age Distribution for Duplicate Names')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    plt.tight_layout()\n    \n    return age_counts, ax", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    Identifies duplicate entries in a DataFrame and records the age distribution for the duplicate names.\n    \n    Parameters:\n        df (pd.DataFrame): DataFrame containing 'name' and 'age' columns.\n        \n    Returns:\n        Tuple[Counter, plt.Axes or None]: \n            - Counter object with the age distribution among duplicate names.\n            - Matplotlib Axes object with the histogram plot, or None if there are no duplicates.\n            \n    Raises:\n        ValueError: If the DataFrame is empty or if any age is negative.\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    if 'age' not in df.columns or 'name' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'name' and 'age' columns.\")\n    \n    # Ensure 'age' is numeric\n    if not pd.api.types.is_numeric_dtype(df['age']):\n        raise ValueError(\"'age' column must be numeric.\")\n    \n    # Check for negative ages\n    if (df['age'] < 0).any():\n        raise ValueError(\"Age values must not be negative.\")\n    \n    # Round down float ages to nearest integer\n    df = df.copy()\n    df['age'] = df['age'].floordiv(1).astype(int)\n    \n    # Identify duplicate names\n    duplicate_names = df['name'][df['name'].duplicated(keep=False)]\n    duplicates_df = df[df['name'].isin(duplicate_names)]\n    \n    if duplicates_df.empty:\n        return Counter(), None\n    \n    # Record age distribution\n    age_counts = Counter(duplicates_df['age'])\n    \n    # Calculate bins\n    min_age = duplicates_df['age'].min()\n    max_age = duplicates_df['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5, 1)\n    \n    # Plot histogram\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(data=duplicates_df, x='age', bins=bins, kde=False)\n    ax.set_title('Age Distribution for Duplicate Names')\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    plt.tight_layout()\n    \n    return age_counts, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    data = {\n        'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'David', 'Eve', 'Bob'],\n        'age': [25.7, 30, 25.2, 22, 30.5, 45, 29.9, 30]\n    }\n    df = pd.DataFrame(data)\n    \n    try:\n        age_distribution, histogram_ax = task_func(df)\n        print(\"Age Distribution among Duplicate Names:\")\n        print(age_distribution)\n        \n        if histogram_ax:\n            plt.show()\n        else:\n            print(\"No duplicate names found. No histogram to display.\")\n    except ValueError as ve:\n        print(f\"ValueError: {ve}\")\n```"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    \"\"\"\n    Identifies and counts duplicate values in the 'value' column of a DataFrame.\n    Plots a histogram of the 'value' column with an overlaid normal distribution curve.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing the 'value' column.\n        bins (int): Number of histogram bins.\n\n    Returns:\n        tuple: A tuple containing:\n            - Counter: A Counter object with the count of each duplicate value.\n            - Axes: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Count occurrences of each value in the 'value' column\n    value_counts = Counter(df['value'])\n    duplicates = {k: v for k, v in value_counts.items() if v > 1}\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    counts, bin_edges, patches = ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(df['value'])\n    \n    # Calculate the bin width for scaling the normal distribution\n    bin_width = bin_edges[1] - bin_edges[0]\n    \n    # Generate x values for the normal distribution curve\n    x = np.linspace(bin_edges[0], bin_edges[-1], 100)\n    p = norm.pdf(x, mu, std) * len(df['value']) * bin_width\n    \n    # Plot the normal distribution curve\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Set plot titles and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    # Display the plot\n    plt.show()\n    \n    return (Counter(duplicates), ax)", "raw_solution": "```python\nimport numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    \"\"\"\n    Identifies and counts duplicate values in the 'value' column of a DataFrame.\n    Plots a histogram of the 'value' column with an overlaid normal distribution curve.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing the 'value' column.\n        bins (int): Number of histogram bins.\n\n    Returns:\n        tuple: A tuple containing:\n            - Counter: A Counter object with the count of each duplicate value.\n            - Axes: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Count occurrences of each value in the 'value' column\n    value_counts = Counter(df['value'])\n    duplicates = {k: v for k, v in value_counts.items() if v > 1}\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    counts, bin_edges, patches = ax.hist(df['value'], bins=bins, color='green', alpha=0.6, edgecolor='black')\n    \n    # Fit a normal distribution to the data\n    mu, std = norm.fit(df['value'])\n    \n    # Calculate the bin width for scaling the normal distribution\n    bin_width = bin_edges[1] - bin_edges[0]\n    \n    # Generate x values for the normal distribution curve\n    x = np.linspace(bin_edges[0], bin_edges[-1], 100)\n    p = norm.pdf(x, mu, std) * len(df['value']) * bin_width\n    \n    # Plot the normal distribution curve\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Set plot titles and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    # Display the plot\n    plt.show()\n    \n    return (Counter(duplicates), ax)\n```"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b',\n    and plots it as a bar chart.\n\n    Parameters:\n    - a (list): List to set the DataFrame's row indices.\n    - b (list): List to determine the number of columns.\n\n    Returns:\n    - matplotlib.axes.Axes: The Axes object of the plotted bar chart.\n    \"\"\"\n    if len(b) > len(COLUMNS):\n        raise ValueError(f\"Length of list 'b' ({len(b)}) exceeds the number of predefined columns ({len(COLUMNS)}).\")\n    \n    # Select the required number of columns\n    selected_columns = COLUMNS[:len(b)]\n    \n    # Generate random data\n    data = np.random.rand(len(a), len(b))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, index=a, columns=selected_columns)\n    \n    # Plot as bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    \n    # Customize the plot\n    ax.set_title('Random Data Bar Chart')\n    ax.set_xlabel('Indices')\n    ax.set_ylabel('Values')\n    ax.legend(title='Columns')\n    \n    plt.tight_layout()\n    \n    return ax", "raw_solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b',\n    and plots it as a bar chart.\n\n    Parameters:\n    - a (list): List to set the DataFrame's row indices.\n    - b (list): List to determine the number of columns.\n\n    Returns:\n    - matplotlib.axes.Axes: The Axes object of the plotted bar chart.\n    \"\"\"\n    if len(b) > len(COLUMNS):\n        raise ValueError(f\"Length of list 'b' ({len(b)}) exceeds the number of predefined columns ({len(COLUMNS)}).\")\n    \n    # Select the required number of columns\n    selected_columns = COLUMNS[:len(b)]\n    \n    # Generate random data\n    data = np.random.rand(len(a), len(b))\n    \n    # Create DataFrame\n    df = pd.DataFrame(data, index=a, columns=selected_columns)\n    \n    # Plot as bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    \n    # Customize the plot\n    ax.set_title('Random Data Bar Chart')\n    ax.set_xlabel('Indices')\n    ax.set_ylabel('Values')\n    ax.legend(title='Columns')\n    \n    plt.tight_layout()\n    \n    return ax\n\nif __name__ == \"__main__\":\n    # Example lists\n    a = ['Row1', 'Row2', 'Row3', 'Row4']\n    b = ['Column1', 'Column2', 'Column3']\n    \n    # Generate and plot the DataFrame\n    axes = task_func(a, b)\n    \n    # Display the plot\n    plt.show()\n```"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A DataFrame containing 'month', 'value', and 'year' columns.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object representing the plot.\n    \"\"\"\n    # Ensure that 'month' is in datetime format to extract month names\n    if not pd.api.types.is_datetime64_any_dtype(data['month']):\n        data['month'] = pd.to_datetime(data['month'], format='%m')\n\n    # Extract year (assuming all data is for the same year)\n    year = data['month'].dt.year.iloc[0]\n\n    # Sort data by month\n    data = data.sort_values('month')\n\n    # Create month names for x-axis\n    data['month_name'] = data['month'].dt.strftime('%B')\n\n    # Create the bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(data['month_name'], data['value'], color='skyblue')\n\n    # Set title and labels\n    ax.set_title(f'Monthly Data for {year}', fontsize=16)\n    ax.set_xlabel('Month', fontsize=14)\n    ax.set_ylabel('Value', fontsize=14)\n\n    # Rotate x-axis labels if necessary\n    plt.xticks(rotation=45)\n\n    # Adjust layout for better fit\n    plt.tight_layout()\n\n    return ax", "raw_solution": "```python\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A DataFrame containing 'month', 'value', and 'year' columns.\n\n    Returns:\n    matplotlib.axes.Axes: The Axes object representing the plot.\n    \"\"\"\n    # Ensure that 'month' is in datetime format to extract month names\n    if not pd.api.types.is_datetime64_any_dtype(data['month']):\n        data['month'] = pd.to_datetime(data['month'], format='%m')\n\n    # Extract year (assuming all data is for the same year)\n    year = data['month'].dt.year.iloc[0]\n\n    # Sort data by month\n    data = data.sort_values('month')\n\n    # Create month names for x-axis\n    data['month_name'] = data['month'].dt.strftime('%B')\n\n    # Create the bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(data['month_name'], data['value'], color='skyblue')\n\n    # Set title and labels\n    ax.set_title(f'Monthly Data for {year}', fontsize=16)\n    ax.set_xlabel('Month', fontsize=14)\n    ax.set_ylabel('Value', fontsize=14)\n\n    # Rotate x-axis labels if necessary\n    plt.xticks(rotation=45)\n\n    # Adjust layout for better fit\n    plt.tight_layout()\n\n    return ax\n```"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\ndef task_func(data):\n    \"\"\"\n    Draws a histogram to visualize the frequency distribution of numeric values provided in a string.\n\n    Parameters:\n        data (str): A string containing numeric values separated by commas, spaces, or newlines.\n\n    Returns:\n        ax (matplotlib.axes._axes.Axes): The Axes object of the created histogram.\n    \"\"\"\n    # Convert the string data to a pandas Series\n    # Replace commas and newlines with spaces, then split\n    cleaned_data = data.replace(',', ' ').replace('\\n', ' ')\n    number_strings = cleaned_data.split()\n    \n    # Convert the list of strings to numeric values\n    try:\n        values = pd.to_numeric(number_strings)\n    except ValueError as e:\n        raise ValueError(\"Input data contains non-numeric values.\") from e\n    \n    if values.empty:\n        raise ValueError(\"No numeric data provided.\")\n    \n    # Calculate bins\n    bins = np.arange(values.min(), values.max() + 2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=bins, edgecolor='black', align='mid')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Optional: Improve layout\n    plt.tight_layout()\n    \n    # Display the plot\n    plt.show()\n    \n    return ax", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndef task_func(data):\n    \"\"\"\n    Draws a histogram to visualize the frequency distribution of numeric values provided in a string.\n\n    Parameters:\n        data (str): A string containing numeric values separated by commas, spaces, or newlines.\n\n    Returns:\n        ax (matplotlib.axes._axes.Axes): The Axes object of the created histogram.\n    \"\"\"\n    # Convert the string data to a pandas Series\n    # Replace commas and newlines with spaces, then split\n    cleaned_data = data.replace(',', ' ').replace('\\n', ' ')\n    number_strings = cleaned_data.split()\n    \n    # Convert the list of strings to numeric values\n    try:\n        values = pd.to_numeric(number_strings)\n    except ValueError as e:\n        raise ValueError(\"Input data contains non-numeric values.\") from e\n    \n    if values.empty:\n        raise ValueError(\"No numeric data provided.\")\n    \n    # Calculate bins\n    bins = np.arange(values.min(), values.max() + 2) - 0.5\n    \n    # Create the histogram\n    fig, ax = plt.subplots()\n    ax.hist(values, bins=bins, edgecolor='black', align='mid')\n    \n    # Set labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Optional: Improve layout\n    plt.tight_layout()\n    \n    # Display the plot\n    plt.show()\n    \n    return ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    data_str = \"1, 2, 2, 3, 3, 3, 4, 4, 4, 4\"\n    ax = task_func(data_str)\n```"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x data\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # True parameters\n    A_true = 1.0      # Amplitude\n    B_true = 1.0      # Frequency\n    C_true = 0.0      # Phase\n    D_true = 0.0      # Offset\n    \n    # Generate clean sine wave\n    y_clean = A_true * np.sin(B_true * x + C_true) + D_true\n    \n    # Add noise\n    np.random.seed(0)  # For reproducibility\n    noise = noise_level * np.random.randn(array_length)\n    y_noisy = y_clean + noise\n    \n    # Define the model function to fit\n    def sine_model(x, A, B, C, D):\n        return A * np.sin(B * x + C) + D\n    \n    # Initial guess for the parameters\n    initial_guess = [1.0, 1.0, 0.0, 0.0]\n    \n    # Perform curve fitting\n    params, params_covariance = curve_fit(sine_model, x, y_noisy, p0=initial_guess)\n    \n    # Generate fitted curve\n    y_fit = sine_model(x, *params)\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.scatter(x, y_noisy, label='Noisy Data', color='blue', s=20)\n    ax.plot(x, y_fit, label='Fitted Curve', color='red', linewidth=2)\n    \n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    ax.grid(True)\n    \n    plt.show()\n    \n    return ax", "raw_solution": "```python\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate x data\n    x = np.linspace(0, 2 * np.pi, array_length)\n    \n    # True parameters\n    A_true = 1.0      # Amplitude\n    B_true = 1.0      # Frequency\n    C_true = 0.0      # Phase\n    D_true = 0.0      # Offset\n    \n    # Generate clean sine wave\n    y_clean = A_true * np.sin(B_true * x + C_true) + D_true\n    \n    # Add noise\n    np.random.seed(0)  # For reproducibility\n    noise = noise_level * np.random.randn(array_length)\n    y_noisy = y_clean + noise\n    \n    # Define the model function to fit\n    def sine_model(x, A, B, C, D):\n        return A * np.sin(B * x + C) + D\n    \n    # Initial guess for the parameters\n    initial_guess = [1.0, 1.0, 0.0, 0.0]\n    \n    # Perform curve fitting\n    params, params_covariance = curve_fit(sine_model, x, y_noisy, p0=initial_guess)\n    \n    # Generate fitted curve\n    y_fit = sine_model(x, *params)\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.scatter(x, y_noisy, label='Noisy Data', color='blue', s=20)\n    ax.plot(x, y_fit, label='Fitted Curve', color='red', linewidth=2)\n    \n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    ax.grid(True)\n    \n    plt.show()\n    \n    return ax\n\n# Example usage:\n# ax = task_func(array_length=200, noise_level=0.3)\n```"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport re\ndef task_func(csv_file):\n    \"\"\"\n    Reads a CSV file, normalizes the text to ASCII, counts the words, and returns the 10 most common words\n    along with their frequencies as a matplotlib bar plot and a list of tuples.\n\n    Parameters:\n        csv_file (str): Path to the CSV file.\n\n    Returns:\n        tuple: A tuple containing:\n            - matplotlib.axes.Axes: The Axes object for the bar plot.\n            - list of tuples: The 10 most common words with their frequencies.\n\n    Raises:\n        FileNotFoundError: If the CSV file cannot be found at the specified path.\n        IOError: If there is an error in reading the file.\n    \"\"\"\n    # Initialize a list to hold all words\n    all_words = []\n\n    # Define a regex pattern to extract words\n    word_pattern = re.compile(r'\\b\\w+\\b')\n\n    # Open and read the CSV file\n    with open(csv_file, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            for cell in row:\n                # Normalize text to ASCII\n                normalized_text = unicodedata.normalize('NFKD', cell).encode('ascii', 'ignore').decode('ascii')\n                # Convert to lowercase\n                normalized_text = normalized_text.lower()\n                # Find all words using regex\n                words = word_pattern.findall(normalized_text)\n                # Extend the list of all words\n                all_words.extend(words)\n\n    # Count the frequency of each word\n    word_counts = Counter(all_words)\n    # Get the 10 most common words\n    most_common = word_counts.most_common(10)\n\n    # Separate the words and their counts for plotting\n    words, counts = zip(*most_common)\n\n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, counts, color='skyblue')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Top 10 Most Common Words')\n    ax.set_xticklabels(words, rotation=45, ha='right')\n\n    # Adjust layout to prevent clipping of tick-labels\n    plt.tight_layout()\n\n    return ax, most_common", "raw_solution": "```python\nimport unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport re\n\ndef task_func(csv_file):\n    \"\"\"\n    Reads a CSV file, normalizes the text to ASCII, counts the words, and returns the 10 most common words\n    along with their frequencies as a matplotlib bar plot and a list of tuples.\n\n    Parameters:\n        csv_file (str): Path to the CSV file.\n\n    Returns:\n        tuple: A tuple containing:\n            - matplotlib.axes.Axes: The Axes object for the bar plot.\n            - list of tuples: The 10 most common words with their frequencies.\n\n    Raises:\n        FileNotFoundError: If the CSV file cannot be found at the specified path.\n        IOError: If there is an error in reading the file.\n    \"\"\"\n    # Initialize a list to hold all words\n    all_words = []\n\n    # Define a regex pattern to extract words\n    word_pattern = re.compile(r'\\b\\w+\\b')\n\n    # Open and read the CSV file\n    with open(csv_file, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            for cell in row:\n                # Normalize text to ASCII\n                normalized_text = unicodedata.normalize('NFKD', cell).encode('ascii', 'ignore').decode('ascii')\n                # Convert to lowercase\n                normalized_text = normalized_text.lower()\n                # Find all words using regex\n                words = word_pattern.findall(normalized_text)\n                # Extend the list of all words\n                all_words.extend(words)\n\n    # Count the frequency of each word\n    word_counts = Counter(all_words)\n    # Get the 10 most common words\n    most_common = word_counts.most_common(10)\n\n    # Separate the words and their counts for plotting\n    words, counts = zip(*most_common)\n\n    # Create a bar plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, counts, color='skyblue')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('Top 10 Most Common Words')\n    ax.set_xticklabels(words, rotation=45, ha='right')\n\n    # Adjust layout to prevent clipping of tick-labels\n    plt.tight_layout()\n\n    return ax, most_common\n```"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    \"\"\"\n    Generates normally distributed random numbers, plots their histogram and PDF,\n    and returns the matplotlib figure object.\n\n    Parameters:\n    size (int): Number of random samples to generate. Default is 1000.\n\n    Returns:\n    matplotlib.figure.Figure: Figure object containing the histogram and PDF plot.\n    \"\"\"\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Plot histogram of the data\n    count, bins, patches = ax.hist(data, bins=30, density=True, alpha=0.6, color='skyblue', edgecolor='black', label='Histogram')\n    \n    # Plot the Probability Density Function (PDF)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 1000)\n    pdf = stats.norm.pdf(x, loc=0, scale=1)\n    ax.plot(x, pdf, 'r', linewidth=2, label='PDF')\n    \n    # Add titles and labels\n    ax.set_title('Histogram and PDF of Normally Distributed Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    # Add a legend\n    ax.legend()\n    \n    # Tight layout for better spacing\n    fig.tight_layout()\n    \n    return fig", "raw_solution": "```python\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    \"\"\"\n    Generates normally distributed random numbers, plots their histogram and PDF,\n    and returns the matplotlib figure object.\n\n    Parameters:\n    size (int): Number of random samples to generate. Default is 1000.\n\n    Returns:\n    matplotlib.figure.Figure: Figure object containing the histogram and PDF plot.\n    \"\"\"\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    # Plot histogram of the data\n    count, bins, patches = ax.hist(data, bins=30, density=True, alpha=0.6, color='skyblue', edgecolor='black', label='Histogram')\n    \n    # Plot the Probability Density Function (PDF)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 1000)\n    pdf = stats.norm.pdf(x, loc=0, scale=1)\n    ax.plot(x, pdf, 'r', linewidth=2, label='PDF')\n    \n    # Add titles and labels\n    ax.set_title('Histogram and PDF of Normally Distributed Data')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Density')\n    \n    # Add a legend\n    ax.legend()\n    \n    # Tight layout for better spacing\n    fig.tight_layout()\n    \n    return fig\n```"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair (2048 bits)\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Serialize the private key to PEM format bytes\n    private_key_bytes = private_key.save_pkcs1(format='PEM')\n\n    # Generate a random password for AES encryption (32 bytes for AES-256)\n    password = get_random_bytes(32)\n\n    # Generate a nonce for AES encryption (12 bytes for GCM mode)\n    nonce = get_random_bytes(12)\n\n    # Create AES cipher in GCM mode\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n\n    # Encrypt the private key bytes\n    ciphertext, tag = cipher.encrypt_and_digest(private_key_bytes)\n\n    # Combine ciphertext and tag, then encode in base64 for storage\n    encrypted_private_key = b64encode(ciphertext + tag).decode('utf-8')\n\n    # Generate a random 8-byte number and convert to hex for the filename\n    random_bytes = get_random_bytes(8)\n    hex_number = random_bytes.hex()\n    filename = f\"private_key_{hex_number}.txt\"\n\n    # Save the encrypted private key to the file\n    with open(filename, 'w') as file:\n        file.write(encrypted_private_key)\n\n    # Return the public key, filename, password, and nonce\n    return public_key, filename, password, nonce", "raw_solution": "```python\nimport rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA key pair (2048 bits)\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Serialize the private key to PEM format bytes\n    private_key_bytes = private_key.save_pkcs1(format='PEM')\n\n    # Generate a random password for AES encryption (32 bytes for AES-256)\n    password = get_random_bytes(32)\n\n    # Generate a nonce for AES encryption (12 bytes for GCM mode)\n    nonce = get_random_bytes(12)\n\n    # Create AES cipher in GCM mode\n    cipher = AES.new(password, AES.MODE_GCM, nonce=nonce)\n\n    # Encrypt the private key bytes\n    ciphertext, tag = cipher.encrypt_and_digest(private_key_bytes)\n\n    # Combine ciphertext and tag, then encode in base64 for storage\n    encrypted_private_key = b64encode(ciphertext + tag).decode('utf-8')\n\n    # Generate a random 8-byte number and convert to hex for the filename\n    random_bytes = get_random_bytes(8)\n    hex_number = random_bytes.hex()\n    filename = f\"private_key_{hex_number}.txt\"\n\n    # Save the encrypted private key to the file\n    with open(filename, 'w') as file:\n        file.write(encrypted_private_key)\n\n    # Return the public key, filename, password, and nonce\n    return public_key, filename, password, nonce\n```"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Generate a random AES key and IV\n    aes_key = os.urandom(32)  # 256-bit key for AES-256\n    aes_iv = os.urandom(16)   # 128-bit IV for AES\n\n    # Read the contents of the file to encrypt\n    with open(file_path, 'rb') as f:\n        data = f.read()\n\n    # Pad the data to be compatible with block cipher requirements\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n\n    # Initialize AES cipher in CBC mode and encrypt the data\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(aes_iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Prepend IV to the encrypted data for use in decryption\n    encrypted_file_content = aes_iv + encrypted_data\n\n    # Define filenames for the encrypted file and encrypted AES key\n    encrypted_file = file_path + '.enc'\n    encrypted_aes_key_file = file_path + '.aes.key.enc'\n\n    # Write the encrypted data to the encrypted file\n    with open(encrypted_file, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt the AES key using the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Write the encrypted AES key to a separate file\n    with open(encrypted_aes_key_file, 'wb') as f:\n        f.write(encrypted_aes_key)\n\n    # Return the RSA public key and the filenames of the encrypted files\n    return public_key, encrypted_file, encrypted_aes_key_file", "raw_solution": "```python\nimport rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    public_key, private_key = rsa.newkeys(2048)\n\n    # Generate a random AES key and IV\n    aes_key = os.urandom(32)  # 256-bit key for AES-256\n    aes_iv = os.urandom(16)   # 128-bit IV for AES\n\n    # Read the contents of the file to encrypt\n    with open(file_path, 'rb') as f:\n        data = f.read()\n\n    # Pad the data to be compatible with block cipher requirements\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n\n    # Initialize AES cipher in CBC mode and encrypt the data\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(aes_iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Prepend IV to the encrypted data for use in decryption\n    encrypted_file_content = aes_iv + encrypted_data\n\n    # Define filenames for the encrypted file and encrypted AES key\n    encrypted_file = file_path + '.enc'\n    encrypted_aes_key_file = file_path + '.aes.key.enc'\n\n    # Write the encrypted data to the encrypted file\n    with open(encrypted_file, 'wb') as f:\n        f.write(encrypted_file_content)\n\n    # Encrypt the AES key using the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # Write the encrypted AES key to a separate file\n    with open(encrypted_aes_key_file, 'wb') as f:\n        f.write(encrypted_aes_key)\n\n    # Return the RSA public key and the filenames of the encrypted files\n    return public_key, encrypted_file, encrypted_aes_key_file\n\n# Example usage:\n# public_key, enc_file, enc_key_file = task_func('path/to/your/file.txt')\n# print(public_key)\n# print(enc_file)\n# print(enc_key_file)\n```"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nimport urllib.error\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url or not isinstance(url, str):\n        raise ValueError(\"The URL provided is invalid or empty.\")\n    \n    try:\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Failed to fetch URL: {e.reason}\") from e\n    \n    # Parse HTML using PyQuery\n    doc = pq(html_content)\n    anchor_tags = doc('a')\n    \n    data = []\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    for a in anchor_tags.items():\n        text = a.text()\n        href = a.attr('href')\n        data.append({\n            'text': text,\n            'href': href,\n            'fetch_time': fetch_time\n        })\n    \n    df = pd.DataFrame(data, columns=['text', 'href', 'fetch_time'])\n    return df", "raw_solution": "```python\nimport urllib.request\nimport urllib.error\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url or not isinstance(url, str):\n        raise ValueError(\"The URL provided is invalid or empty.\")\n    \n    try:\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read()\n    except urllib.error.URLError as e:\n        raise urllib.error.URLError(f\"Failed to fetch URL: {e.reason}\") from e\n    \n    # Parse HTML using PyQuery\n    doc = pq(html_content)\n    anchor_tags = doc('a')\n    \n    data = []\n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \n    for a in anchor_tags.items():\n        text = a.text()\n        href = a.attr('href')\n        data.append({\n            'text': text,\n            'href': href,\n            'fetch_time': fetch_time\n        })\n    \n    df = pd.DataFrame(data, columns=['text', 'href', 'fetch_time'])\n    return df\n```"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it to a CSV file.\n\n    Parameters:\n        hours (int): Number of hours to generate data for.\n        output_dir (str): Directory where the CSV file will be saved.\n    \"\"\"\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define the start time\n    end_time = datetime.now()\n    start_time = end_time - timedelta(hours=hours)\n    \n    # Generate data\n    data = []\n    for hour in range(hours):\n        current_time = start_time + timedelta(hours=hour+1)\n        timestamp = current_time.strftime('%Y-%m-%d %H:%M:%S')\n        temperature = randint(-20, 40)  # Example range in Celsius\n        humidity = randint(0, 100)      # Percentage\n        pressure = randint(950, 1050)   # hPa\n        data.append({\n            'Time': timestamp,\n            'Temperature': temperature,\n            'Humidity': humidity,\n            'Pressure': pressure\n        })\n    \n    # Define the CSV file path\n    filename = f'sensor_data_{start_time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n    file_path = os.path.join(output_dir, filename)\n    \n    # Write data to CSV\n    with open(file_path, mode='w', newline='') as csvfile:\n        fieldnames = ['Time'] + SENSORS\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for entry in data:\n            writer.writerow(entry)\n    \n    print(f\"Sensor data for {hours} hours saved to '{file_path}'.\")", "raw_solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates sensor data for the specified number of hours and saves it to a CSV file.\n\n    Parameters:\n        hours (int): Number of hours to generate data for.\n        output_dir (str): Directory where the CSV file will be saved.\n    \"\"\"\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define the start time\n    end_time = datetime.now()\n    start_time = end_time - timedelta(hours=hours)\n    \n    # Generate data\n    data = []\n    for hour in range(hours):\n        current_time = start_time + timedelta(hours=hour+1)\n        timestamp = current_time.strftime('%Y-%m-%d %H:%M:%S')\n        temperature = randint(-20, 40)  # Example range in Celsius\n        humidity = randint(0, 100)      # Percentage\n        pressure = randint(950, 1050)   # hPa\n        data.append({\n            'Time': timestamp,\n            'Temperature': temperature,\n            'Humidity': humidity,\n            'Pressure': pressure\n        })\n    \n    # Define the CSV file path\n    filename = f'sensor_data_{start_time.strftime(\"%Y%m%d_%H%M%S\")}.csv'\n    file_path = os.path.join(output_dir, filename)\n    \n    # Write data to CSV\n    with open(file_path, mode='w', newline='') as csvfile:\n        fieldnames = ['Time'] + SENSORS\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for entry in data:\n            writer.writerow(entry)\n    \n    print(f\"Sensor data for {hours} hours saved to '{file_path}'.\")\n\nif __name__ == \"__main__\":\n    import argparse\n\n    # Set up command-line argument parsing\n    parser = argparse.ArgumentParser(description='Generate sensor data for a specified number of hours.')\n    parser.add_argument('hours', type=int, help='Number of hours to generate data for.')\n    parser.add_argument('--output_dir', type=str, default=OUTPUT_DIR, help='Directory to save the CSV file.')\n    \n    args = parser.parse_args()\n    \n    # Call the task function with provided arguments\n    task_func(args.hours, args.output_dir)\n```"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates traffic data for different vehicle types over a specified number of hours,\n    saves the data to a CSV file, and plots the data in a line chart.\n\n    Parameters:\n        hours (int): Number of hours to generate data for.\n        output_dir (str): Directory where the CSV file will be saved.\n\n    Returns:\n        tuple: Path to the CSV file and the matplotlib axes object of the line plot.\n    \"\"\"\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate time data\n    start_time = datetime.now()\n    time_data = [start_time + timedelta(hours=i) for i in range(hours)]\n\n    # Generate random vehicle counts\n    data = {'Time': time_data}\n    for vehicle in VEHICLE_TYPES:\n        if vehicle == 'Car':\n            data[vehicle] = [randint(50, 150) for _ in range(hours)]\n        elif vehicle == 'Bus':\n            data[vehicle] = [randint(10, 50) for _ in range(hours)]\n        elif vehicle == 'Truck':\n            data[vehicle] = [randint(5, 30) for _ in range(hours)]\n        elif vehicle == 'Bike':\n            data[vehicle] = [randint(20, 100) for _ in range(hours)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n\n    # Define CSV file path\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    csv_filename = f\"traffic_data_{timestamp}.csv\"\n    csv_path = os.path.join(output_dir, csv_filename)\n\n    # Save DataFrame to CSV\n    df.to_csv(csv_path, index=False)\n\n    # Plot the data\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return (csv_path, ax)", "raw_solution": "```python\nimport csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates traffic data for different vehicle types over a specified number of hours,\n    saves the data to a CSV file, and plots the data in a line chart.\n\n    Parameters:\n        hours (int): Number of hours to generate data for.\n        output_dir (str): Directory where the CSV file will be saved.\n\n    Returns:\n        tuple: Path to the CSV file and the matplotlib axes object of the line plot.\n    \"\"\"\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate time data\n    start_time = datetime.now()\n    time_data = [start_time + timedelta(hours=i) for i in range(hours)]\n\n    # Generate random vehicle counts\n    data = {'Time': time_data}\n    for vehicle in VEHICLE_TYPES:\n        if vehicle == 'Car':\n            data[vehicle] = [randint(50, 150) for _ in range(hours)]\n        elif vehicle == 'Bus':\n            data[vehicle] = [randint(10, 50) for _ in range(hours)]\n        elif vehicle == 'Truck':\n            data[vehicle] = [randint(5, 30) for _ in range(hours)]\n        elif vehicle == 'Bike':\n            data[vehicle] = [randint(20, 100) for _ in range(hours)]\n\n    # Create a DataFrame\n    df = pd.DataFrame(data)\n\n    # Define CSV file path\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    csv_filename = f\"traffic_data_{timestamp}.csv\"\n    csv_path = os.path.join(output_dir, csv_filename)\n\n    # Save DataFrame to CSV\n    df.to_csv(csv_path, index=False)\n\n    # Plot the data\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for vehicle in VEHICLE_TYPES:\n        ax.plot(df['Time'], df[vehicle], label=vehicle)\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    return (csv_path, ax)\n\nif __name__ == \"__main__\":\n    # Example usage\n    hours_to_generate = 24  # Generate data for 24 hours\n    csv_file, plot_axes = task_func(hours_to_generate)\n    print(f\"Data saved to: {csv_file}\")\n    plt.show()\n```"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates weather data for the specified number of hours, saves it to a CSV file,\n    and backs up the file to a backup directory.\n\n    Args:\n        hours (int): Number of hours to generate weather data for.\n        output_dir (str): Directory to save the CSV file.\n\n    Returns:\n        str: The path of the generated CSV file.\n    \"\"\"\n    # Define backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    \n    # Create output and backup directories if they don't exist\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    # Initialize list for weather data\n    weather_data = []\n    \n    # Get current time\n    current_time = datetime.now()\n    \n    for i in range(hours):\n        # Calculate time for the current hour\n        time = current_time + timedelta(hours=i)\n        time_str = time.strftime('%Y-%m-%d %H:%M:%S')\n        \n        # Randomly select a weather condition\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        \n        # Append the data to the list\n        weather_data.append({'Time': time_str, 'Condition': condition})\n    \n    # Define CSV filename with timestamp\n    timestamp = current_time.strftime('%Y%m%d_%H%M%S')\n    filename = f'weather_data_{timestamp}.csv'\n    csv_path = os.path.join(output_dir, filename)\n    \n    # Write data to CSV\n    with open(csv_path, mode='w', newline='') as csvfile:\n        fieldnames = ['Time', 'Condition']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # Write header\n        writer.writeheader()\n        \n        # Write each row of weather data\n        for data in weather_data:\n            writer.writerow(data)\n    \n    # Backup the CSV file to the backup directory\n    shutil.copy(csv_path, backup_dir)\n    \n    return csv_path", "raw_solution": "```python\nimport csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates weather data for the specified number of hours, saves it to a CSV file,\n    and backs up the file to a backup directory.\n\n    Args:\n        hours (int): Number of hours to generate weather data for.\n        output_dir (str): Directory to save the CSV file.\n\n    Returns:\n        str: The path of the generated CSV file.\n    \"\"\"\n    # Define backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    \n    # Create output and backup directories if they don't exist\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    # Initialize list for weather data\n    weather_data = []\n    \n    # Get current time\n    current_time = datetime.now()\n    \n    for i in range(hours):\n        # Calculate time for the current hour\n        time = current_time + timedelta(hours=i)\n        time_str = time.strftime('%Y-%m-%d %H:%M:%S')\n        \n        # Randomly select a weather condition\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        \n        # Append the data to the list\n        weather_data.append({'Time': time_str, 'Condition': condition})\n    \n    # Define CSV filename with timestamp\n    timestamp = current_time.strftime('%Y%m%d_%H%M%S')\n    filename = f'weather_data_{timestamp}.csv'\n    csv_path = os.path.join(output_dir, filename)\n    \n    # Write data to CSV\n    with open(csv_path, mode='w', newline='') as csvfile:\n        fieldnames = ['Time', 'Condition']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # Write header\n        writer.writeheader()\n        \n        # Write each row of weather data\n        for data in weather_data:\n            writer.writerow(data)\n    \n    # Backup the CSV file to the backup directory\n    shutil.copy(csv_path, backup_dir)\n    \n    return csv_path\n\n# Example usage\nif __name__ == \"__main__\":\n    number_of_hours = 24  # Specify the number of hours you want to generate data for\n    generated_csv_path = task_func(number_of_hours)\n    print(f'Weather data CSV generated at: {generated_csv_path}')\n```"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func():\n    # Generate random goals and penalties for each team\n    data = {\n        'Team': TEAMS,\n        'Goals': [randint(0, 5) for _ in TEAMS],\n        'Penalties': [randint(0, 10) for _ in TEAMS]\n    }\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    df['Penalty Cost'] = df['Penalties'] * PENALTY_COST\n    \n    # Initialize the matplotlib figure\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Plot Goals\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0], palette='Blues_d')\n    axes[0].set_title('Goals per Team')\n    axes[0].set_ylabel('Number of Goals')\n    axes[0].set_xlabel('Team')\n    \n    # Plot Penalty Costs\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axes[1], palette='Reds_d')\n    axes[1].set_title('Penalty Costs per Team ($)')\n    axes[1].set_ylabel('Penalty Cost ($)')\n    axes[1].set_xlabel('Team')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    # Show plots\n    plt.show()\n    \n    return df, list(axes)", "raw_solution": "```python\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func():\n    # Generate random goals and penalties for each team\n    data = {\n        'Team': TEAMS,\n        'Goals': [randint(0, 5) for _ in TEAMS],\n        'Penalties': [randint(0, 10) for _ in TEAMS]\n    }\n    \n    # Create DataFrame\n    df = pd.DataFrame(data)\n    df['Penalty Cost'] = df['Penalties'] * PENALTY_COST\n    \n    # Initialize the matplotlib figure\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Plot Goals\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0], palette='Blues_d')\n    axes[0].set_title('Goals per Team')\n    axes[0].set_ylabel('Number of Goals')\n    axes[0].set_xlabel('Team')\n    \n    # Plot Penalty Costs\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axes[1], palette='Reds_d')\n    axes[1].set_title('Penalty Costs per Team ($)')\n    axes[1].set_ylabel('Penalty Cost ($)')\n    axes[1].set_xlabel('Team')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    # Show plots\n    plt.show()\n    \n    return df, list(axes)\n\n# Example usage:\nif __name__ == \"__main__\":\n    dataframe, plots = task_func()\n    print(dataframe)\n```"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    \"\"\"\n    Creates a DataFrame with random integers between 0 and 9, counts non-zero values in each column,\n    and visualizes the counts using a bar plot.\n\n    Parameters:\n        rows (int): Number of rows in the DataFrame.\n\n    Returns:\n        tuple: A tuple containing the generated DataFrame and the matplotlib Axes object.\n    \"\"\"\n    # Generate random integer DataFrame\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax, color='skyblue')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-zero Counts')\n    ax.set_title('Non-zero Values per Column')\n    plt.tight_layout()\n    \n    return (df, ax)", "raw_solution": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    \"\"\"\n    Creates a DataFrame with random integers between 0 and 9, counts non-zero values in each column,\n    and visualizes the counts using a bar plot.\n\n    Parameters:\n        rows (int): Number of rows in the DataFrame.\n\n    Returns:\n        tuple: A tuple containing the generated DataFrame and the matplotlib Axes object.\n    \"\"\"\n    # Generate random integer DataFrame\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values in each column\n    non_zero_counts = (df != 0).sum()\n    \n    # Create bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax, color='skyblue')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-zero Counts')\n    ax.set_title('Non-zero Values per Column')\n    plt.tight_layout()\n    \n    return (df, ax)\n\nif __name__ == \"__main__\":\n    # Example usage\n    number_of_rows = 100\n    dataframe, axes = task_func(number_of_rows)\n    \n    # Display the DataFrame\n    print(\"Generated DataFrame:\")\n    print(dataframe)\n    \n    # Show the plot\n    plt.show()\n```"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Generates a DataFrame with student grades, calculates statistics, and creates a bar plot.\n\n    Parameters:\n        num_students (int): Number of students to generate grades for.\n\n    Returns:\n        Tuple[pd.DataFrame, plt.Axes]: A tuple containing the DataFrame and the plot's Axes object.\n    \"\"\"\n    # Define course names\n    courses = ['Math', 'Physics', 'Chemistry', 'Biology', 'English']\n\n    # Generate student names\n    students = [f'Student{i+1}' for i in range(num_students)]\n\n    # Assign random grades between 0 and 100 for each student in each course\n    grades = np.random.randint(0, 101, size=(num_students, len(courses)))\n\n    # Create DataFrame with students as rows and courses as columns\n    df = pd.DataFrame(grades, columns=courses, index=students)\n\n    # Calculate average grade for each course\n    avg_grades = df.mean()\n\n    # Calculate number of students with passing grades (>= 60) for each course\n    passing_counts = (df >= 60).sum()\n\n    # Create a bar plot\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n\n    # Bar for average grades\n    bar_width = 0.4\n    index = np.arange(len(courses))\n    bars1 = ax1.bar(index - bar_width/2, avg_grades, bar_width, label='Average Grade', color='skyblue')\n\n    # Bar for passing grade counts\n    bars2 = ax1.bar(index + bar_width/2, passing_counts, bar_width, label='Passing Count', color='salmon')\n\n    # Set labels and title\n    ax1.set_xlabel('Courses')\n    ax1.set_ylabel('Average Grade / Passing Count')\n    ax1.set_title('Course-wise Average and Passing Grade Counts')\n    ax1.set_xticks(index)\n    ax1.set_xticklabels(courses)\n    ax1.legend()\n\n    # Annotate bars with their values\n    for bar in bars1 + bars2:\n        height = bar.get_height()\n        ax1.annotate(f'{height:.1f}' if bar in bars1 else f'{int(height)}',\n                     xy=(bar.get_x() + bar.get_width() / 2, height),\n                     xytext=(0, 3),  # 3 points vertical offset\n                     textcoords=\"offset points\",\n                     ha='center', va='bottom')\n\n    plt.tight_layout()\n\n    return df, ax1", "raw_solution": "```python\nfrom random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students: int) -> Tuple[pd.DataFrame, plt.Axes]:\n    \"\"\"\n    Generates a DataFrame with student grades, calculates statistics, and creates a bar plot.\n\n    Parameters:\n        num_students (int): Number of students to generate grades for.\n\n    Returns:\n        Tuple[pd.DataFrame, plt.Axes]: A tuple containing the DataFrame and the plot's Axes object.\n    \"\"\"\n    # Define course names\n    courses = ['Math', 'Physics', 'Chemistry', 'Biology', 'English']\n\n    # Generate student names\n    students = [f'Student{i+1}' for i in range(num_students)]\n\n    # Assign random grades between 0 and 100 for each student in each course\n    grades = np.random.randint(0, 101, size=(num_students, len(courses)))\n\n    # Create DataFrame with students as rows and courses as columns\n    df = pd.DataFrame(grades, columns=courses, index=students)\n\n    # Calculate average grade for each course\n    avg_grades = df.mean()\n\n    # Calculate number of students with passing grades (>= 60) for each course\n    passing_counts = (df >= 60).sum()\n\n    # Create a bar plot\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n\n    # Bar for average grades\n    bar_width = 0.4\n    index = np.arange(len(courses))\n    bars1 = ax1.bar(index - bar_width/2, avg_grades, bar_width, label='Average Grade', color='skyblue')\n\n    # Bar for passing grade counts\n    bars2 = ax1.bar(index + bar_width/2, passing_counts, bar_width, label='Passing Count', color='salmon')\n\n    # Set labels and title\n    ax1.set_xlabel('Courses')\n    ax1.set_ylabel('Average Grade / Passing Count')\n    ax1.set_title('Course-wise Average and Passing Grade Counts')\n    ax1.set_xticks(index)\n    ax1.set_xticklabels(courses)\n    ax1.legend()\n\n    # Annotate bars with their values\n    for bar in bars1 + bars2:\n        height = bar.get_height()\n        ax1.annotate(f'{height:.1f}' if bar in bars1 else f'{int(height)}',\n                     xy=(bar.get_x() + bar.get_width() / 2, height),\n                     xytext=(0, 3),  # 3 points vertical offset\n                     textcoords=\"offset points\",\n                     ha='center', va='bottom')\n\n    plt.tight_layout()\n\n    return df, ax1\n```"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \n    Parameters:\n        array (np.ndarray): 2D NumPy array.\n        target_value (float or int): The value to match in the first column of the array.\n        \n    Returns:\n        tuple: (popt, ax)\n            popt (tuple): Optimized parameters of the exponential decay function.\n            ax (matplotlib.axes.Axes): Matplotlib Axes object with the plot.\n    \"\"\"\n    # Find the row indices where the first column matches the target value\n    matching_indices = np.where(array[:, 0] == target_value)[0]\n    \n    if len(matching_indices) == 0:\n        raise ValueError(f\"No entries found with target value {target_value} in the first column.\")\n    \n    # Prepare data for fitting\n    x_data = np.arange(1, len(matching_indices) + 1)\n    y_data = matching_indices + 1  # Adding 1 to make it 1-based indexing if preferred\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Initial guess for the parameters\n    initial_guess = (y_data[0] - y_data[-1], 1.0, y_data[-1])\n    \n    try:\n        # Perform the curve fitting\n        popt, pcov = optimize.curve_fit(exp_decay, x_data, y_data, p0=initial_guess)\n    except RuntimeError as e:\n        raise RuntimeError(\"Curve fitting failed: \" + str(e))\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.scatter(x_data, y_data, label='Data', color='blue')\n    \n    # Generate smooth x values for the fitted curve\n    x_fit = np.linspace(x_data.min(), x_data.max(), 100)\n    y_fit = exp_decay(x_fit, *popt)\n    ax.plot(x_fit, y_fit, label='Exponential Decay Fit', color='red')\n    \n    ax.set_title(f'Exponential Decay Fit for Target Value {target_value}')\n    ax.set_xlabel('Occurrence Number')\n    ax.set_ylabel('Row Index')\n    ax.legend()\n    ax.grid(True)\n    \n    plt.close(fig)  # Prevents the plot from displaying immediately\n    \n    return (popt, ax)", "raw_solution": "```python\nimport matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \n    Parameters:\n        array (np.ndarray): 2D NumPy array.\n        target_value (float or int): The value to match in the first column of the array.\n        \n    Returns:\n        tuple: (popt, ax)\n            popt (tuple): Optimized parameters of the exponential decay function.\n            ax (matplotlib.axes.Axes): Matplotlib Axes object with the plot.\n    \"\"\"\n    # Find the row indices where the first column matches the target value\n    matching_indices = np.where(array[:, 0] == target_value)[0]\n    \n    if len(matching_indices) == 0:\n        raise ValueError(f\"No entries found with target value {target_value} in the first column.\")\n    \n    # Prepare data for fitting\n    x_data = np.arange(1, len(matching_indices) + 1)\n    y_data = matching_indices + 1  # Adding 1 to make it 1-based indexing if preferred\n    \n    # Define the exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n    \n    # Initial guess for the parameters\n    initial_guess = (y_data[0] - y_data[-1], 1.0, y_data[-1])\n    \n    try:\n        # Perform the curve fitting\n        popt, pcov = optimize.curve_fit(exp_decay, x_data, y_data, p0=initial_guess)\n    except RuntimeError as e:\n        raise RuntimeError(\"Curve fitting failed: \" + str(e))\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.scatter(x_data, y_data, label='Data', color='blue')\n    \n    # Generate smooth x values for the fitted curve\n    x_fit = np.linspace(x_data.min(), x_data.max(), 100)\n    y_fit = exp_decay(x_fit, *popt)\n    ax.plot(x_fit, y_fit, label='Exponential Decay Fit', color='red')\n    \n    ax.set_title(f'Exponential Decay Fit for Target Value {target_value}')\n    ax.set_xlabel('Occurrence Number')\n    ax.set_ylabel('Row Index')\n    ax.legend()\n    ax.grid(True)\n    \n    plt.close(fig)  # Prevents the plot from displaying immediately\n    \n    return (popt, ax)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample array\n    np.random.seed(0)\n    num_rows = 100\n    target = 5\n    # First column has some target and non-target values\n    first_col = np.random.choice([target, target + 1, target + 2], size=num_rows, p=[0.3, 0.5, 0.2])\n    # Fill the rest of the array with random data\n    other_cols = np.random.rand(num_rows, 3)\n    array = np.column_stack((first_col, other_cols))\n    \n    try:\n        popt, ax = task_func(array, target)\n        print(f\"Optimized parameters: a={popt[0]:.4f}, b={popt[1]:.4f}, c={popt[2]:.4f}\")\n        \n        # To display the plot, uncomment the following lines:\n        # fig = ax.get_figure()\n        # fig.show()\n    except Exception as e:\n        print(str(e))\n```"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\ndef task_func(texts, num_topics):\n    \"\"\"\n    Extracts topics from a collection of text documents using NMF.\n\n    Parameters:\n        texts (list of str): The input text documents.\n        num_topics (int): The number of topics to extract.\n\n    Returns:\n        list of list of str: A list where each element is a list of words representing a topic.\n    \"\"\"\n    # Preprocess texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        tokens = text.split()\n        tokens = [word for word in tokens if word not in STOPWORDS]\n        processed_texts.append(' '.join(tokens))\n    \n    # Vectorize using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n    \n    # Apply NMF\n    nmf_model = NMF(n_components=num_topics, random_state=42)\n    nmf_model.fit(tfidf_matrix)\n    \n    # Get feature names\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf_model.components_):\n        # Get indices of top 10 words in the topic\n        top_indices = topic.argsort()[::-1][:10]\n        top_words = [feature_names[i] for i in top_indices]\n        topics.append(top_words)\n    \n    return topics", "raw_solution": "```python\nimport re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = set(nltk.corpus.stopwords.words('english'))\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    Extracts topics from a collection of text documents using NMF.\n\n    Parameters:\n        texts (list of str): The input text documents.\n        num_topics (int): The number of topics to extract.\n\n    Returns:\n        list of list of str: A list where each element is a list of words representing a topic.\n    \"\"\"\n    # Preprocess texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        tokens = text.split()\n        tokens = [word for word in tokens if word not in STOPWORDS]\n        processed_texts.append(' '.join(tokens))\n    \n    # Vectorize using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n    \n    # Apply NMF\n    nmf_model = NMF(n_components=num_topics, random_state=42)\n    nmf_model.fit(tfidf_matrix)\n    \n    # Get feature names\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Extract topics\n    topics = []\n    for topic_idx, topic in enumerate(nmf_model.components_):\n        # Get indices of top 10 words in the topic\n        top_indices = topic.argsort()[::-1][:10]\n        top_words = [feature_names[i] for i in top_indices]\n        topics.append(top_words)\n    \n    return topics\n\n# Example usage\nif __name__ == \"__main__\":\n    sample_texts = [\n        \"Natural language processing enables computers to understand human language.\",\n        \"Machine learning algorithms can discover patterns in data.\",\n        \"Deep learning is a subset of machine learning with neural networks.\",\n        \"Artificial intelligence encompasses machine learning and deep learning.\",\n        \"Data science involves statistics, data analysis, and machine learning.\"\n    ]\n    num_topics = 2\n    extracted_topics = task_func(sample_texts, num_topics)\n    for idx, topic in enumerate(extracted_topics, 1):\n        print(f\"Topic {idx}: {', '.join(topic)}\")\n```"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generates word vectors from a list of texts using gensim's Word2Vec model.\n    \n    Parameters:\n        texts (list of str): The input texts to process.\n        stopwords (set of str, optional): A set of stop words to remove. \n            If None, NLTK's English stopwords are used.\n    \n    Returns:\n        Word2Vec: A trained Word2Vec model.\n    \"\"\"\n    # Download stopwords if not provided\n    if stopwords is None:\n        nltk.download('stopwords', quiet=True)\n        from nltk.corpus import stopwords as sw\n        stopwords = set(sw.words('english'))\n    \n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space and lowercase\n        cleaned = ALPHANUMERIC.sub(' ', text).lower()\n        # Tokenize by splitting on whitespace\n        tokens = cleaned.split()\n        # Remove stopwords\n        tokens = [word for word in tokens if word not in stopwords]\n        cleaned_texts.append(tokens)\n    \n    # Train Word2Vec model\n    model = Word2Vec(\n        sentences=cleaned_texts,\n        vector_size=100,  # You can adjust the size as needed\n        window=5,         # Context window size\n        min_count=1,      # Minimum frequency count of words\n        workers=4,        # Number of worker threads\n        sg=0              # 0 for CBOW, 1 for skip-gram\n    )\n    \n    return model", "raw_solution": "```python\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generates word vectors from a list of texts using gensim's Word2Vec model.\n    \n    Parameters:\n        texts (list of str): The input texts to process.\n        stopwords (set of str, optional): A set of stop words to remove. \n            If None, NLTK's English stopwords are used.\n    \n    Returns:\n        Word2Vec: A trained Word2Vec model.\n    \"\"\"\n    # Download stopwords if not provided\n    if stopwords is None:\n        nltk.download('stopwords', quiet=True)\n        from nltk.corpus import stopwords as sw\n        stopwords = set(sw.words('english'))\n    \n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space and lowercase\n        cleaned = ALPHANUMERIC.sub(' ', text).lower()\n        # Tokenize by splitting on whitespace\n        tokens = cleaned.split()\n        # Remove stopwords\n        tokens = [word for word in tokens if word not in stopwords]\n        cleaned_texts.append(tokens)\n    \n    # Train Word2Vec model\n    model = Word2Vec(\n        sentences=cleaned_texts,\n        vector_size=100,  # You can adjust the size as needed\n        window=5,         # Context window size\n        min_count=1,      # Minimum frequency count of words\n        workers=4,        # Number of worker threads\n        sg=0              # 0 for CBOW, 1 for skip-gram\n    )\n    \n    return model\n\n# Example usage\nif __name__ == \"__main__\":\n    sample_texts = [\n        \"This is a sample sentence, showing off the stop words filtration.\",\n        \"Another example! With different: punctuation; and StopWords.\",\n        \"Gensim's Word2Vec is a powerful tool for natural language processing.\",\n        \"Cleaning text data is an essential step in building machine learning models.\"\n    ]\n    \n    model = task_func(sample_texts)\n    \n    # Display the vocabulary\n    print(\"Vocabulary:\")\n    print(model.wv.index_to_key)\n    \n    # Display the vector for a specific word\n    word = 'sample'\n    if word in model.wv:\n        print(f\"\\nVector for '{word}':\")\n        print(model.wv[word])\n    else:\n        print(f\"\\nWord '{word}' not in vocabulary.\")\n```"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in the specified directory. \n    Reads each JSON file alphabetically into a DataFrame, \n    adds a \"Source\" column with the filename, \n    and moves the processed files to a \"processed\" subdirectory.\n    \n    Args:\n        path (str): The directory path containing JSON files.\n        \n    Returns:\n        pd.DataFrame: A DataFrame containing data from all processed files.\n    \"\"\"\n    # Ensure the provided path exists\n    if not os.path.isdir(path):\n        raise ValueError(f\"The provided path '{path}' is not a valid directory.\")\n    \n    # Define the processed directory path\n    processed_dir = os.path.join(path, \"processed\")\n    \n    # Create the processed directory if it doesn't exist\n    os.makedirs(processed_dir, exist_ok=True)\n    \n    # List all JSON files in the directory, sorted alphabetically\n    json_files = sorted([f for f in os.listdir(path) if f.lower().endswith('.json')])\n    \n    # Initialize a list to hold DataFrames\n    df_list = []\n    \n    # Process each JSON file\n    for filename in json_files:\n        file_path = os.path.join(path, filename)\n        \n        try:\n            # Read JSON file into a DataFrame\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n            df = pd.DataFrame(data)\n            \n            # Add the \"Source\" column with the filename\n            df['Source'] = filename\n            \n            # Append the DataFrame to the list\n            df_list.append(df)\n            \n            # Move the processed file to the processed directory\n            shutil.move(file_path, os.path.join(processed_dir, filename))\n            \n        except Exception as e:\n            print(f\"Error processing file {filename}: {e}\")\n    \n    # Concatenate all DataFrames if any were processed\n    if df_list:\n        combined_df = pd.concat(df_list, ignore_index=True)\n    else:\n        combined_df = pd.DataFrame()\n    \n    return combined_df", "raw_solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in the specified directory. \n    Reads each JSON file alphabetically into a DataFrame, \n    adds a \"Source\" column with the filename, \n    and moves the processed files to a \"processed\" subdirectory.\n    \n    Args:\n        path (str): The directory path containing JSON files.\n        \n    Returns:\n        pd.DataFrame: A DataFrame containing data from all processed files.\n    \"\"\"\n    # Ensure the provided path exists\n    if not os.path.isdir(path):\n        raise ValueError(f\"The provided path '{path}' is not a valid directory.\")\n    \n    # Define the processed directory path\n    processed_dir = os.path.join(path, \"processed\")\n    \n    # Create the processed directory if it doesn't exist\n    os.makedirs(processed_dir, exist_ok=True)\n    \n    # List all JSON files in the directory, sorted alphabetically\n    json_files = sorted([f for f in os.listdir(path) if f.lower().endswith('.json')])\n    \n    # Initialize a list to hold DataFrames\n    df_list = []\n    \n    # Process each JSON file\n    for filename in json_files:\n        file_path = os.path.join(path, filename)\n        \n        try:\n            # Read JSON file into a DataFrame\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = json.load(file)\n            df = pd.DataFrame(data)\n            \n            # Add the \"Source\" column with the filename\n            df['Source'] = filename\n            \n            # Append the DataFrame to the list\n            df_list.append(df)\n            \n            # Move the processed file to the processed directory\n            shutil.move(file_path, os.path.join(processed_dir, filename))\n            \n        except Exception as e:\n            print(f\"Error processing file {filename}: {e}\")\n    \n    # Concatenate all DataFrames if any were processed\n    if df_list:\n        combined_df = pd.concat(df_list, ignore_index=True)\n    else:\n        combined_df = pd.DataFrame()\n    \n    return combined_df\n\n# Example usage:\n# if __name__ == \"__main__\":\n#     directory_path = \"/path/to/your/json/files\"\n#     result_df = task_func(directory_path)\n#     print(result_df)\n```"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    directory = \"task_func_data\"\n    filename = \"Output.txt\"\n    filepath = os.path.join(directory, filename)\n    \n    # Ensure the directory exists\n    os.makedirs(directory, exist_ok=True)\n    \n    # Open the file in append mode\n    with open(filepath, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # If the file is empty, write the header\n        if os.path.getsize(filepath) == 0:\n            writer.writerow([\"Timestamp\", \"Temperature (\u00b0C)\", \"Humidity (%)\"])\n        \n        # Generate random sensor data\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        temperature = round(random.uniform(20.0, 30.0), 2)  # Temperature between 20\u00b0C and 30\u00b0C\n        humidity = round(random.uniform(30.0, 70.0), 2)     # Humidity between 30% and 70%\n        \n        # Write the data to the CSV file\n        writer.writerow([timestamp, temperature, humidity])\n    \n    # Store the path to return\n    file_path_to_return = os.path.abspath(filepath)\n    \n    # Delete the file after use\n    os.remove(filepath)\n    \n    return file_path_to_return", "raw_solution": "```python\nimport os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    directory = \"task_func_data\"\n    filename = \"Output.txt\"\n    filepath = os.path.join(directory, filename)\n    \n    # Ensure the directory exists\n    os.makedirs(directory, exist_ok=True)\n    \n    # Open the file in append mode\n    with open(filepath, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # If the file is empty, write the header\n        if os.path.getsize(filepath) == 0:\n            writer.writerow([\"Timestamp\", \"Temperature (\u00b0C)\", \"Humidity (%)\"])\n        \n        # Generate random sensor data\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        temperature = round(random.uniform(20.0, 30.0), 2)  # Temperature between 20\u00b0C and 30\u00b0C\n        humidity = round(random.uniform(30.0, 70.0), 2)     # Humidity between 30% and 70%\n        \n        # Write the data to the CSV file\n        writer.writerow([timestamp, temperature, humidity])\n    \n    # Store the path to return\n    file_path_to_return = os.path.abspath(filepath)\n    \n    # Delete the file after use\n    os.remove(filepath)\n    \n    return file_path_to_return\n\n# Example usage:\nif __name__ == \"__main__\":\n    path = task_func()\n    print(f\"CSV file was located at: {path} and has been deleted.\")\n```"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrapes data from the given URL and saves it to a CSV file.\n\n    Args:\n        url (str): The URL to scrape data from.\n\n    Returns:\n        str: The path to the CSV file where the data is saved.\n    \"\"\"\n    try:\n        # Fetch the content from the URL\n        response = urllib.request.urlopen(url)\n        html_content = response.read()\n    except Exception as e:\n        print(f\"Error fetching the URL: {e}\")\n        return CSV_FILE_PATH\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Example: Extract data from the first HTML table found\n    table = soup.find('table')\n    if not table:\n        print(\"No table found on the provided URL.\")\n        return CSV_FILE_PATH\n\n    # Extract table headers\n    headers = []\n    header_row = table.find('tr')\n    if header_row:\n        for th in header_row.find_all(['th', 'td']):\n            headers.append(th.get_text(strip=True))\n    else:\n        print(\"No header row found in the table.\")\n        return CSV_FILE_PATH\n\n    # Extract table rows\n    rows = []\n    for tr in table.find_all('tr')[1:]:  # Skip the header row\n        cells = tr.find_all(['td', 'th'])\n        row = [cell.get_text(strip=True) for cell in cells]\n        if row:\n            rows.append(row)\n\n    # Write the extracted data to a CSV file\n    try:\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csv_file:\n            writer = csv.writer(csv_file)\n            writer.writerow(headers)\n            writer.writerows(rows)\n        print(f\"Data successfully written to {CSV_FILE_PATH}\")\n    except Exception as e:\n        print(f\"Error writing to CSV file: {e}\")\n\n    return CSV_FILE_PATH", "raw_solution": "```python\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Scrapes data from the given URL and saves it to a CSV file.\n\n    Args:\n        url (str): The URL to scrape data from.\n\n    Returns:\n        str: The path to the CSV file where the data is saved.\n    \"\"\"\n    try:\n        # Fetch the content from the URL\n        response = urllib.request.urlopen(url)\n        html_content = response.read()\n    except Exception as e:\n        print(f\"Error fetching the URL: {e}\")\n        return CSV_FILE_PATH\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Example: Extract data from the first HTML table found\n    table = soup.find('table')\n    if not table:\n        print(\"No table found on the provided URL.\")\n        return CSV_FILE_PATH\n\n    # Extract table headers\n    headers = []\n    header_row = table.find('tr')\n    if header_row:\n        for th in header_row.find_all(['th', 'td']):\n            headers.append(th.get_text(strip=True))\n    else:\n        print(\"No header row found in the table.\")\n        return CSV_FILE_PATH\n\n    # Extract table rows\n    rows = []\n    for tr in table.find_all('tr')[1:]:  # Skip the header row\n        cells = tr.find_all(['td', 'th'])\n        row = [cell.get_text(strip=True) for cell in cells]\n        if row:\n            rows.append(row)\n\n    # Write the extracted data to a CSV file\n    try:\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csv_file:\n            writer = csv.writer(csv_file)\n            writer.writerow(headers)\n            writer.writerows(rows)\n        print(f\"Data successfully written to {CSV_FILE_PATH}\")\n    except Exception as e:\n        print(f\"Error writing to CSV file: {e}\")\n\n    return CSV_FILE_PATH\n\n# Example usage:\nif __name__ == \"__main__\":\n    url_to_scrape = 'https://example.com/data-table'\n    csv_path = task_func(url_to_scrape)\n    print(f\"CSV file saved at: {csv_path}\")\n```\n\n**Explanation:**\n\n1. **Imports and Constants:**\n   - The script starts by importing necessary modules: `urllib.request` for handling URL requests, `BeautifulSoup` from `bs4` for parsing HTML content, `csv` for writing CSV files, and `os` for interacting with the operating system.\n   - `CSV_FILE_PATH` is defined as the name of the CSV file where the scraped data will be saved.\n\n2. **`task_func` Function:**\n   - **Fetching Content:** The function takes a URL as input and attempts to fetch its content. It handles exceptions that may occur during this process.\n   - **Parsing HTML:** Using BeautifulSoup, it parses the fetched HTML content.\n   - **Extracting Data:** \n     - The example focuses on extracting data from the first HTML `<table>` found on the page.\n     - It retrieves headers from the first row of the table.\n     - It then iterates over the remaining rows to extract cell data.\n   - **Writing to CSV:** The extracted headers and rows are written to the specified CSV file. Any errors during this process are caught and printed.\n   - **Return Value:** After successfully writing to the CSV, the function returns the path to the CSV file.\n\n3. **Example Usage:**\n   - The `if __name__ == \"__main__\":` block allows the script to be run as a standalone program.\n   - Replace `'https://example.com/data-table'` with the actual URL you intend to scrape.\n   - After running, it will print the path to the saved CSV file.\n\n**Note:** \n- This script assumes that the target webpage contains at least one HTML table. If the structure of the webpage is different, you might need to adjust the parsing logic accordingly.\n- Always ensure you have permission to scrape the target website and that you comply with its `robots.txt` and terms of service."}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Validate that data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"If data is not a DataFrame.\")\n    \n    # Validate that data is not empty\n    if data.empty:\n        raise ValueError(\"If data is empty.\")\n    \n    # Validate that target_column exists in data\n    if target_column not in data.columns:\n        raise ValueError(\"If target_column ist not a column of data.\")\n    \n    # Validate that all data is numeric\n    if not all(np.issubdtype(dtype, np.number) for dtype in data.dtypes):\n        raise ValueError(\"If data contains values that are not numeric.\")\n    \n    # Validate that random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"If random_state is not an integer.\")\n    \n    # Validate that test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"If test_size is not between 0 and 1.\")\n    \n    # Separate features and target\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    \n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model's score on the test set\n    return model.score(X_test, y_test)", "raw_solution": "```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Validate that data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"If data is not a DataFrame.\")\n    \n    # Validate that data is not empty\n    if data.empty:\n        raise ValueError(\"If data is empty.\")\n    \n    # Validate that target_column exists in data\n    if target_column not in data.columns:\n        raise ValueError(\"If target_column ist not a column of data.\")\n    \n    # Validate that all data is numeric\n    if not all(np.issubdtype(dtype, np.number) for dtype in data.dtypes):\n        raise ValueError(\"If data contains values that are not numeric.\")\n    \n    # Validate that random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"If random_state is not an integer.\")\n    \n    # Validate that test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"If test_size is not between 0 and 1.\")\n    \n    # Separate features and target\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    \n    # Initialize and train the Linear Regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model's score on the test set\n    return model.score(X_test, y_test)\n\n# Example usage\nif __name__ == \"__main__\":\n    rng = np.random.default_rng(seed=9)\n    data = pd.DataFrame({\n        'x1': rng.random(500),\n        'x2': rng.random(500),\n        'x3': rng.random(500)\n    })\n    data['y'] = data['x1'] * 2 + 1\n    result = task_func(data, 'y', random_state=9, test_size=0.1)\n    print(result)  # Expected output: 1.0\n```"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n             latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n             other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n             rng_seed=None):\n    \"\"\"\n    Generates a random DataFrame with user data.\n\n    Parameters:\n        start_year (int): The earliest year for Date of Birth.\n        end_year (int): The latest year for Date of Birth.\n        email_domain (str): The domain to use for email addresses.\n        latin_names (list): List of Latin names.\n        other_names (list): List of other names.\n        rng_seed (int, optional): Seed for random number generator.\n\n    Returns:\n        DataFrame: A pandas DataFrame containing the generated user data.\n                  Columns: 'ID', 'Name', 'Date of Birth', 'Email'.\n    \"\"\"\n    np.random.seed(rng_seed)\n    \n    # Total number of records\n    num_records = 100\n\n    # Generate IDs from 1 to 100\n    ids = np.arange(1, num_records + 1)\n\n    # Combine Latin and other names\n    names_list = latin_names + other_names\n\n    # Randomly select names\n    selected_names = np.random.choice(names_list, size=num_records)\n\n    # Correct improperly encoded Latin characters if any\n    # Here we ensure proper encoding by encoding to 'utf-8' and decoding back\n    corrected_names = [name.encode('utf-8').decode('utf-8') for name in selected_names]\n\n    # Generate random years between start_year and end_year inclusive\n    years = np.random.randint(start_year, end_year + 1, size=num_records)\n\n    # Generate random months between 1 and 12\n    months = np.random.randint(1, 13, size=num_records)\n\n    # Generate random days between 1 and 28 to avoid invalid dates\n    days = np.random.randint(1, 29, size=num_records)\n\n    # Create Date of Birth strings in 'YYYY-MM-DD 00:00:00' format with leading zeros\n    dob = [f\"{year:04d}-{month:02d}-{day:02d} 00:00:00\" for year, month, day in zip(years, months, days)]\n\n    # Generate Emails\n    emails = []\n    for name, year in zip(corrected_names, years):\n        # Lowercase the name\n        name_lower = name.lower()\n        # Remove any non-word characters (optional, based on requirements)\n        # name_clean = re.sub(r'\\W+', '', name_lower)\n        # Append year without leading zeros\n        email = f\"{name_lower}{int(year)}@{email_domain}\"\n        emails.append(email)\n\n    # Create the DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': corrected_names,\n        'Date of Birth': dob,\n        'Email': emails\n    })\n\n    return df", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n             latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n             other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n             rng_seed=None):\n    \"\"\"\n    Generates a random DataFrame with user data.\n\n    Parameters:\n        start_year (int): The earliest year for Date of Birth.\n        end_year (int): The latest year for Date of Birth.\n        email_domain (str): The domain to use for email addresses.\n        latin_names (list): List of Latin names.\n        other_names (list): List of other names.\n        rng_seed (int, optional): Seed for random number generator.\n\n    Returns:\n        DataFrame: A pandas DataFrame containing the generated user data.\n                  Columns: 'ID', 'Name', 'Date of Birth', 'Email'.\n    \"\"\"\n    np.random.seed(rng_seed)\n    \n    # Total number of records\n    num_records = 100\n\n    # Generate IDs from 1 to 100\n    ids = np.arange(1, num_records + 1)\n\n    # Combine Latin and other names\n    names_list = latin_names + other_names\n\n    # Randomly select names\n    selected_names = np.random.choice(names_list, size=num_records)\n\n    # Correct improperly encoded Latin characters if any\n    # Here we ensure proper encoding by encoding to 'utf-8' and decoding back\n    corrected_names = [name.encode('utf-8').decode('utf-8') for name in selected_names]\n\n    # Generate random years between start_year and end_year inclusive\n    years = np.random.randint(start_year, end_year + 1, size=num_records)\n\n    # Generate random months between 1 and 12\n    months = np.random.randint(1, 13, size=num_records)\n\n    # Generate random days between 1 and 28 to avoid invalid dates\n    days = np.random.randint(1, 29, size=num_records)\n\n    # Create Date of Birth strings in 'YYYY-MM-DD 00:00:00' format with leading zeros\n    dob = [f\"{year:04d}-{month:02d}-{day:02d} 00:00:00\" for year, month, day in zip(years, months, days)]\n\n    # Generate Emails\n    emails = []\n    for name, year in zip(corrected_names, years):\n        # Lowercase the name\n        name_lower = name.lower()\n        # Remove any non-word characters (optional, based on requirements)\n        # name_clean = re.sub(r'\\W+', '', name_lower)\n        # Append year without leading zeros\n        email = f\"{name_lower}{int(year)}@{email_domain}\"\n        emails.append(email)\n\n    # Create the DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': corrected_names,\n        'Date of Birth': dob,\n        'Email': emails\n    })\n\n    return df\n\n# Example usage\nif __name__ == \"__main__\":\n    df = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)\n    print(df)\n```"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median for each key,\n    writes the results to a CSV file, and returns a dictionary with the statistics.\n\n    Parameters:\n    - input_file: Path to the input JSON file.\n    - output_file: Path to the output CSV file.\n\n    Returns:\n    - dict: A dictionary where each key is a field from the input JSON and each value\n            is another dictionary with the mean and median of that field.\n    \"\"\"\n    # Read data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    if not isinstance(data, list):\n        raise ValueError(\"JSON file must contain a list of dictionaries.\")\n    \n    if not data:\n        raise ValueError(\"JSON file is empty.\")\n    \n    # Initialize a defaultdict to collect values for each key\n    values_dict = defaultdict(list)\n    \n    for entry in data:\n        if not isinstance(entry, dict):\n            raise ValueError(\"Each item in the JSON list must be a dictionary.\")\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                values_dict[key].append(value)\n            else:\n                raise ValueError(f\"Value for key '{key}' is not a number.\")\n    \n    # Calculate mean and median for each key\n    stats_dict = {}\n    for key, values in values_dict.items():\n        mean_val = np.mean(values)\n        median_val = np.median(values)\n        stats_dict[key] = {\"mean\": mean_val, \"median\": median_val}\n    \n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['field', 'mean', 'median']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for key, stats in stats_dict.items():\n            writer.writerow({\n                'field': key,\n                'mean': stats['mean'],\n                'median': stats['median']\n            })\n    \n    return stats_dict", "raw_solution": "```python\nimport numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    \"\"\"\n    Reads a list of dictionaries from a JSON file, calculates the mean and median for each key,\n    writes the results to a CSV file, and returns a dictionary with the statistics.\n\n    Parameters:\n    - input_file: Path to the input JSON file.\n    - output_file: Path to the output CSV file.\n\n    Returns:\n    - dict: A dictionary where each key is a field from the input JSON and each value\n            is another dictionary with the mean and median of that field.\n    \"\"\"\n    # Read data from JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    if not isinstance(data, list):\n        raise ValueError(\"JSON file must contain a list of dictionaries.\")\n    \n    if not data:\n        raise ValueError(\"JSON file is empty.\")\n    \n    # Initialize a defaultdict to collect values for each key\n    values_dict = defaultdict(list)\n    \n    for entry in data:\n        if not isinstance(entry, dict):\n            raise ValueError(\"Each item in the JSON list must be a dictionary.\")\n        for key, value in entry.items():\n            if isinstance(value, (int, float)):\n                values_dict[key].append(value)\n            else:\n                raise ValueError(f\"Value for key '{key}' is not a number.\")\n    \n    # Calculate mean and median for each key\n    stats_dict = {}\n    for key, values in values_dict.items():\n        mean_val = np.mean(values)\n        median_val = np.median(values)\n        stats_dict[key] = {\"mean\": mean_val, \"median\": median_val}\n    \n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as csvfile:\n        fieldnames = ['field', 'mean', 'median']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for key, stats in stats_dict.items():\n            writer.writerow({\n                'field': key,\n                'mean': stats['mean'],\n                'median': stats['median']\n            })\n    \n    return stats_dict\n\n# Example usage:\nif __name__ == \"__main__\":\n    input_json = 'input_data.json'\n    output_csv = 'output_stats.csv'\n    results = task_func(input_json, output_csv)\n    print(\"Calculated Statistics:\")\n    print(json.dumps(results, indent=4))\n```"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Processes files from a dictionary by checking if each file exists and has content.\n    Copies qualifying files to the target directory.\n\n    Args:\n        kwargs (dict): A dictionary where keys are identifiers and values are file paths.\n        target_dir (str, optional): The directory to copy files to. Defaults to \"non_none_files\".\n\n    Returns:\n        list: A list of full file paths that were copied.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    copied_files = []\n\n    for key, file_path in kwargs.items():\n        src_path = Path(file_path)\n        \n        # Check if the file exists and is a file\n        if src_path.is_file():\n            # Check if the file has content (size > 0)\n            if src_path.stat().st_size > 0:\n                destination = target_path / src_path.name\n                try:\n                    shutil.copy2(src_path, destination)\n                    copied_files.append(str(destination.resolve()))\n                    print(f\"Copied: {src_path} to {destination}\")\n                except Exception as e:\n                    print(f\"Failed to copy {src_path} to {destination}. Error: {e}\")\n            else:\n                print(f\"Skipped (empty file): {src_path}\")\n        else:\n            print(f\"Skipped (file does not exist): {src_path}\")\n\n    return copied_files", "raw_solution": "```python\nimport os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Processes files from a dictionary by checking if each file exists and has content.\n    Copies qualifying files to the target directory.\n\n    Args:\n        kwargs (dict): A dictionary where keys are identifiers and values are file paths.\n        target_dir (str, optional): The directory to copy files to. Defaults to \"non_none_files\".\n\n    Returns:\n        list: A list of full file paths that were copied.\n    \"\"\"\n    # Create the target directory if it doesn't exist\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    copied_files = []\n\n    for key, file_path in kwargs.items():\n        src_path = Path(file_path)\n        \n        # Check if the file exists and is a file\n        if src_path.is_file():\n            # Check if the file has content (size > 0)\n            if src_path.stat().st_size > 0:\n                destination = target_path / src_path.name\n                try:\n                    shutil.copy2(src_path, destination)\n                    copied_files.append(str(destination.resolve()))\n                    print(f\"Copied: {src_path} to {destination}\")\n                except Exception as e:\n                    print(f\"Failed to copy {src_path} to {destination}. Error: {e}\")\n            else:\n                print(f\"Skipped (empty file): {src_path}\")\n        else:\n            print(f\"Skipped (file does not exist): {src_path}\")\n\n    return copied_files\n\nif __name__ == \"__main__\":\n    # Example usage\n    files_dict = {\n        \"file1\": \"path/to/file1.txt\",\n        \"file2\": \"path/to/file2.txt\",\n        \"file3\": \"path/to/nonexistent_file.txt\",\n        \"file4\": \"path/to/empty_file.txt\"\n    }\n\n    copied = task_func(files_dict, target_dir=\"copied_files\")\n    print(\"\\nCopied Files:\")\n    for file in copied:\n        print(file)\n```"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in the specified directory matching the given pattern,\n    creates new files with altered names while preserving the content.\n\n    Args:\n        directory (str): The directory path containing CSV files to process.\n        pattern (str): The regex pattern to match filenames. Defaults to r'^(.*?)-\\d+\\.csv$'.\n\n    Returns:\n        list: A list of strings, each being the filename of a new CSV file created.\n    \"\"\"\n    new_files = []\n    dir_path = Path(directory)\n    \n    if not dir_path.is_dir():\n        raise ValueError(f\"The path {directory} is not a valid directory.\")\n\n    regex = re.compile(pattern)\n    \n    for file_path in dir_path.iterdir():\n        if file_path.is_file() and file_path.suffix.lower() == '.csv':\n            match = regex.match(file_path.name)\n            if match:\n                base_name = match.group(1)\n                new_filename = f\"{base_name}.csv\"\n                new_file_path = dir_path / new_filename\n\n                # Avoid overwriting existing new file if already created\n                if new_file_path.exists():\n                    print(f\"File {new_filename} already exists. Skipping.\")\n                    continue\n\n                # Copy the content\n                with file_path.open('r', newline='', encoding='utf-8') as src_file, \\\n                     new_file_path.open('w', newline='', encoding='utf-8') as dst_file:\n                    reader = csv.reader(src_file)\n                    writer = csv.writer(dst_file)\n                    for row in reader:\n                        writer.writerow(row)\n                \n                new_files.append(new_filename)\n    \n    return new_files", "raw_solution": "```python\nimport re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    \"\"\"\n    Processes CSV files in the specified directory matching the given pattern,\n    creates new files with altered names while preserving the content.\n\n    Args:\n        directory (str): The directory path containing CSV files to process.\n        pattern (str): The regex pattern to match filenames. Defaults to r'^(.*?)-\\d+\\.csv$'.\n\n    Returns:\n        list: A list of strings, each being the filename of a new CSV file created.\n    \"\"\"\n    new_files = []\n    dir_path = Path(directory)\n    \n    if not dir_path.is_dir():\n        raise ValueError(f\"The path {directory} is not a valid directory.\")\n\n    regex = re.compile(pattern)\n    \n    for file_path in dir_path.iterdir():\n        if file_path.is_file() and file_path.suffix.lower() == '.csv':\n            match = regex.match(file_path.name)\n            if match:\n                base_name = match.group(1)\n                new_filename = f\"{base_name}.csv\"\n                new_file_path = dir_path / new_filename\n\n                # Avoid overwriting existing new file if already created\n                if new_file_path.exists():\n                    print(f\"File {new_filename} already exists. Skipping.\")\n                    continue\n\n                # Copy the content\n                with file_path.open('r', newline='', encoding='utf-8') as src_file, \\\n                     new_file_path.open('w', newline='', encoding='utf-8') as dst_file:\n                    reader = csv.reader(src_file)\n                    writer = csv.writer(dst_file)\n                    for row in reader:\n                        writer.writerow(row)\n                \n                new_files.append(new_filename)\n    \n    return new_files\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Process and rename CSV files based on a pattern.\")\n    parser.add_argument('directory', type=str, help='Directory containing CSV files to process.')\n    parser.add_argument('--pattern', type=str, default=r'^(.*?)-\\d+\\.csv$', help='Regex pattern to match filenames.')\n\n    args = parser.parse_args()\n\n    try:\n        created_files = task_func(args.directory, args.pattern)\n        if created_files:\n            print(\"New files created:\")\n            for f in created_files:\n                print(f)\n        else:\n            print(\"No files were created.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n```"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    regex = re.compile(pattern)\n\n    for filename in os.listdir(directory):\n        match = regex.match(filename)\n        if match:\n            prefix = match.group(1)\n            extract_dir = os.path.join(directory, prefix)\n            zip_path = os.path.join(directory, filename)\n\n            os.makedirs(extract_dir, exist_ok=True)\n\n            try:\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(extract_dir)\n                extracted_dirs.append(extract_dir)\n                print(f\"Extracted '{filename}' to '{extract_dir}'\")\n            except zipfile.BadZipFile:\n                print(f\"Error: '{filename}' is not a valid zip file.\")\n            except Exception as e:\n                print(f\"An error occurred while extracting '{filename}': {e}\")\n\n    return extracted_dirs", "raw_solution": "```python\nimport re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    regex = re.compile(pattern)\n\n    for filename in os.listdir(directory):\n        match = regex.match(filename)\n        if match:\n            prefix = match.group(1)\n            extract_dir = os.path.join(directory, prefix)\n            zip_path = os.path.join(directory, filename)\n\n            os.makedirs(extract_dir, exist_ok=True)\n\n            try:\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(extract_dir)\n                extracted_dirs.append(extract_dir)\n                print(f\"Extracted '{filename}' to '{extract_dir}'\")\n            except zipfile.BadZipFile:\n                print(f\"Error: '{filename}' is not a valid zip file.\")\n            except Exception as e:\n                print(f\"An error occurred while extracting '{filename}': {e}\")\n\n    return extracted_dirs\n```"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nimport time\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    \"\"\"\n    Archives all files matching the given pattern and deletes the original files.\n\n    Args:\n        pattern (str): The glob pattern to match files.\n\n    Returns:\n        archive_file (str): The archive file path.\n    \"\"\"\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n    \n    if not files:\n        print(\"No files found matching the pattern.\")\n        return None\n\n    # Create a timestamp for the archive file name\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    archive_file = os.path.join(ARCHIVE_DIR, f'archive_{timestamp}.tar.gz')\n\n    try:\n        # Create the archive using tar\n        subprocess.check_call(['tar', '-czf', archive_file] + files)\n        print(f\"Archived files to {archive_file}\")\n\n        # Delete the original files\n        for file_path in files:\n            os.remove(file_path)\n            print(f\"Deleted original file: {file_path}\")\n\n        return archive_file\n\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred while creating the archive: {e}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return None", "raw_solution": "```python\nimport subprocess\nimport os\nimport glob\nimport time\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    Archives all files matching the given pattern and deletes the original files.\n\n    Args:\n        pattern (str): The glob pattern to match files.\n\n    Returns:\n        archive_file (str): The archive file path.\n    \"\"\"\n    # Ensure the archive directory exists\n    os.makedirs(ARCHIVE_DIR, exist_ok=True)\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n    \n    if not files:\n        print(\"No files found matching the pattern.\")\n        return None\n\n    # Create a timestamp for the archive file name\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    archive_file = os.path.join(ARCHIVE_DIR, f'archive_{timestamp}.tar.gz')\n\n    try:\n        # Create the archive using tar\n        subprocess.check_call(['tar', '-czf', archive_file] + files)\n        print(f\"Archived files to {archive_file}\")\n\n        # Delete the original files\n        for file_path in files:\n            os.remove(file_path)\n            print(f\"Deleted original file: {file_path}\")\n\n        return archive_file\n\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred while creating the archive: {e}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return None\n\n# Example usage:\nif __name__ == \"__main__\":\n    pattern = '/path/to/files/*.log'  # Replace with your file pattern\n    archive_path = task_func(pattern)\n    if archive_path:\n        print(f\"Files have been archived to: {archive_path}\")\n    else:\n        print(\"No archive was created.\")\n```"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Counts the total number of goals and penalties from a CSV file,\n    updates it with the given goals and penalties, and returns the totals.\n\n    Parameters:\n    - goals (int): Number of goals to add.\n    - penalties (int): Number of penalties to add.\n    - csv_file_path (str): Path to the CSV file.\n\n    Returns:\n    - Counter: A Counter object with total counts of goals and penalties.\n    \"\"\"\n    # Ensure the CSV file exists; if not, create it with headers\n    if not os.path.exists(csv_file_path):\n        with open(csv_file_path, 'w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow(['team', 'goals', 'penalties'])\n\n    # Append the new goals and penalties with a placeholder team name\n    with open(csv_file_path, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Updated Team', str(goals), str(penalties)])\n\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Read the CSV and sum up the goals and penalties\n    with open(csv_file_path, 'r', newline='') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            try:\n                total_goals += int(row['goals'])\n                total_penalties += int(row['penalties'])\n            except ValueError:\n                # Handle the case where goals or penalties are not integers\n                continue\n\n    return Counter({'goals': total_goals, 'penalties': total_penalties})", "raw_solution": "```python\nimport csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Counts the total number of goals and penalties from a CSV file,\n    updates it with the given goals and penalties, and returns the totals.\n\n    Parameters:\n    - goals (int): Number of goals to add.\n    - penalties (int): Number of penalties to add.\n    - csv_file_path (str): Path to the CSV file.\n\n    Returns:\n    - Counter: A Counter object with total counts of goals and penalties.\n    \"\"\"\n    # Ensure the CSV file exists; if not, create it with headers\n    if not os.path.exists(csv_file_path):\n        with open(csv_file_path, 'w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow(['team', 'goals', 'penalties'])\n\n    # Append the new goals and penalties with a placeholder team name\n    with open(csv_file_path, 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Updated Team', str(goals), str(penalties)])\n\n    # Initialize counters\n    total_goals = 0\n    total_penalties = 0\n\n    # Read the CSV and sum up the goals and penalties\n    with open(csv_file_path, 'r', newline='') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            try:\n                total_goals += int(row['goals'])\n                total_penalties += int(row['penalties'])\n            except ValueError:\n                # Handle the case where goals or penalties are not integers\n                continue\n\n    return Counter({'goals': total_goals, 'penalties': total_penalties})\n\n# Example usage\nif __name__ == '__main__':\n    # Setup the test CSV\n    setup_csv()\n    \n    # Define the path to the test CSV\n    test_csv_path = 'test_data/test_case_2.csv'\n    \n    # Add new goals and penalties\n    new_goals = 4\n    new_penalties = 1\n    total_counts = task_func(new_goals, new_penalties, csv_file_path=test_csv_path)\n    \n    # Output the total counts\n    print(total_counts)\n```"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Move files from source_dir to target_dir based on the specified file_pattern.\n\n    Args:\n        source_dir (str): The path to the source directory.\n        target_dir (str): The path to the target directory.\n        file_pattern (str): The regex pattern to match filenames.\n\n    Returns:\n        int: The number of files successfully moved.\n    \"\"\"\n    moved_files_count = 0\n\n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Compile the regex pattern for efficiency\n    pattern = re.compile(file_pattern)\n\n    # Iterate through all items in the source directory\n    for item in os.listdir(source_dir):\n        source_path = os.path.join(source_dir, item)\n        \n        # Check if the item is a file\n        if os.path.isfile(source_path):\n            # Check if the filename matches the pattern\n            if pattern.fullmatch(item):\n                target_path = os.path.join(target_dir, item)\n                try:\n                    shutil.move(source_path, target_path)\n                    moved_files_count += 1\n                except Exception as e:\n                    print(f\"Failed to move {item}: {e}\")\n\n    return moved_files_count", "raw_solution": "```python\nimport re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Move files from source_dir to target_dir based on the specified file_pattern.\n\n    Args:\n        source_dir (str): The path to the source directory.\n        target_dir (str): The path to the target directory.\n        file_pattern (str): The regex pattern to match filenames.\n\n    Returns:\n        int: The number of files successfully moved.\n    \"\"\"\n    moved_files_count = 0\n\n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Compile the regex pattern for efficiency\n    pattern = re.compile(file_pattern)\n\n    # Iterate through all items in the source directory\n    for item in os.listdir(source_dir):\n        source_path = os.path.join(source_dir, item)\n        \n        # Check if the item is a file\n        if os.path.isfile(source_path):\n            # Check if the filename matches the pattern\n            if pattern.fullmatch(item):\n                target_path = os.path.join(target_dir, item)\n                try:\n                    shutil.move(source_path, target_path)\n                    moved_files_count += 1\n                except Exception as e:\n                    print(f\"Failed to move {item}: {e}\")\n\n    return moved_files_count\n\n# Example usage:\nif __name__ == \"__main__\":\n    source = \"/path/to/source_directory\"\n    target = \"/path/to/target_directory\"\n    pattern = r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'\n    \n    count = task_func(source, target, pattern)\n    print(f\"Number of files moved: {count}\")\n```"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity and Levenshtein ratio between two texts.\n\n    Parameters:\n        text1 (str): The first text string.\n        text2 (str): The second text string.\n\n    Returns:\n        tuple: A tuple containing:\n            - cosine_similarity (float): Ranges from 0 to 1.\n            - levenshtein_ratio (float): Ranges from 0 to 1.\n    \"\"\"\n    # Clean the texts by removing non-alphanumeric characters except spaces and convert to lowercase\n    cleaned1 = ALPHANUMERIC.sub(' ', text1).lower()\n    cleaned2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Tokenize the texts into words\n    tokens1 = cleaned1.split()\n    tokens2 = cleaned2.split()\n\n    # Create term frequency counters\n    tf1 = Counter(tokens1)\n    tf2 = Counter(tokens2)\n\n    # Get the set of all unique terms in both texts\n    all_terms = set(tf1.keys()).union(tf2.keys())\n\n    # Create vectors for term frequencies\n    vec1 = np.array([tf1.get(term, 0) for term in all_terms])\n    vec2 = np.array([tf2.get(term, 0) for term in all_terms])\n\n    # Compute cosine similarity\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n\n    if norm1 == 0 or norm2 == 0:\n        cosine_similarity = 0.0\n    else:\n        cosine_similarity = dot_product / (norm1 * norm2)\n\n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(cleaned1, cleaned2)\n\n    return (cosine_similarity, levenshtein_ratio)", "raw_solution": "```python\nimport re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the cosine similarity and Levenshtein ratio between two texts.\n\n    Parameters:\n        text1 (str): The first text string.\n        text2 (str): The second text string.\n\n    Returns:\n        tuple: A tuple containing:\n            - cosine_similarity (float): Ranges from 0 to 1.\n            - levenshtein_ratio (float): Ranges from 0 to 1.\n    \"\"\"\n    # Clean the texts by removing non-alphanumeric characters except spaces and convert to lowercase\n    cleaned1 = ALPHANUMERIC.sub(' ', text1).lower()\n    cleaned2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Tokenize the texts into words\n    tokens1 = cleaned1.split()\n    tokens2 = cleaned2.split()\n\n    # Create term frequency counters\n    tf1 = Counter(tokens1)\n    tf2 = Counter(tokens2)\n\n    # Get the set of all unique terms in both texts\n    all_terms = set(tf1.keys()).union(tf2.keys())\n\n    # Create vectors for term frequencies\n    vec1 = np.array([tf1.get(term, 0) for term in all_terms])\n    vec2 = np.array([tf2.get(term, 0) for term in all_terms])\n\n    # Compute cosine similarity\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n\n    if norm1 == 0 or norm2 == 0:\n        cosine_similarity = 0.0\n    else:\n        cosine_similarity = dot_product / (norm1 * norm2)\n\n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(cleaned1, cleaned2)\n\n    return (cosine_similarity, levenshtein_ratio)\n\nif __name__ == \"__main__\":\n    # Example texts\n    text1 = \"Hello, world! This is a sample text.\"\n    text2 = \"hello world! This sample text is simple.\"\n\n    # Calculate similarities\n    cosine_sim, levenshtein_rat = task_func(text1, text2)\n\n    # Output the results\n    print(f\"Cosine Similarity: {cosine_sim:.4f}\")\n    print(f\"Levenshtein Ratio: {levenshtein_rat:.4f}\")\n```"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    # Validate that input is a list\n    if not isinstance(numbers, list):\n        raise TypeError(\"numbers must be a list of integers.\")\n    \n    # Validate that all elements are integers\n    if not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"All elements in numbers must be integers.\")\n    \n    # Validate that all numbers are non-negative\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"All numbers must be non-negative.\")\n    \n    # If the list is empty, return empty lists\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n    \n    # Calculate the sum of factorials for each permutation\n    sums = [sum(math.factorial(n) for n in perm) for perm in perms]\n    \n    return sums, perms", "raw_solution": "```python\nfrom functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    # Validate that input is a list\n    if not isinstance(numbers, list):\n        raise TypeError(\"numbers must be a list of integers.\")\n    \n    # Validate that all elements are integers\n    if not all(isinstance(n, int) for n in numbers):\n        raise TypeError(\"All elements in numbers must be integers.\")\n    \n    # Validate that all numbers are non-negative\n    if any(n < 0 for n in numbers):\n        raise ValueError(\"All numbers must be non-negative.\")\n    \n    # If the list is empty, return empty lists\n    if not numbers:\n        return [], []\n    \n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n    \n    # Calculate the sum of factorials for each permutation\n    sums = [sum(math.factorial(n) for n in perm) for perm in perms]\n    \n    return sums, perms\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        fac, perm = task_func([0, 4])\n        print(fac)   # Output: [25, 25]\n        print(perm)  # Output: [(0, 4), (4, 0)]\n    except (TypeError, ValueError) as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfers files from SOURCE_DIR to DEST_DIR based on specified EXTENSIONS.\n    Issues warnings for files that could not be transferred.\n    \n    Parameters:\n        SOURCE_DIR (str): Path to the source directory.\n        DEST_DIR (str): Path to the destination directory.\n        EXTENSIONS (list): List of file extensions to transfer (e.g., ['.txt', '.jpg']).\n    \n    Returns:\n        transferred_files (list): List of filenames that were successfully transferred.\n    \"\"\"\n    transferred_files = []\n\n    # Ensure SOURCE_DIR exists\n    if not os.path.isdir(SOURCE_DIR):\n        raise NotADirectoryError(f\"Source directory does not exist: {SOURCE_DIR}\")\n\n    # Create DEST_DIR if it does not exist\n    os.makedirs(DEST_DIR, exist_ok=True)\n\n    # Iterate over each extension\n    for ext in EXTENSIONS:\n        # Construct the search pattern\n        pattern = os.path.join(SOURCE_DIR, f'*{ext}')\n        # Use glob to find all files matching the pattern\n        for file_path in glob.glob(pattern):\n            try:\n                # Ensure it's a file\n                if os.path.isfile(file_path):\n                    # Get the base filename\n                    filename = os.path.basename(file_path)\n                    dest_path = os.path.join(DEST_DIR, filename)\n                    # Move the file\n                    shutil.move(file_path, dest_path)\n                    # Append to the list of transferred files\n                    transferred_files.append(filename)\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer {file_path}: {e}\")\n\n    return transferred_files", "raw_solution": "```python\nimport warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfers files from SOURCE_DIR to DEST_DIR based on specified EXTENSIONS.\n    Issues warnings for files that could not be transferred.\n    \n    Parameters:\n        SOURCE_DIR (str): Path to the source directory.\n        DEST_DIR (str): Path to the destination directory.\n        EXTENSIONS (list): List of file extensions to transfer (e.g., ['.txt', '.jpg']).\n    \n    Returns:\n        transferred_files (list): List of filenames that were successfully transferred.\n    \"\"\"\n    transferred_files = []\n\n    # Ensure SOURCE_DIR exists\n    if not os.path.isdir(SOURCE_DIR):\n        raise NotADirectoryError(f\"Source directory does not exist: {SOURCE_DIR}\")\n\n    # Create DEST_DIR if it does not exist\n    os.makedirs(DEST_DIR, exist_ok=True)\n\n    # Iterate over each extension\n    for ext in EXTENSIONS:\n        # Construct the search pattern\n        pattern = os.path.join(SOURCE_DIR, f'*{ext}')\n        # Use glob to find all files matching the pattern\n        for file_path in glob.glob(pattern):\n            try:\n                # Ensure it's a file\n                if os.path.isfile(file_path):\n                    # Get the base filename\n                    filename = os.path.basename(file_path)\n                    dest_path = os.path.join(DEST_DIR, filename)\n                    # Move the file\n                    shutil.move(file_path, dest_path)\n                    # Append to the list of transferred files\n                    transferred_files.append(filename)\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer {file_path}: {e}\")\n\n    return transferred_files\n\n# Example usage:\nif __name__ == \"__main__\":\n    SOURCE_DIR = '/path/to/source'\n    DEST_DIR = '/path/to/destination'\n    EXTENSIONS = ['.txt', '.jpg', '.png']\n    \n    start_time = time.time()\n    transferred = task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS)\n    end_time = time.time()\n    \n    print(f\"Transferred {len(transferred)} files:\")\n    for file in transferred:\n        print(f\"- {file}\")\n    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n```"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    \"\"\"\n    Normalize counts using z-score and weights using min-max scaling.\n\n    Parameters:\n    data (list of tuples): Each tuple contains (Item, Count, Weight)\n\n    Returns:\n    pandas.DataFrame: DataFrame with 'Item', 'Normalized Count', and 'Normalized Weight'\n    \"\"\"\n    # Create DataFrame from input data\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n    \n    # Normalize 'Count' using z-score\n    df['Normalized Count'] = zscore(df['Count'])\n    \n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Reshape 'Weight' for scaler and apply min-max scaling\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n    \n    # Select the desired columns\n    result_df = df[['Item', 'Normalized Count', 'Normalized Weight']]\n    \n    return result_df", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    \"\"\"\n    Normalize counts using z-score and weights using min-max scaling.\n\n    Parameters:\n    data (list of tuples): Each tuple contains (Item, Count, Weight)\n\n    Returns:\n    pandas.DataFrame: DataFrame with 'Item', 'Normalized Count', and 'Normalized Weight'\n    \"\"\"\n    # Create DataFrame from input data\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n    \n    # Normalize 'Count' using z-score\n    df['Normalized Count'] = zscore(df['Count'])\n    \n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Reshape 'Weight' for scaler and apply min-max scaling\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n    \n    # Select the desired columns\n    result_df = df[['Item', 'Normalized Count', 'Normalized Weight']]\n    \n    return result_df\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_data = [\n        ('A', 10, 100),\n        ('B', 20, 150),\n        ('C', 15, 200),\n        ('D', 25, 250),\n        ('E', 30, 300)\n    ]\n    \n    normalized_df = task_func(sample_data)\n    print(normalized_df)\n```"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\ndef task_func(data_list=[('a', 1, 2.1),\n                          ('b', 2, 3.2),\n                          ('c', 3, 4.3),\n                          ('d', 4, 5.4),\n                          ('e', 5, 6.5)]):\n    \"\"\"\n    Calculate the mean of numerical values in each position across tuples in a list.\n    Non-numeric values are ignored. Returns a pandas DataFrame with one column \n    named 'Mean Value' and index labeled as 'Position i'.\n    \n    Parameters:\n    data_list (list of tuples): List containing tuples with numerical and non-numerical values.\n    \n    Returns:\n    pd.DataFrame: DataFrame containing mean values for each position.\n    \"\"\"\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n    \n    # Create DataFrame from list of tuples\n    df = pd.DataFrame(data_list)\n    \n    # Convert all columns to numeric, setting non-convertible values to NaN\n    numeric_df = df.apply(pd.to_numeric, errors='coerce')\n    \n    # Calculate mean for each column, ignoring NaN values\n    mean_series = numeric_df.mean(skipna=True)\n    \n    # Create index labels as 'Position i'\n    mean_series.index = [f'Position {i}' for i in range(len(mean_series))]\n    \n    # Convert the Series to a DataFrame with 'Mean Value' column\n    result_df = mean_series.to_frame(name='Mean Value')\n    \n    return result_df", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\n\ndef task_func(data_list=[('a', 1, 2.1),\n                          ('b', 2, 3.2),\n                          ('c', 3, 4.3),\n                          ('d', 4, 5.4),\n                          ('e', 5, 6.5)]):\n    \"\"\"\n    Calculate the mean of numerical values in each position across tuples in a list.\n    Non-numeric values are ignored. Returns a pandas DataFrame with one column \n    named 'Mean Value' and index labeled as 'Position i'.\n    \n    Parameters:\n    data_list (list of tuples): List containing tuples with numerical and non-numerical values.\n    \n    Returns:\n    pd.DataFrame: DataFrame containing mean values for each position.\n    \"\"\"\n    if not data_list:\n        return pd.DataFrame(columns=['Mean Value'])\n    \n    # Create DataFrame from list of tuples\n    df = pd.DataFrame(data_list)\n    \n    # Convert all columns to numeric, setting non-convertible values to NaN\n    numeric_df = df.apply(pd.to_numeric, errors='coerce')\n    \n    # Calculate mean for each column, ignoring NaN values\n    mean_series = numeric_df.mean(skipna=True)\n    \n    # Create index labels as 'Position i'\n    mean_series.index = [f'Position {i}' for i in range(len(mean_series))]\n    \n    # Convert the Series to a DataFrame with 'Mean Value' column\n    result_df = mean_series.to_frame(name='Mean Value')\n    \n    return result_df\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [('a', '1', 2.1),\n            ('b', 21, 'c'),\n            (12, 3, 4.3),\n            (['d'], 4, 5.4),\n            ('e', 5, 6.5)]\n    \n    df = task_func()\n    print(df)\n```\n\n**Output:**\n```\n             Mean Value\nPosition 0          NaN\nPosition 1          3.0\nPosition 2          4.3\n```"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"The data DataFrame is empty.\")\n    \n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both columns '{col1}', '{col2}' are not in the DataFrame.\")\n    \n    # Check if both columns contain categorical data\n    if not pd.api.types.is_categorical_dtype(data[col1]) and not pd.api.types.is_object_dtype(data[col1]):\n        raise TypeError(f\"Column '{col1}' does not contain categorical data.\")\n    if not pd.api.types.is_categorical_dtype(data[col2]) and not pd.api.types.is_object_dtype(data[col2]):\n        raise TypeError(f\"Column '{col2}' does not contain categorical data.\")\n    \n    # Construct contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check that both columns have multiple categories\n    if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n        raise ValueError(\"One or both columns do not have multiple categories.\")\n    \n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Check if all expected frequencies are at least 5\n    if (expected < 5).any():\n        raise ValueError(\"Some categories have less than 5 observations, violating the chi-square test assumptions.\")\n    \n    return p", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"The data DataFrame is empty.\")\n    \n    # Check if col1 and col2 are in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both columns '{col1}', '{col2}' are not in the DataFrame.\")\n    \n    # Check if both columns contain categorical data\n    if not pd.api.types.is_categorical_dtype(data[col1]) and not pd.api.types.is_object_dtype(data[col1]):\n        raise TypeError(f\"Column '{col1}' does not contain categorical data.\")\n    if not pd.api.types.is_categorical_dtype(data[col2]) and not pd.api.types.is_object_dtype(data[col2]):\n        raise TypeError(f\"Column '{col2}' does not contain categorical data.\")\n    \n    # Construct contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Check that both columns have multiple categories\n    if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n        raise ValueError(\"One or both columns do not have multiple categories.\")\n    \n    # Perform chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Check if all expected frequencies are at least 5\n    if (expected < 5).any():\n        raise ValueError(\"Some categories have less than 5 observations, violating the chi-square test assumptions.\")\n    \n    return p\n\n# Example usage:\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    data = pd.DataFrame({\n        'a': np.random.choice(['A', 'B'], size=100),\n        'b': np.random.choice(['X', 'Y'], size=100)\n    })\n    print(task_func(data, 'a', 'b'))  # Expected output: 1.0\n```"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate dice rolls, calculate frequencies, and plot a histogram.\n\n    Parameters:\n        rolls (int): Number of dice rolls to simulate.\n        seed (int, optional): Seed for random number generators.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.array: Frequency of each dice outcome.\n            - matplotlib.axes.Axes: Axes object of the histogram.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Simulate dice rolls\n    rolls_result = np.random.randint(1, 7, size=rolls)\n    \n    # Calculate frequencies for each dice value\n    frequencies = np.array([np.sum(rolls_result == number) for number in NUMBERS])\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies, color='skyblue', edgecolor='black')\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    plt.tight_layout()\n    \n    return frequencies, ax", "raw_solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate dice rolls, calculate frequencies, and plot a histogram.\n\n    Parameters:\n        rolls (int): Number of dice rolls to simulate.\n        seed (int, optional): Seed for random number generators.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.array: Frequency of each dice outcome.\n            - matplotlib.axes.Axes: Axes object of the histogram.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    # Simulate dice rolls\n    rolls_result = np.random.randint(1, 7, size=rolls)\n    \n    # Calculate frequencies for each dice value\n    frequencies = np.array([np.sum(rolls_result == number) for number in NUMBERS])\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequencies, color='skyblue', edgecolor='black')\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xticks(NUMBERS)\n    plt.tight_layout()\n    \n    return frequencies, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    num_rolls = 1000\n    freq, histogram_ax = task_func(num_rolls, seed=42)\n    print(\"Frequencies:\", freq)\n    plt.show()\n```"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from the source directory to the target directory.\n    A processed file is identified by the '_processed' suffix in its filename.\n\n    Args:\n        source_dir (str): The directory to search for processed files.\n        target_dir (str): The directory where the archive will be saved.\n        archive_name (str, optional): The name of the archive file. Defaults to 'archive.zip'.\n\n    Returns:\n        str: The path to the created archive.\n    \"\"\"\n    # Ensure the source directory exists\n    if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Define the path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Regular expression to match files with '_processed' before the file extension\n    processed_pattern = re.compile(r'^(.+)_processed(\\.[^.]+)$')\n\n    # List to store paths of processed files\n    processed_files = []\n\n    # Iterate through the files in the source directory\n    for filename in os.listdir(source_dir):\n        file_path = os.path.join(source_dir, filename)\n        if os.path.isfile(file_path):\n            match = processed_pattern.match(filename)\n            if match:\n                processed_files.append(file_path)\n\n    if not processed_files:\n        print(\"No processed files found to archive.\")\n        return archive_path\n\n    # Create a zip archive and add the processed files\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in processed_files:\n            # Add file to the zip archive with its basename\n            zipf.write(file, arcname=os.path.basename(file))\n            print(f\"Added '{file}' to archive.\")\n\n    print(f\"Archive created at '{archive_path}'.\")\n    return archive_path", "raw_solution": "```python\nimport zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from the source directory to the target directory.\n    A processed file is identified by the '_processed' suffix in its filename.\n\n    Args:\n        source_dir (str): The directory to search for processed files.\n        target_dir (str): The directory where the archive will be saved.\n        archive_name (str, optional): The name of the archive file. Defaults to 'archive.zip'.\n\n    Returns:\n        str: The path to the created archive.\n    \"\"\"\n    # Ensure the source directory exists\n    if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Define the path for the archive\n    archive_path = os.path.join(target_dir, archive_name)\n\n    # Regular expression to match files with '_processed' before the file extension\n    processed_pattern = re.compile(r'^(.+)_processed(\\.[^.]+)$')\n\n    # List to store paths of processed files\n    processed_files = []\n\n    # Iterate through the files in the source directory\n    for filename in os.listdir(source_dir):\n        file_path = os.path.join(source_dir, filename)\n        if os.path.isfile(file_path):\n            match = processed_pattern.match(filename)\n            if match:\n                processed_files.append(file_path)\n\n    if not processed_files:\n        print(\"No processed files found to archive.\")\n        return archive_path\n\n    # Create a zip archive and add the processed files\n    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in processed_files:\n            # Add file to the zip archive with its basename\n            zipf.write(file, arcname=os.path.basename(file))\n            print(f\"Added '{file}' to archive.\")\n\n    print(f\"Archive created at '{archive_path}'.\")\n    return archive_path\n```"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Ensure the DataFrame has a 'Date' column or a DateTime index\n    if 'Date' in df.columns:\n        df = df.copy()\n        df['Date'] = pd.to_datetime(df['Date'])\n        df.set_index('Date', inplace=True)\n    elif not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\"DataFrame must have a 'Date' column or a DateTime index.\")\n    \n    # Sort the DataFrame by date\n    df_sorted = df.sort_index()\n    df_sorted = df_sorted.reset_index()\n    \n    # Convert dates to UNIX timestamp in seconds\n    df_sorted['Timestamp'] = df_sorted['Date'].astype(np.int64) // 10**9\n    \n    # Prepare features and target\n    X = df_sorted['Timestamp'].values.reshape(-1, 1)\n    y = df_sorted['Close'].values\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future timestamps for the next 7 days\n    last_timestamp = df_sorted['Timestamp'].iloc[-1]\n    future_timestamps = (last_timestamp + np.arange(1, 8) * 24 * 60 * 60).reshape(-1, 1)\n    \n    # Predict the closing prices\n    predicted_prices = model.predict(future_timestamps).tolist()\n    \n    # Convert future timestamps back to datetime\n    future_dates = pd.to_datetime(future_timestamps.flatten(), unit='s')\n    \n    # Plot historical data and predictions\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df_sorted['Date'], y, label='Historical Close Prices', marker='o')\n    ax.plot(future_dates, predicted_prices, marker='x', linestyle='--', label='Predicted Close Prices')\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Closing Prices Prediction for Next 7 Days')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return (predicted_prices, ax)", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Ensure the DataFrame has a 'Date' column or a DateTime index\n    if 'Date' in df.columns:\n        df = df.copy()\n        df['Date'] = pd.to_datetime(df['Date'])\n        df.set_index('Date', inplace=True)\n    elif not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\"DataFrame must have a 'Date' column or a DateTime index.\")\n    \n    # Sort the DataFrame by date\n    df_sorted = df.sort_index()\n    df_sorted = df_sorted.reset_index()\n    \n    # Convert dates to UNIX timestamp in seconds\n    df_sorted['Timestamp'] = df_sorted['Date'].astype(np.int64) // 10**9\n    \n    # Prepare features and target\n    X = df_sorted['Timestamp'].values.reshape(-1, 1)\n    y = df_sorted['Close'].values\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future timestamps for the next 7 days\n    last_timestamp = df_sorted['Timestamp'].iloc[-1]\n    future_timestamps = (last_timestamp + np.arange(1, 8) * 24 * 60 * 60).reshape(-1, 1)\n    \n    # Predict the closing prices\n    predicted_prices = model.predict(future_timestamps).tolist()\n    \n    # Convert future timestamps back to datetime\n    future_dates = pd.to_datetime(future_timestamps.flatten(), unit='s')\n    \n    # Plot historical data and predictions\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df_sorted['Date'], y, label='Historical Close Prices', marker='o')\n    ax.plot(future_dates, predicted_prices, marker='x', linestyle='--', label='Predicted Close Prices')\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Stock Closing Prices Prediction for Next 7 Days')\n    ax.legend()\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    return (predicted_prices, ax)\n```"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies and plots outliers in the 'closing_price' column of a DataFrame using the Z-Score method.\n\n    Parameters:\n    - df (pandas.DataFrame): The input DataFrame containing a 'closing_price' column.\n    - z_threshold (float): The Z-Score threshold to identify outliers.\n\n    Returns:\n    - tuple:\n        - pandas.DataFrame: DataFrame containing the outliers.\n        - matplotlib.axes._axes.Axes: The plot object displaying the outliers.\n    \"\"\"\n    # Calculate Z-Scores for the 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers_df = df[np.abs(df['z_score']) > z_threshold].copy()\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df.index, df['closing_price'], marker='o', linestyle='-', label='Closing Price')\n    \n    # Highlight outliers\n    ax.scatter(outliers_df.index, outliers_df['closing_price'], color='red', label='Outliers', zorder=5)\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    \n    # Optional: Improve layout\n    plt.tight_layout()\n    \n    # Drop the temporary 'z_score' column\n    df.drop('z_score', axis=1, inplace=True)\n    \n    return outliers_df, ax", "raw_solution": "```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies and plots outliers in the 'closing_price' column of a DataFrame using the Z-Score method.\n\n    Parameters:\n    - df (pandas.DataFrame): The input DataFrame containing a 'closing_price' column.\n    - z_threshold (float): The Z-Score threshold to identify outliers.\n\n    Returns:\n    - tuple:\n        - pandas.DataFrame: DataFrame containing the outliers.\n        - matplotlib.axes._axes.Axes: The plot object displaying the outliers.\n    \"\"\"\n    # Calculate Z-Scores for the 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers_df = df[np.abs(df['z_score']) > z_threshold].copy()\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(df.index, df['closing_price'], marker='o', linestyle='-', label='Closing Price')\n    \n    # Highlight outliers\n    ax.scatter(outliers_df.index, outliers_df['closing_price'], color='red', label='Outliers', zorder=5)\n    \n    # Set labels and title\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    \n    # Optional: Improve layout\n    plt.tight_layout()\n    \n    # Drop the temporary 'z_score' column\n    df.drop('z_score', axis=1, inplace=True)\n    \n    return outliers_df, ax\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample DataFrame\n    df2 = pd.DataFrame({\n        'closing_price': [10, 20, 30, 40, 50, 100]\n    })\n    \n    # Identify outliers with a Z-Score threshold of 1.5\n    outliers2, plot2 = task_func(df2, z_threshold=1.5)\n    \n    # Display the outliers\n    print(\"Outliers based on Z-Score threshold of 1.5:\")\n    print(outliers2)\n    \n    # Show the plot\n    plt.show()\n```"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using a box plot and a histogram.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing stock data with a 'Close' column.\n\n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects:\n           (boxplot_ax, histogram_ax)\n    \"\"\"\n    # Set the style for seaborn\n    sns.set(style=\"whitegrid\")\n\n    # Create a figure with two subplots side by side\n    fig, (box_ax, hist_ax) = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Box Plot\n    sns.boxplot(y=df['Close'], ax=box_ax, color='skyblue')\n    box_ax.set_title('Box Plot of Closing Prices')\n    box_ax.set_ylabel('Closing Price')\n\n    # Histogram\n    sns.histplot(df['Close'], bins=30, kde=True, ax=hist_ax, color='salmon')\n    hist_ax.set_title('Histogram of Closing Prices')\n    hist_ax.set_xlabel('Closing Price')\n    hist_ax.set_ylabel('Frequency')\n\n    plt.tight_layout()\n    \n    return (box_ax, hist_ax)", "raw_solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualizes the distribution of stock closing prices using a box plot and a histogram.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing stock data with a 'Close' column.\n\n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects:\n           (boxplot_ax, histogram_ax)\n    \"\"\"\n    # Set the style for seaborn\n    sns.set(style=\"whitegrid\")\n\n    # Create a figure with two subplots side by side\n    fig, (box_ax, hist_ax) = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Box Plot\n    sns.boxplot(y=df['Close'], ax=box_ax, color='skyblue')\n    box_ax.set_title('Box Plot of Closing Prices')\n    box_ax.set_ylabel('Closing Price')\n\n    # Histogram\n    sns.histplot(df['Close'], bins=30, kde=True, ax=hist_ax, color='salmon')\n    hist_ax.set_title('Histogram of Closing Prices')\n    hist_ax.set_xlabel('Closing Price')\n    hist_ax.set_ylabel('Frequency')\n\n    plt.tight_layout()\n    \n    return (box_ax, hist_ax)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    data = {\n        'Date': pd.date_range(start='2023-01-01', periods=100),\n        'Close': pd.np.random.normal(loc=100, scale=10, size=100)  # Using normal distribution for example\n    }\n    df = pd.DataFrame(data)\n\n    # Call the function\n    box_ax, hist_ax = task_func(df)\n\n    # Display the plots\n    plt.show()\n```"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing historical share closing prices with a datetime index or a 'Date' column.\n\n    Returns:\n    Tuple[List[float], Axes]: A tuple containing:\n        - A list with forecasted prices for the next 7 days.\n        - A matplotlib Axes object containing the plot.\n    \"\"\"\n    # Ensure 'Date' column is datetime if present\n    if 'Date' in df.columns:\n        df['Date'] = pd.to_datetime(df['Date'])\n        df.set_index('Date', inplace=True)\n    \n    # Assume the closing prices are in a column named 'Close'\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column with closing prices.\")\n    \n    # Sort the DataFrame by date\n    df = df.sort_index()\n    \n    # Fit ARIMA model (order can be tuned)\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast_result = model_fit.get_forecast(steps=7)\n    forecast = forecast_result.predicted_mean\n    forecast_conf_int = forecast_result.conf_int()\n    \n    # Prepare forecast dates\n    last_date = df.index[-1]\n    if isinstance(last_date, pd.Timestamp):\n        forecast_dates = [last_date + pd.Timedelta(days=i) for i in range(1, 8)]\n    else:\n        # If the index is not datetime, use integer-based range\n        forecast_dates = range(len(df), len(df) + 7)\n    \n    # Plot the historical data and the forecast\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(df.index, df['Close'], label='Historical')\n    ax.plot(forecast_dates, forecast, label='Forecast', color='red')\n    ax.fill_between(forecast_dates, \n                    forecast_conf_int['lower Close'], \n                    forecast_conf_int['upper Close'], \n                    color='pink', alpha=0.3, label='Confidence Interval')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Share Closing Price Forecast')\n    ax.legend()\n    plt.tight_layout()\n    \n    # Convert forecast to list\n    forecast_list = forecast.tolist()\n    \n    return forecast_list, ax", "raw_solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts the share closing prices for the next 7 days using the ARIMA model and plots the forecast.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing historical share closing prices with a datetime index or a 'Date' column.\n\n    Returns:\n    Tuple[List[float], Axes]: A tuple containing:\n        - A list with forecasted prices for the next 7 days.\n        - A matplotlib Axes object containing the plot.\n    \"\"\"\n    # Ensure 'Date' column is datetime if present\n    if 'Date' in df.columns:\n        df['Date'] = pd.to_datetime(df['Date'])\n        df.set_index('Date', inplace=True)\n    \n    # Assume the closing prices are in a column named 'Close'\n    if 'Close' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'Close' column with closing prices.\")\n    \n    # Sort the DataFrame by date\n    df = df.sort_index()\n    \n    # Fit ARIMA model (order can be tuned)\n    model = ARIMA(df['Close'], order=(5,1,0))\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast_result = model_fit.get_forecast(steps=7)\n    forecast = forecast_result.predicted_mean\n    forecast_conf_int = forecast_result.conf_int()\n    \n    # Prepare forecast dates\n    last_date = df.index[-1]\n    if isinstance(last_date, pd.Timestamp):\n        forecast_dates = [last_date + pd.Timedelta(days=i) for i in range(1, 8)]\n    else:\n        # If the index is not datetime, use integer-based range\n        forecast_dates = range(len(df), len(df) + 7)\n    \n    # Plot the historical data and the forecast\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(df.index, df['Close'], label='Historical')\n    ax.plot(forecast_dates, forecast, label='Forecast', color='red')\n    ax.fill_between(forecast_dates, \n                    forecast_conf_int['lower Close'], \n                    forecast_conf_int['upper Close'], \n                    color='pink', alpha=0.3, label='Confidence Interval')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Share Closing Price Forecast')\n    ax.legend()\n    plt.tight_layout()\n    \n    # Convert forecast to list\n    forecast_list = forecast.tolist()\n    \n    return forecast_list, ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data creation\n    dates = pd.date_range(start='2023-01-01', periods=100, freq='D')\n    import numpy as np\n    np.random.seed(0)\n    prices = np.random.lognormal(mean=0, sigma=0.1, size=len(dates)).cumsum()\n    df_sample = pd.DataFrame({'Date': dates, 'Close': prices})\n    \n    # Call the task_func\n    forecasted_prices, plot_ax = task_func(df_sample)\n    \n    # Show the plot\n    plt.show()\n    \n    # Print forecasted prices\n    print(\"Forecasted Prices for the next 7 days:\")\n    for i, price in enumerate(forecasted_prices, 1):\n        print(f\"Day {i}: {price:.2f}\")\n```"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Creates a dictionary of all possible two-letter combinations of lowercase English alphabets.\n    The dictionary values represent the frequency of these combinations in the given word.\n    \n    Parameters:\n        word (str): The input word to analyze.\n    \n    Returns:\n        dict: A dictionary with keys as two-letter combinations and values as their counts in the word.\n    \"\"\"\n    # Convert the word to lowercase to ensure consistency\n    word = word.lower()\n    \n    # Generate all possible two-letter combinations\n    all_combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Extract all two-letter sequences from the word\n    two_letter_seqs = [word[i:i+2] for i in range(len(word) - 1)]\n    \n    # Count the frequency of each two-letter sequence in the word\n    counts = Counter(two_letter_seqs)\n    \n    # Create the final dictionary with all possible combinations and their counts\n    result = {comb: counts.get(comb, 0) for comb in all_combinations}\n    \n    return result", "raw_solution": "```python\nfrom collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Creates a dictionary of all possible two-letter combinations of lowercase English alphabets.\n    The dictionary values represent the frequency of these combinations in the given word.\n    \n    Parameters:\n        word (str): The input word to analyze.\n    \n    Returns:\n        dict: A dictionary with keys as two-letter combinations and values as their counts in the word.\n    \"\"\"\n    # Convert the word to lowercase to ensure consistency\n    word = word.lower()\n    \n    # Generate all possible two-letter combinations\n    all_combinations = [''.join(pair) for pair in itertools.product(string.ascii_lowercase, repeat=2)]\n    \n    # Extract all two-letter sequences from the word\n    two_letter_seqs = [word[i:i+2] for i in range(len(word) - 1)]\n    \n    # Count the frequency of each two-letter sequence in the word\n    counts = Counter(two_letter_seqs)\n    \n    # Create the final dictionary with all possible combinations and their counts\n    result = {comb: counts.get(comb, 0) for comb in all_combinations}\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    sample_word = \"hello\"\n    combination_counts = task_func(sample_word)\n    # To display only combinations that appear in the word\n    present_combinations = {k: v for k, v in combination_counts.items() if v > 0}\n    print(f\"Two-letter combinations in '{sample_word}':\")\n    for combo, count in present_combinations.items():\n        print(f\"'{combo}': {count}\")\n```"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Creates and visualizes a sales report for different categories over a period of time.\n\n    Parameters:\n        start_date (str): The start date for the sales data.\n        periods (int): Number of periods to generate.\n        freq (str): Frequency of dates.\n        categories (list): List of sales categories.\n\n    Returns:\n        df (pd.DataFrame): DataFrame containing the sales data with 'Date', 'Category', 'Sales'.\n        ax (matplotlib.axes.Axes): The Matplotlib Axes object for the plot.\n    \"\"\"\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Generate sales data\n    data = []\n    np.random.seed(0)  # For reproducibility\n    for category in categories:\n        sales = np.random.randint(1000, 5000, size=periods)\n        for date, sale in zip(dates, sales):\n            data.append({'Date': date, 'Category': category, 'Sales': sale})\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], marker='o', label=category)\n\n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n    plt.tight_layout()\n\n    return df, ax", "raw_solution": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Creates and visualizes a sales report for different categories over a period of time.\n\n    Parameters:\n        start_date (str): The start date for the sales data.\n        periods (int): Number of periods to generate.\n        freq (str): Frequency of dates.\n        categories (list): List of sales categories.\n\n    Returns:\n        df (pd.DataFrame): DataFrame containing the sales data with 'Date', 'Category', 'Sales'.\n        ax (matplotlib.axes.Axes): The Matplotlib Axes object for the plot.\n    \"\"\"\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Generate sales data\n    data = []\n    np.random.seed(0)  # For reproducibility\n    for category in categories:\n        sales = np.random.randint(1000, 5000, size=periods)\n        for date, sale in zip(dates, sales):\n            data.append({'Date': date, 'Category': category, 'Sales': sale})\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        category_data = df[df['Category'] == category]\n        ax.plot(category_data['Date'], category_data['Sales'], marker='o', label=category)\n\n    ax.set_title('Sales Report by Category')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.legend()\n    plt.tight_layout()\n\n    return df, ax\n\nif __name__ == \"__main__\":\n    # Generate the sales report\n    sales_df, sales_ax = task_func()\n\n    # Display the DataFrame\n    print(sales_df)\n\n    # Show the plot\n    plt.show()\n```"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    Generates a time series of sales data starting from a specified date, then uses linear regression\n    to forecast future sales based on the provided or generated sales data.\n\n    Parameters:\n        start_date (str): The start date for the time series (default '2016-01-01').\n        periods (int): Number of periods to generate or forecast (default 13).\n        freq (str): Frequency of the time series (default 'WOM-2FRI' - second Friday of each month).\n        sales_data (array-like, optional): Existing sales data to use. If None, synthetic data is generated.\n\n    Returns:\n        np.ndarray: Forecasted future sales for the same number of periods as the input data.\n    \"\"\"\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate synthetic sales data if not provided\n    if sales_data is None:\n        np.random.seed(0)  # For reproducibility\n        trend = np.linspace(100, 200, periods)  # Example trend from 100 to 200\n        seasonal = 10 * np.sin(np.linspace(0, 3 * np.pi, periods))  # Example seasonal component\n        noise = np.random.normal(0, 5, periods)  # Random noise\n        sales = trend + seasonal + noise\n    else:\n        sales = np.array(sales_data)\n        if len(sales) != periods:\n            raise ValueError(\"Length of sales_data must match the number of periods.\")\n    \n    # Prepare the feature variable (time as integer)\n    X = np.arange(periods).reshape(-1, 1)\n    \n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, sales)\n    \n    # Prepare future time steps\n    future_X = np.arange(periods, 2 * periods).reshape(-1, 1)\n    \n    # Predict future sales\n    forecast = model.predict(future_X)\n    \n    return forecast", "raw_solution": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    Generates a time series of sales data starting from a specified date, then uses linear regression\n    to forecast future sales based on the provided or generated sales data.\n\n    Parameters:\n        start_date (str): The start date for the time series (default '2016-01-01').\n        periods (int): Number of periods to generate or forecast (default 13).\n        freq (str): Frequency of the time series (default 'WOM-2FRI' - second Friday of each month).\n        sales_data (array-like, optional): Existing sales data to use. If None, synthetic data is generated.\n\n    Returns:\n        np.ndarray: Forecasted future sales for the same number of periods as the input data.\n    \"\"\"\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate synthetic sales data if not provided\n    if sales_data is None:\n        np.random.seed(0)  # For reproducibility\n        trend = np.linspace(100, 200, periods)  # Example trend from 100 to 200\n        seasonal = 10 * np.sin(np.linspace(0, 3 * np.pi, periods))  # Example seasonal component\n        noise = np.random.normal(0, 5, periods)  # Random noise\n        sales = trend + seasonal + noise\n    else:\n        sales = np.array(sales_data)\n        if len(sales) != periods:\n            raise ValueError(\"Length of sales_data must match the number of periods.\")\n    \n    # Prepare the feature variable (time as integer)\n    X = np.arange(periods).reshape(-1, 1)\n    \n    # Fit linear regression model\n    model = LinearRegression()\n    model.fit(X, sales)\n    \n    # Prepare future time steps\n    future_X = np.arange(periods, 2 * periods).reshape(-1, 1)\n    \n    # Predict future sales\n    forecast = model.predict(future_X)\n    \n    return forecast\n\n# Example usage\nif __name__ == \"__main__\":\n    forecasted_sales = task_func()\n    print(\"Forecasted Sales:\")\n    print(forecasted_sales)\n```"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees with a due date of the current day.\n\n    Parameters:\n        task_list (list of str): List of task names.\n        n_tasks (int): Number of tasks to assign.\n        employees (list of str, optional): List of employee names. Defaults to a predefined list.\n        seed (int, optional): Seed for random number generator. Defaults to None.\n\n    Returns:\n        pd.DataFrame: DataFrame containing 'Task Name', 'Assigned To', and 'Due Date'.\n\n    Raises:\n        ValueError: If n_tasks is negative.\n    \"\"\"\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Get current date\n    due_date = datetime.now().date()\n\n    # Assign tasks randomly to employees\n    assigned_tasks = []\n    for _ in range(n_tasks):\n        task = random.choice(sanitized_tasks)\n        employee = random.choice(employees)\n        assigned_tasks.append({\n            \"Task Name\": task,\n            \"Assigned To\": employee,\n            \"Due Date\": due_date\n        })\n\n    # Create DataFrame\n    df = pd.DataFrame(assigned_tasks, columns=[\"Task Name\", \"Assigned To\", \"Due Date\"])\n    return df", "raw_solution": "```python\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns a specified number of tasks to employees with a due date of the current day.\n\n    Parameters:\n        task_list (list of str): List of task names.\n        n_tasks (int): Number of tasks to assign.\n        employees (list of str, optional): List of employee names. Defaults to a predefined list.\n        seed (int, optional): Seed for random number generator. Defaults to None.\n\n    Returns:\n        pd.DataFrame: DataFrame containing 'Task Name', 'Assigned To', and 'Due Date'.\n\n    Raises:\n        ValueError: If n_tasks is negative.\n    \"\"\"\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks must be non-negative\")\n\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Get current date\n    due_date = datetime.now().date()\n\n    # Assign tasks randomly to employees\n    assigned_tasks = []\n    for _ in range(n_tasks):\n        task = random.choice(sanitized_tasks)\n        employee = random.choice(employees)\n        assigned_tasks.append({\n            \"Task Name\": task,\n            \"Assigned To\": employee,\n            \"Due Date\": due_date\n        })\n\n    # Create DataFrame\n    df = pd.DataFrame(assigned_tasks, columns=[\"Task Name\", \"Assigned To\", \"Due Date\"])\n    return df\n```"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty.\")\n    \n    # Replace spaces in mystrings with underscores and convert to lowercase\n    modified_words = [word.replace(' ', '_').lower() for word in mystrings]\n    \n    # Create a mapping from original words (lowercase) to modified words\n    mapping = {original.lower(): modified for original, modified in zip(mystrings, modified_words)}\n    \n    # Function to replace words in text based on the mapping\n    def replace_words(match):\n        word = match.group(0).lower()\n        return mapping.get(word, word)\n    \n    # Replace specified words in the text\n    pattern = re.compile(r'\\b(' + '|'.join(re.escape(word) for word in mapping.keys()) + r')\\b', re.IGNORECASE)\n    modified_text = pattern.sub(replace_words, text.lower())\n    \n    # Split the text into words using regex to handle punctuation\n    words = re.findall(r'\\b\\w+\\b', modified_text)\n    \n    # Count the frequency of each unique word\n    freq_counter = Counter(words)\n    \n    # Maintain the order of first appearance\n    unique_words_order = []\n    for word in words:\n        if word not in unique_words_order:\n            unique_words_order.append(word)\n    \n    # Get frequencies in the order of unique_words_order\n    frequencies = [freq_counter[word] for word in unique_words_order]\n    \n    # Plot the frequency\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(unique_words_order, frequencies, color='skyblue')\n    ax.set_xlabel('Unique Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()\n    \n    return ax", "raw_solution": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty.\")\n    \n    # Replace spaces in mystrings with underscores and convert to lowercase\n    modified_words = [word.replace(' ', '_').lower() for word in mystrings]\n    \n    # Create a mapping from original words (lowercase) to modified words\n    mapping = {original.lower(): modified for original, modified in zip(mystrings, modified_words)}\n    \n    # Function to replace words in text based on the mapping\n    def replace_words(match):\n        word = match.group(0).lower()\n        return mapping.get(word, word)\n    \n    # Replace specified words in the text\n    pattern = re.compile(r'\\b(' + '|'.join(re.escape(word) for word in mapping.keys()) + r')\\b', re.IGNORECASE)\n    modified_text = pattern.sub(replace_words, text.lower())\n    \n    # Split the text into words using regex to handle punctuation\n    words = re.findall(r'\\b\\w+\\b', modified_text)\n    \n    # Count the frequency of each unique word\n    freq_counter = Counter(words)\n    \n    # Maintain the order of first appearance\n    unique_words_order = []\n    for word in words:\n        if word not in unique_words_order:\n            unique_words_order.append(word)\n    \n    # Get frequencies in the order of unique_words_order\n    frequencies = [freq_counter[word] for word in unique_words_order]\n    \n    # Plot the frequency\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(unique_words_order, frequencies, color='skyblue')\n    ax.set_xlabel('Unique Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Word Frequency')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()\n    \n    return ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    mystrings = [\"Hello World\", \"Python\", \"Data Science\"]\n    text = \"Hello world! Python is great. Data science is fun. Hello Python.\"\n    task_func(mystrings, text)\n```"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zips files with specific extensions from the source directory and saves the zip file to the target directory.\n\n    Parameters:\n        source_directory (str): Path to the source directory.\n        target_directory (str): Path to the target directory where the zip file will be saved.\n        zip_name (str): Name of the resulting zip file (without .zip extension).\n\n    Returns:\n        str: The full path to the created zip file.\n\n    Raises:\n        OSError: If the source_directory does not exist.\n    \"\"\"\n    # Define valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Convert to Path objects for easier manipulation\n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n    \n    # Check if source directory exists\n    if not source_path.is_dir():\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Ensure target directory exists; create it if it doesn't\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # Create the full path for the zip file\n    zip_file_path = target_path / f\"{zip_name}.zip\"\n    \n    # Initialize the zip file\n    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Iterate over all files in the source directory\n        for ext in valid_extensions:\n            # Use glob to find files with the current extension\n            for file_path in source_path.glob(f'*{ext}'):\n                # Add the file to the zip archive with its name only\n                zipf.write(file_path, arcname=file_path.name)\n    \n    return str(zip_file_path)", "raw_solution": "```python\nimport os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zips files with specific extensions from the source directory and saves the zip file to the target directory.\n\n    Parameters:\n        source_directory (str): Path to the source directory.\n        target_directory (str): Path to the target directory where the zip file will be saved.\n        zip_name (str): Name of the resulting zip file (without .zip extension).\n\n    Returns:\n        str: The full path to the created zip file.\n\n    Raises:\n        OSError: If the source_directory does not exist.\n    \"\"\"\n    # Define valid file extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Convert to Path objects for easier manipulation\n    source_path = Path(source_directory)\n    target_path = Path(target_directory)\n    \n    # Check if source directory exists\n    if not source_path.is_dir():\n        raise OSError(f\"The source directory '{source_directory}' does not exist.\")\n    \n    # Ensure target directory exists; create it if it doesn't\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # Create the full path for the zip file\n    zip_file_path = target_path / f\"{zip_name}.zip\"\n    \n    # Initialize the zip file\n    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Iterate over all files in the source directory\n        for ext in valid_extensions:\n            # Use glob to find files with the current extension\n            for file_path in source_path.glob(f'*{ext}'):\n                # Add the file to the zip archive with its name only\n                zipf.write(file_path, arcname=file_path.name)\n    \n    return str(zip_file_path)\n\n# Example usage:\nif __name__ == \"__main__\":\n    source_dir = \"/path/to/source_directory\"\n    target_dir = \"/path/to/target_directory\"\n    zip_filename = \"archive\"\n\n    try:\n        created_zip = task_func(source_dir, target_dir, zip_filename)\n        print(f\"Zip file created at: {created_zip}\")\n    except OSError as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files\n    and saves them in a target directory. Each CSV contains a single 'Text' column with the file's text content.\n    Existing files in the target directory with the same names will be overwritten.\n\n    Args:\n        source_directory (str): Path to the source directory containing the files to convert.\n        target_directory (str): Path to the target directory where CSV files will be saved.\n\n    Returns:\n        int: The number of files successfully converted to CSV.\n\n    Raises:\n        FileNotFoundError: If the source directory does not exist.\n    \"\"\"\n    # Define allowed file extensions\n    allowed_extensions = {'.txt', '.docx', '.xlsx', '.csv'}\n\n    # Convert to Path objects\n    src_path = Path(source_directory)\n    tgt_path = Path(target_directory)\n\n    # Check if source directory exists\n    if not src_path.exists() or not src_path.is_dir():\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Create target directory if it doesn't exist\n    tgt_path.mkdir(parents=True, exist_ok=True)\n\n    # Initialize counter for successfully converted files\n    converted_count = 0\n\n    # Iterate over each file in the source directory\n    for file in src_path.iterdir():\n        if file.is_file() and file.suffix.lower() in allowed_extensions:\n            try:\n                text_content = \"\"\n\n                if file.suffix.lower() == '.txt':\n                    # Read text file\n                    with file.open('r', encoding='utf-8') as f:\n                        text_content = f.read()\n\n                elif file.suffix.lower() == '.docx':\n                    # Read Word document\n                    doc = docx.Document(file)\n                    paragraphs = [para.text for para in doc.paragraphs]\n                    text_content = '\\n'.join(paragraphs)\n\n                elif file.suffix.lower() == '.xlsx':\n                    # Read Excel file\n                    excel_data = pd.read_excel(file, sheet_name=None, engine='openpyxl')\n                    sheet_texts = []\n                    for sheet_name, df in excel_data.items():\n                        sheet_texts.append(f\"Sheet: {sheet_name}\")\n                        sheet_texts.append(df.to_csv(index=False))\n                    text_content = '\\n'.join(sheet_texts)\n\n                elif file.suffix.lower() == '.csv':\n                    # Read CSV file as text\n                    with file.open('r', encoding='utf-8') as f:\n                        text_content = f.read()\n\n                # Create DataFrame with a single 'Text' column\n                df = pd.DataFrame({'Text': [text_content]})\n\n                # Define target CSV file path\n                target_file = tgt_path / (file.stem + '.csv')\n\n                # Save DataFrame to CSV without index\n                df.to_csv(target_file, index=False, encoding='utf-8')\n\n                # Increment the counter\n                converted_count += 1\n\n            except Exception as e:\n                # Optionally, you can log the error or handle it as needed\n                # For now, we'll skip files that cause exceptions\n                pass\n\n    return converted_count", "raw_solution": "```python\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from a source directory to CSV files\n    and saves them in a target directory. Each CSV contains a single 'Text' column with the file's text content.\n    Existing files in the target directory with the same names will be overwritten.\n\n    Args:\n        source_directory (str): Path to the source directory containing the files to convert.\n        target_directory (str): Path to the target directory where CSV files will be saved.\n\n    Returns:\n        int: The number of files successfully converted to CSV.\n\n    Raises:\n        FileNotFoundError: If the source directory does not exist.\n    \"\"\"\n    # Define allowed file extensions\n    allowed_extensions = {'.txt', '.docx', '.xlsx', '.csv'}\n\n    # Convert to Path objects\n    src_path = Path(source_directory)\n    tgt_path = Path(target_directory)\n\n    # Check if source directory exists\n    if not src_path.exists() or not src_path.is_dir():\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Create target directory if it doesn't exist\n    tgt_path.mkdir(parents=True, exist_ok=True)\n\n    # Initialize counter for successfully converted files\n    converted_count = 0\n\n    # Iterate over each file in the source directory\n    for file in src_path.iterdir():\n        if file.is_file() and file.suffix.lower() in allowed_extensions:\n            try:\n                text_content = \"\"\n\n                if file.suffix.lower() == '.txt':\n                    # Read text file\n                    with file.open('r', encoding='utf-8') as f:\n                        text_content = f.read()\n\n                elif file.suffix.lower() == '.docx':\n                    # Read Word document\n                    doc = docx.Document(file)\n                    paragraphs = [para.text for para in doc.paragraphs]\n                    text_content = '\\n'.join(paragraphs)\n\n                elif file.suffix.lower() == '.xlsx':\n                    # Read Excel file\n                    excel_data = pd.read_excel(file, sheet_name=None, engine='openpyxl')\n                    sheet_texts = []\n                    for sheet_name, df in excel_data.items():\n                        sheet_texts.append(f\"Sheet: {sheet_name}\")\n                        sheet_texts.append(df.to_csv(index=False))\n                    text_content = '\\n'.join(sheet_texts)\n\n                elif file.suffix.lower() == '.csv':\n                    # Read CSV file as text\n                    with file.open('r', encoding='utf-8') as f:\n                        text_content = f.read()\n\n                # Create DataFrame with a single 'Text' column\n                df = pd.DataFrame({'Text': [text_content]})\n\n                # Define target CSV file path\n                target_file = tgt_path / (file.stem + '.csv')\n\n                # Save DataFrame to CSV without index\n                df.to_csv(target_file, index=False, encoding='utf-8')\n\n                # Increment the counter\n                converted_count += 1\n\n            except Exception as e:\n                # Optionally, you can log the error or handle it as needed\n                # For now, we'll skip files that cause exceptions\n                pass\n\n    return converted_count\n\n# Example usage:\n# if __name__ == \"__main__\":\n#     source_dir = \"path/to/source\"\n#     target_dir = \"path/to/target\"\n#     try:\n#         num_converted = task_func(source_dir, target_dir)\n#         print(f\"Successfully converted {num_converted} files.\")\n#     except FileNotFoundError as e:\n#         print(e)\n```"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    # Check for NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n\n    # Check if all columns are numeric\n    if not all(pd.api.types.is_numeric_dtype(dtype) for dtype in df.dtypes):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    # Compute cumulative sum for each column\n    cumsum_df = df.cumsum()\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply MinMax scaling to the cumulative sums\n    normalized_cumsum = scaler.fit_transform(cumsum_df)\n\n    # Create a new DataFrame with the normalized cumulative sums\n    normalized_cumsum_df = pd.DataFrame(normalized_cumsum, \n                                        columns=df.columns, \n                                        index=df.index)\n\n    return normalized_cumsum_df", "raw_solution": "```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n\n    # Check for NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n\n    # Check if all columns are numeric\n    if not all(pd.api.types.is_numeric_dtype(dtype) for dtype in df.dtypes):\n        raise TypeError(\"The DataFrame contains non-numeric data types.\")\n\n    # Compute cumulative sum for each column\n    cumsum_df = df.cumsum()\n\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Apply MinMax scaling to the cumulative sums\n    normalized_cumsum = scaler.fit_transform(cumsum_df)\n\n    # Create a new DataFrame with the normalized cumulative sums\n    normalized_cumsum_df = pd.DataFrame(normalized_cumsum, \n                                        columns=df.columns, \n                                        index=df.index)\n\n    return normalized_cumsum_df\n```"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes the given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n\n    Args:\n        directory_path (str): The path to the directory to analyze.\n\n    Returns:\n        list of tuples: Each tuple contains (file name, file size in bytes,\n        creation time in ISO format, modification time in ISO format).\n\n    Raises:\n        ValueError: If the provided directory does not exist.\n    \"\"\"\n    p = Path(directory_path)\n    if not p.is_dir():\n        raise ValueError(\"Directory does not exist.\")\n\n    result = []\n    for entry in p.iterdir():\n        if entry.is_file():\n            stat = entry.stat()\n            file_name = entry.name\n            file_size = stat.st_size\n            creation_time = datetime.fromtimestamp(stat.st_ctime, tz=timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time, modification_time))\n    \n    return result", "raw_solution": "```python\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes the given directory, listing each file it contains along with its size,\n    creation time, and last modification time without recursing into subdirectories.\n\n    Args:\n        directory_path (str): The path to the directory to analyze.\n\n    Returns:\n        list of tuples: Each tuple contains (file name, file size in bytes,\n        creation time in ISO format, modification time in ISO format).\n\n    Raises:\n        ValueError: If the provided directory does not exist.\n    \"\"\"\n    p = Path(directory_path)\n    if not p.is_dir():\n        raise ValueError(\"Directory does not exist.\")\n\n    result = []\n    for entry in p.iterdir():\n        if entry.is_file():\n            stat = entry.stat()\n            file_name = entry.name\n            file_size = stat.st_size\n            creation_time = datetime.fromtimestamp(stat.st_ctime, tz=timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time, modification_time))\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        # Replace '/path/to/directory' with the actual directory path you want to analyze\n        result = task_func('/path/to/empty_directory')\n        print(result)\n    except ValueError as ve:\n        print(f\"Error: {ve}\")\n```\n\n**Explanation:**\n\n1. **Imports:**\n    - `os`: Provides a way of using operating system dependent functionality.\n    - `Path` from `pathlib`: Offers an object-oriented approach to handling filesystem paths.\n    - `datetime` and `timezone` from `datetime`: Used for handling and formatting timestamps.\n\n2. **Function `task_func`:**\n    - **Parameter:** `directory_path` (str) \u2013 The path to the directory to analyze.\n    - **Validation:** Checks if the provided path is a directory. If not, raises a `ValueError`.\n    - **Processing:**\n        - Iterates over each entry in the directory using `iterdir()`.\n        - Checks if the entry is a file (ignores subdirectories).\n        - Retrieves the file's name, size, creation time, and modification time.\n        - Converts the creation and modification times to UTC and formats them in ISO format.\n        - Appends the information as a tuple to the `result` list.\n    - **Return:** A list of tuples containing the file information.\n\n3. **Example Usage:**\n    - The script includes a sample usage within the `__main__` block.\n    - It attempts to analyze the directory `'/path/to/empty_directory'` and prints the result.\n    - If the directory does not exist, it catches the `ValueError` and prints an error message.\n\n**Note:**\n- Ensure that the directory path provided to `task_func` exists and is accessible.\n- The creation time (`st_ctime`) may represent different meanings on different operating systems. On some systems, it might represent the last metadata change time instead of the creation time.\n\n**Sample Output:**\nIf the directory is empty:\n```\n[]\n```\n\nIf the directory contains files, an example output might look like:\n```\n[\n    ('example.txt', 1024, '2023-10-05T12:34:56+00:00', '2023-10-06T08:21:30+00:00'),\n    ('image.png', 204800, '2023-09-15T09:10:11+00:00', '2023-09-20T14:22:33+00:00')\n]\n```"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n\n    Parameters:\n        json_data (str): JSON string containing country population data.\n        output_dir (str): Directory where the CSV report will be saved. Defaults to current directory.\n        file_name (str): Name of the output CSV file. Defaults to 'country_population_report.csv'.\n\n    Returns:\n        tuple:\n            - str: The file path of the generated CSV report.\n            - pd.DataFrame: DataFrame containing the country-population data.\n\n    Raises:\n        ValueError: If JSON data is malformed, empty, contains non-string country names,\n                    non-numeric or negative populations.\n        IOError: If the file cannot be written to the specified directory.\n    \"\"\"\n    # Parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Malformed JSON data: {e}\")\n\n    if not data:\n        raise ValueError(\"JSON data is empty.\")\n\n    # Normalize data to a list of dictionaries with 'Country' and 'Population'\n    if isinstance(data, dict):\n        data_list = []\n        for country, population in data.items():\n            data_list.append({\"Country\": country, \"Population\": population})\n    elif isinstance(data, list):\n        data_list = data\n    else:\n        raise ValueError(\"JSON data must be a dictionary or a list of dictionaries.\")\n\n    processed_data = []\n    for entry in data_list:\n        if not isinstance(entry, dict):\n            raise ValueError(\"Each entry in JSON data must be a dictionary.\")\n\n        country = entry.get(\"Country\")\n        population = entry.get(\"Population\")\n\n        if country is None or population is None:\n            raise ValueError(\"Each entry must contain 'Country' and 'Population' keys.\")\n\n        if not isinstance(country, str):\n            raise ValueError(f\"Country name must be a string. Found: {type(country).__name__}\")\n\n        if not isinstance(population, (int, float)):\n            raise ValueError(f\"Population must be a number. Found: {type(population).__name__}\")\n\n        if population < 0:\n            raise ValueError(\"Population cannot be negative.\")\n\n        # Round down float populations to the nearest integer\n        population_int = math.floor(population) if isinstance(population, float) else population\n\n        processed_data.append({\n            \"Country\": country,\n            \"Population\": population_int\n        })\n\n    if not processed_data:\n        raise ValueError(\"No valid data to process.\")\n\n    # Create DataFrame\n    df = pd.DataFrame(processed_data, columns=[\"Country\", \"Population\"])\n\n    # Define file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write DataFrame to CSV\n    try:\n        df.to_csv(file_path, index=False)\n    except Exception as e:\n        raise IOError(f\"Failed to write CSV file: {e}\")\n\n    return file_path, df", "raw_solution": "```python\nimport pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates a population report DataFrame and CSV file based on provided JSON data.\n\n    Parameters:\n        json_data (str): JSON string containing country population data.\n        output_dir (str): Directory where the CSV report will be saved. Defaults to current directory.\n        file_name (str): Name of the output CSV file. Defaults to 'country_population_report.csv'.\n\n    Returns:\n        tuple:\n            - str: The file path of the generated CSV report.\n            - pd.DataFrame: DataFrame containing the country-population data.\n\n    Raises:\n        ValueError: If JSON data is malformed, empty, contains non-string country names,\n                    non-numeric or negative populations.\n        IOError: If the file cannot be written to the specified directory.\n    \"\"\"\n    # Parse JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Malformed JSON data: {e}\")\n\n    if not data:\n        raise ValueError(\"JSON data is empty.\")\n\n    # Normalize data to a list of dictionaries with 'Country' and 'Population'\n    if isinstance(data, dict):\n        data_list = []\n        for country, population in data.items():\n            data_list.append({\"Country\": country, \"Population\": population})\n    elif isinstance(data, list):\n        data_list = data\n    else:\n        raise ValueError(\"JSON data must be a dictionary or a list of dictionaries.\")\n\n    processed_data = []\n    for entry in data_list:\n        if not isinstance(entry, dict):\n            raise ValueError(\"Each entry in JSON data must be a dictionary.\")\n\n        country = entry.get(\"Country\")\n        population = entry.get(\"Population\")\n\n        if country is None or population is None:\n            raise ValueError(\"Each entry must contain 'Country' and 'Population' keys.\")\n\n        if not isinstance(country, str):\n            raise ValueError(f\"Country name must be a string. Found: {type(country).__name__}\")\n\n        if not isinstance(population, (int, float)):\n            raise ValueError(f\"Population must be a number. Found: {type(population).__name__}\")\n\n        if population < 0:\n            raise ValueError(\"Population cannot be negative.\")\n\n        # Round down float populations to the nearest integer\n        population_int = math.floor(population) if isinstance(population, float) else population\n\n        processed_data.append({\n            \"Country\": country,\n            \"Population\": population_int\n        })\n\n    if not processed_data:\n        raise ValueError(\"No valid data to process.\")\n\n    # Create DataFrame\n    df = pd.DataFrame(processed_data, columns=[\"Country\", \"Population\"])\n\n    # Define file path\n    file_path = os.path.join(output_dir, file_name)\n\n    # Write DataFrame to CSV\n    try:\n        df.to_csv(file_path, index=False)\n    except Exception as e:\n        raise IOError(f\"Failed to write CSV file: {e}\")\n\n    return file_path, df\n```"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (file or directory) in the specified directory against the provided predicates.\n\n    Args:\n        dir_path (str): The path to the directory to evaluate.\n        predicates (list): A list of regex patterns to apply to each item's name.\n\n    Returns:\n        dict: A dictionary where each key is an item name and the value is another dictionary\n              mapping each predicate to a boolean indicating if it matched.\n\n    Raises:\n        ValueError: If no valid predicates are provided after deduplication.\n        FileNotFoundError: If the specified directory does not exist or is not a directory.\n    \"\"\"\n    # Convert dir_path to a Path object\n    path = Path(dir_path)\n    \n    # Check if the path exists and is a directory\n    if not path.exists() or not path.is_dir():\n        raise FileNotFoundError(f\"The directory '{dir_path}' does not exist or is not a directory.\")\n    \n    # Deduplicate predicates\n    unique_predicates = list(set(predicates))\n    \n    # Raise ValueError if no predicates are provided\n    if not unique_predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    \n    # Compile all regex patterns\n    try:\n        compiled_predicates = {pattern: re.compile(pattern) for pattern in unique_predicates}\n    except re.error as e:\n        raise ValueError(f\"Invalid regex pattern in predicates: {e}\")\n    \n    # Initialize the result dictionary\n    results = {}\n    \n    # Iterate over each item in the directory\n    for item in path.iterdir():\n        item_name = item.name\n        predicate_results = {}\n        \n        # Evaluate each predicate against the item's name\n        for pattern, compiled_pattern in compiled_predicates.items():\n            match = bool(compiled_pattern.search(item_name))\n            predicate_results[pattern] = match\n        \n        # Add the results to the main dictionary\n        results[item_name] = predicate_results\n    \n    return results", "raw_solution": "```python\nimport os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (file or directory) in the specified directory against the provided predicates.\n\n    Args:\n        dir_path (str): The path to the directory to evaluate.\n        predicates (list): A list of regex patterns to apply to each item's name.\n\n    Returns:\n        dict: A dictionary where each key is an item name and the value is another dictionary\n              mapping each predicate to a boolean indicating if it matched.\n\n    Raises:\n        ValueError: If no valid predicates are provided after deduplication.\n        FileNotFoundError: If the specified directory does not exist or is not a directory.\n    \"\"\"\n    # Convert dir_path to a Path object\n    path = Path(dir_path)\n    \n    # Check if the path exists and is a directory\n    if not path.exists() or not path.is_dir():\n        raise FileNotFoundError(f\"The directory '{dir_path}' does not exist or is not a directory.\")\n    \n    # Deduplicate predicates\n    unique_predicates = list(set(predicates))\n    \n    # Raise ValueError if no predicates are provided\n    if not unique_predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    \n    # Compile all regex patterns\n    try:\n        compiled_predicates = {pattern: re.compile(pattern) for pattern in unique_predicates}\n    except re.error as e:\n        raise ValueError(f\"Invalid regex pattern in predicates: {e}\")\n    \n    # Initialize the result dictionary\n    results = {}\n    \n    # Iterate over each item in the directory\n    for item in path.iterdir():\n        item_name = item.name\n        predicate_results = {}\n        \n        # Evaluate each predicate against the item's name\n        for pattern, compiled_pattern in compiled_predicates.items():\n            match = bool(compiled_pattern.search(item_name))\n            predicate_results[pattern] = match\n        \n        # Add the results to the main dictionary\n        results[item_name] = predicate_results\n    \n    return results\n```"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    try:\n        # Decode hexadecimal string to bytes\n        decoded_bytes = binascii.unhexlify(hex_string)\n    except (binascii.Error, ValueError) as e:\n        return {'error': 'Invalid hexadecimal input'}\n\n    try:\n        # Decode bytes to UTF-8 string\n        utf8_string = decoded_bytes.decode('utf-8')\n    except UnicodeDecodeError:\n        utf8_string = 'Invalid UTF-8 encoding'\n\n    # Initialize the result dictionary with 'hex' and 'utf-8'\n    result = {\n        'hex': hex_string,\n        'utf-8': utf8_string\n    }\n\n    # Base64 encoding\n    base64_encoded = base64.b64encode(decoded_bytes).decode('utf-8')\n    result['base64'] = base64_encoded\n\n    # UTF-16 encoding and decoding\n    try:\n        utf16_bytes = utf8_string.encode('utf-16')\n        utf16_string = utf16_bytes.decode('utf-16')\n        result['utf-16'] = utf16_string\n    except UnicodeEncodeError:\n        result['utf-16'] = 'Invalid UTF-16 encoding'\n\n    # UTF-32 encoding and decoding\n    try:\n        utf32_bytes = utf8_string.encode('utf-32')\n        utf32_string = utf32_bytes.decode('utf-32')\n        result['utf-32'] = utf32_string\n    except UnicodeEncodeError:\n        result['utf-32'] = 'Invalid UTF-32 encoding'\n\n    # ASCII encoding\n    try:\n        ascii_encoded = utf8_string.encode('ascii').decode('ascii')\n        result['ASCII'] = ascii_encoded\n    except UnicodeEncodeError:\n        result['ASCII'] = 'Not representable in ASCII'\n\n    # URL encoding\n    url_encoded = urllib.parse.quote(utf8_string)\n    result['URL'] = url_encoded\n\n    # ROT13 encoding\n    rot13_encoded = codecs.encode(utf8_string, 'rot_13')\n    result['ROT13'] = rot13_encoded\n\n    return result", "raw_solution": "```python\nimport binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    try:\n        # Decode hexadecimal string to bytes\n        decoded_bytes = binascii.unhexlify(hex_string)\n    except (binascii.Error, ValueError) as e:\n        return {'error': 'Invalid hexadecimal input'}\n\n    try:\n        # Decode bytes to UTF-8 string\n        utf8_string = decoded_bytes.decode('utf-8')\n    except UnicodeDecodeError:\n        utf8_string = 'Invalid UTF-8 encoding'\n\n    # Initialize the result dictionary with 'hex' and 'utf-8'\n    result = {\n        'hex': hex_string,\n        'utf-8': utf8_string\n    }\n\n    # Base64 encoding\n    base64_encoded = base64.b64encode(decoded_bytes).decode('utf-8')\n    result['base64'] = base64_encoded\n\n    # UTF-16 encoding and decoding\n    try:\n        utf16_bytes = utf8_string.encode('utf-16')\n        utf16_string = utf16_bytes.decode('utf-16')\n        result['utf-16'] = utf16_string\n    except UnicodeEncodeError:\n        result['utf-16'] = 'Invalid UTF-16 encoding'\n\n    # UTF-32 encoding and decoding\n    try:\n        utf32_bytes = utf8_string.encode('utf-32')\n        utf32_string = utf32_bytes.decode('utf-32')\n        result['utf-32'] = utf32_string\n    except UnicodeEncodeError:\n        result['utf-32'] = 'Invalid UTF-32 encoding'\n\n    # ASCII encoding\n    try:\n        ascii_encoded = utf8_string.encode('ascii').decode('ascii')\n        result['ASCII'] = ascii_encoded\n    except UnicodeEncodeError:\n        result['ASCII'] = 'Not representable in ASCII'\n\n    # URL encoding\n    url_encoded = urllib.parse.quote(utf8_string)\n    result['URL'] = url_encoded\n\n    # ROT13 encoding\n    rot13_encoded = codecs.encode(utf8_string, 'rot_13')\n    result['ROT13'] = rot13_encoded\n\n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    input_hex = \"68656c6c6f\"\n    encoded_dict = task_func(input_hex)\n    print(encoded_dict)\n```"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    \"\"\"\n    Downloads a tar.gz file from the specified URL, validates its MD5 checksum,\n    and extracts it if the checksum is correct.\n\n    Args:\n        url (str): The URL to download the tar.gz file from.\n\n    Returns:\n        bool: True if the file is successfully downloaded, its MD5 checksum matches,\n              and it is extracted. False otherwise.\n    \"\"\"\n    try:\n        print(f\"Starting download from {url}...\")\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        print(f\"Downloaded file saved as {TARGET_TAR_FILE}.\")\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n\n    # Calculate MD5 checksum\n    try:\n        print(\"Calculating MD5 checksum...\")\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        file_md5 = md5_hash.hexdigest()\n        print(f\"Calculated MD5: {file_md5}\")\n    except Exception as e:\n        print(f\"Failed to calculate MD5 checksum: {e}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n            print(f\"Deleted corrupted file {TARGET_TAR_FILE}.\")\n        return False\n\n    # Compare MD5 checksum\n    if file_md5.lower() == EXPECTED_MD5_CHECKSUM.lower():\n        print(\"MD5 checksum matches. Extracting the tar.gz file...\")\n        try:\n            with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n                tar.extractall()\n            print(\"Extraction completed successfully.\")\n            return True\n        except Exception as e:\n            print(f\"Extraction failed: {e}\")\n            return False\n    else:\n        print(\"MD5 checksum does not match. Deleting the downloaded file.\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n            print(f\"Deleted file {TARGET_TAR_FILE}.\")\n        return False", "raw_solution": "```python\nimport urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    \"\"\"\n    Downloads a tar.gz file from the specified URL, validates its MD5 checksum,\n    and extracts it if the checksum is correct.\n\n    Args:\n        url (str): The URL to download the tar.gz file from.\n\n    Returns:\n        bool: True if the file is successfully downloaded, its MD5 checksum matches,\n              and it is extracted. False otherwise.\n    \"\"\"\n    try:\n        print(f\"Starting download from {url}...\")\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        print(f\"Downloaded file saved as {TARGET_TAR_FILE}.\")\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        return False\n\n    # Calculate MD5 checksum\n    try:\n        print(\"Calculating MD5 checksum...\")\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        file_md5 = md5_hash.hexdigest()\n        print(f\"Calculated MD5: {file_md5}\")\n    except Exception as e:\n        print(f\"Failed to calculate MD5 checksum: {e}\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n            print(f\"Deleted corrupted file {TARGET_TAR_FILE}.\")\n        return False\n\n    # Compare MD5 checksum\n    if file_md5.lower() == EXPECTED_MD5_CHECKSUM.lower():\n        print(\"MD5 checksum matches. Extracting the tar.gz file...\")\n        try:\n            with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n                tar.extractall()\n            print(\"Extraction completed successfully.\")\n            return True\n        except Exception as e:\n            print(f\"Extraction failed: {e}\")\n            return False\n    else:\n        print(\"MD5 checksum does not match. Deleting the downloaded file.\")\n        if os.path.exists(TARGET_TAR_FILE):\n            os.remove(TARGET_TAR_FILE)\n            print(f\"Deleted file {TARGET_TAR_FILE}.\")\n        return False\n\n# Example usage:\n# url = \"http://example.com/path/to/file.tar.gz\"\n# success = task_func(url)\n# print(f\"Operation successful: {success}\")\n```"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a Counter to count occurrences\n        counter = collections.Counter()\n        \n        # Open and read the CSV file\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            \n            # Check if the specified column exists\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                value = row[column_name]\n                counter[value] += 1\n                \n        # Return the count as a regular dictionary\n        return dict(counter)\n    \n    except urllib.error.URLError as e:\n        # Handle file download errors\n        raise e\n    \n    finally:\n        # Delete the downloaded CSV file after processing\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)", "raw_solution": "```python\nimport urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file from the given URL\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a Counter to count occurrences\n        counter = collections.Counter()\n        \n        # Open and read the CSV file\n        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n            reader = csv.DictReader(csvfile)\n            \n            # Check if the specified column exists\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                value = row[column_name]\n                counter[value] += 1\n                \n        # Return the count as a regular dictionary\n        return dict(counter)\n    \n    except urllib.error.URLError as e:\n        # Handle file download errors\n        raise e\n    \n    finally:\n        # Delete the downloaded CSV file after processing\n        if os.path.exists(csv_file_path):\n            os.remove(csv_file_path)\n```"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom urllib.error import URLError, HTTPError\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML content from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n    except (URLError, HTTPError) as e:\n        raise ValueError(\"Invalid URL or cannot fetch XML.\") from e\n\n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(\"Invalid XML syntax.\") from e\n\n    # Find all 'item' elements\n    items = root.findall('.//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    data = []\n    columns = set()\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n            columns.add(child.tag)\n        data.append(item_data)\n\n    if not data:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=sorted(columns))\n    return df", "raw_solution": "```python\nimport urllib.request\nfrom urllib.error import URLError, HTTPError\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML content from the URL\n        with urllib.request.urlopen(url) as response:\n            xml_content = response.read()\n    except (URLError, HTTPError) as e:\n        raise ValueError(\"Invalid URL or cannot fetch XML.\") from e\n\n    try:\n        # Parse the XML content\n        root = etree.fromstring(xml_content)\n    except etree.XMLSyntaxError as e:\n        raise ValueError(\"Invalid XML syntax.\") from e\n\n    # Find all 'item' elements\n    items = root.findall('.//item')\n    if not items:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    data = []\n    columns = set()\n    for item in items:\n        item_data = {}\n        for child in item:\n            item_data[child.tag] = child.text\n            columns.add(child.tag)\n        data.append(item_data)\n\n    if not data:\n        raise ValueError(\"XML structure does not match expected format.\")\n\n    # Create DataFrame\n    df = pd.DataFrame(data, columns=sorted(columns))\n    return df\n```"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from the specified URL, counts the frequency of each word,\n    and plots a bar chart of the ten most frequently occurring words.\n    \n    Parameters:\n        url (str): The URL pointing to a plain text file.\n    \n    Returns:\n        tuple: A tuple containing:\n            - Counter: A Counter object with word frequencies.\n            - Axes: A matplotlib Axes object with the bar chart.\n    \"\"\"\n    # Download the text content from the URL\n    with urllib.request.urlopen(url) as response:\n        charset = response.headers.get_content_charset() or 'utf-8'\n        text = response.read().decode(charset)\n    \n    # Use a regular expression to find all words (case-sensitive)\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    top_ten = word_counts.most_common(10)\n    words, counts = zip(*top_ten) if top_ten else ([], [])\n    \n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, counts, color='skyblue')\n    ax.set_title('Top 10 Most Frequent Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_xticklabels(words, rotation=45, ha='right')\n    plt.tight_layout()\n    \n    return (word_counts, ax)", "raw_solution": "```python\nimport urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    \"\"\"\n    Downloads a text file from the specified URL, counts the frequency of each word,\n    and plots a bar chart of the ten most frequently occurring words.\n    \n    Parameters:\n        url (str): The URL pointing to a plain text file.\n    \n    Returns:\n        tuple: A tuple containing:\n            - Counter: A Counter object with word frequencies.\n            - Axes: A matplotlib Axes object with the bar chart.\n    \"\"\"\n    # Download the text content from the URL\n    with urllib.request.urlopen(url) as response:\n        charset = response.headers.get_content_charset() or 'utf-8'\n        text = response.read().decode(charset)\n    \n    # Use a regular expression to find all words (case-sensitive)\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the ten most common words\n    top_ten = word_counts.most_common(10)\n    words, counts = zip(*top_ten) if top_ten else ([], [])\n    \n    # Create a bar chart\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.bar(words, counts, color='skyblue')\n    ax.set_title('Top 10 Most Frequent Words')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_xticklabels(words, rotation=45, ha='right')\n    plt.tight_layout()\n    \n    return (word_counts, ax)\n```"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n        \n        # Download the file\n        response = requests.get(url, stream=True, timeout=10)\n        response.raise_for_status()\n        \n        # Check if the content type is 'application/zip'\n        content_type = response.headers.get('Content-Type', '')\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Determine the filename from URL\n        filename = url.split('/')[-1]\n        if not filename.endswith('.zip'):\n            filename += '.zip'\n        local_zip_path = os.path.join(download_path, filename)\n        \n        # Save the downloaded content to a file\n        with open(local_zip_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    file.write(chunk)\n        \n        # Create extraction directory\n        extract_dir = os.path.join(download_path, os.path.splitext(filename)[0])\n        os.makedirs(extract_dir, exist_ok=True)\n        \n        # Extract the ZIP file\n        with ZipFile(local_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_dir)\n        \n        return extract_dir\n    \n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except BadZipFile:\n        return \"Error: The downloaded file is not a valid ZIP file.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"", "raw_solution": "```python\nimport os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n        \n        # Download the file\n        response = requests.get(url, stream=True, timeout=10)\n        response.raise_for_status()\n        \n        # Check if the content type is 'application/zip'\n        content_type = response.headers.get('Content-Type', '')\n        if 'application/zip' not in content_type:\n            return \"Error: The URL does not point to a ZIP file.\"\n        \n        # Determine the filename from URL\n        filename = url.split('/')[-1]\n        if not filename.endswith('.zip'):\n            filename += '.zip'\n        local_zip_path = os.path.join(download_path, filename)\n        \n        # Save the downloaded content to a file\n        with open(local_zip_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    file.write(chunk)\n        \n        # Create extraction directory\n        extract_dir = os.path.join(download_path, os.path.splitext(filename)[0])\n        os.makedirs(extract_dir, exist_ok=True)\n        \n        # Extract the ZIP file\n        with ZipFile(local_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_dir)\n        \n        return extract_dir\n    \n    except requests.exceptions.RequestException:\n        return \"Error: Unable to download the file from the provided URL.\"\n    except BadZipFile:\n        return \"Error: The downloaded file is not a valid ZIP file.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts and converts data from a specified HTML table into a Pandas DataFrame.\n\n    Parameters:\n        url (str): The URL of the webpage containing the HTML table.\n        table_id (str): The ID of the table to extract.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the table data. Returns an empty DataFrame if the table has no data rows.\n\n    Raises:\n        requests.exceptions.HTTPError: If the HTTP request fails.\n        ValueError: If no table with the specified ID is found.\n    \"\"\"\n    # Send HTTP GET request to the specified URL\n    response = requests.get(url)\n    # Raise an HTTPError if the request was unsuccessful\n    response.raise_for_status()\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find the table with the specified ID\n    table = soup.find('table', id=table_id)\n    if not table:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    # Find all <tr> tags within the table\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()  # Return empty DataFrame if no rows are found\n\n    # Initialize lists to store headers and data\n    headers = []\n    data = []\n\n    # Assume the first row contains headers if <th> tags are present\n    first_row = rows[0]\n    header_cells = first_row.find_all('th')\n    if header_cells:\n        headers = [cell.get_text(strip=True) for cell in header_cells]\n        data_rows = rows[1:]  # Data starts from the second row\n    else:\n        data_rows = rows  # No headers, all rows are data\n\n    # Iterate over each data row and extract cell values\n    for row in data_rows:\n        cells = row.find_all(['td', 'th'])  # Include <th> in case of nested headers\n        row_data = [cell.get_text(strip=True) for cell in cells]\n        data.append(row_data)\n\n    # Create DataFrame with or without headers\n    if headers:\n        df = pd.DataFrame(data, columns=headers)\n    else:\n        df = pd.DataFrame(data)\n\n    return df", "raw_solution": "```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts and converts data from a specified HTML table into a Pandas DataFrame.\n\n    Parameters:\n        url (str): The URL of the webpage containing the HTML table.\n        table_id (str): The ID of the table to extract.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the table data. Returns an empty DataFrame if the table has no data rows.\n\n    Raises:\n        requests.exceptions.HTTPError: If the HTTP request fails.\n        ValueError: If no table with the specified ID is found.\n    \"\"\"\n    # Send HTTP GET request to the specified URL\n    response = requests.get(url)\n    # Raise an HTTPError if the request was unsuccessful\n    response.raise_for_status()\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find the table with the specified ID\n    table = soup.find('table', id=table_id)\n    if not table:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    # Find all <tr> tags within the table\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()  # Return empty DataFrame if no rows are found\n\n    # Initialize lists to store headers and data\n    headers = []\n    data = []\n\n    # Assume the first row contains headers if <th> tags are present\n    first_row = rows[0]\n    header_cells = first_row.find_all('th')\n    if header_cells:\n        headers = [cell.get_text(strip=True) for cell in header_cells]\n        data_rows = rows[1:]  # Data starts from the second row\n    else:\n        data_rows = rows  # No headers, all rows are data\n\n    # Iterate over each data row and extract cell values\n    for row in data_rows:\n        cells = row.find_all(['td', 'th'])  # Include <th> in case of nested headers\n        row_data = [cell.get_text(strip=True) for cell in cells]\n        data.append(row_data)\n\n    # Create DataFrame with or without headers\n    if headers:\n        df = pd.DataFrame(data, columns=headers)\n    else:\n        df = pd.DataFrame(data)\n\n    return df\n```\n\n**Usage Examples:**\n\n```python\n# Example 1: Table does not exist\ntry:\n    df = task_func('https://example.com/data.html', 'nonexistent_table')\nexcept ValueError as ve:\n    print(ve)  # Output: Table with the specified ID not found.\n\n# Example 2: Table exists but is empty\ndf = task_func('https://example.com/emptytable.html', 'empty_table')\nprint(df)\n# Output:\n# Empty DataFrame\n# Columns: []\n# Index: []\n```\n\n**Notes:**\n- Ensure that the `requests`, `beautifulsoup4`, and `pandas` libraries are installed in your Python environment.\n- You can install any missing libraries using `pip`. For example:\n  ```bash\n  pip install requests beautifulsoup4 pandas\n  ```\n- The function handles HTTP errors gracefully by raising `requests.exceptions.HTTPError`.\n- It specifically checks for the presence of a table with the given ID and handles cases where the table might be empty."}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    \"\"\"\n    Downloads a zip file from the specified URL and extracts its contents.\n\n    Parameters:\n        url (str): The URL to download the zip file from.\n        filename (str): The name to save the downloaded zip file as.\n\n    Returns:\n        tuple: A tuple containing a status message and a list of filenames in the unzipped directory,\n               or an empty list if extraction fails.\n    \"\"\"\n    # Ensure download and extraction directories exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    download_path = DOWNLOAD_DIR / filename\n    extracted_files = []\n    \n    try:\n        # Download the zip file\n        with requests.get(url, stream=True) as response:\n            response.raise_for_status()  # Raise HTTPError for bad responses\n            with open(download_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n        status_message = \"Download successful.\"\n    except requests.exceptions.RequestException as e:\n        status_message = f\"Error during download: {e}\"\n        return (status_message, [])\n\n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        # List extracted files\n        extracted_files = [str(file) for file in ZIP_DIR.glob('*') if file.is_file()]\n        status_message = \"Download and extraction successful.\"\n    except (zipfile.BadZipFile, zipfile.LargeZipFile, OSError) as e:\n        status_message = f\"Error during extraction: {e}\"\n        extracted_files = []\n    \n    return (status_message, extracted_files)", "raw_solution": "```python\nimport requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    \"\"\"\n    Downloads a zip file from the specified URL and extracts its contents.\n\n    Parameters:\n        url (str): The URL to download the zip file from.\n        filename (str): The name to save the downloaded zip file as.\n\n    Returns:\n        tuple: A tuple containing a status message and a list of filenames in the unzipped directory,\n               or an empty list if extraction fails.\n    \"\"\"\n    # Ensure download and extraction directories exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    download_path = DOWNLOAD_DIR / filename\n    extracted_files = []\n    \n    try:\n        # Download the zip file\n        with requests.get(url, stream=True) as response:\n            response.raise_for_status()  # Raise HTTPError for bad responses\n            with open(download_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n        status_message = \"Download successful.\"\n    except requests.exceptions.RequestException as e:\n        status_message = f\"Error during download: {e}\"\n        return (status_message, [])\n\n    try:\n        # Extract the zip file\n        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        # List extracted files\n        extracted_files = [str(file) for file in ZIP_DIR.glob('*') if file.is_file()]\n        status_message = \"Download and extraction successful.\"\n    except (zipfile.BadZipFile, zipfile.LargeZipFile, OSError) as e:\n        status_message = f\"Error during extraction: {e}\"\n        extracted_files = []\n    \n    return (status_message, extracted_files)\n\nif __name__ == \"__main__\":\n    # Example usage\n    sample_url = \"https://example.com/sample.zip\"  # Replace with a valid URL\n    sample_filename = \"sample.zip\"\n    \n    status, files = task_func(sample_url, sample_filename)\n    print(status)\n    if files:\n        print(\"Extracted files:\")\n        for file in files:\n            print(f\"- {file}\")\n    else:\n        print(\"No files were extracted.\")\n```"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin, urlparse\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    Scrapes all unique absolute hyperlinks from the specified webpage and saves them to a CSV file.\n\n    Args:\n        url (str): The URL of the webpage to scrape.\n        base_url (str): The base URL to resolve relative links. Defaults to \"https://www.example.com\".\n        csv_file (str): The filename for the CSV output. Defaults to \"scraped_data.csv\".\n\n    Returns:\n        int: The number of unique absolute links scraped.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        return 0\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    hrefs = soup.find_all('a', href=True)\n\n    unique_links = set()\n    for tag in hrefs:\n        href = tag.get('href')\n        absolute_url = urljoin(base_url, href)\n        # Optional: Validate the URL\n        parsed_url = urlparse(absolute_url)\n        if parsed_url.scheme and parsed_url.netloc:\n            unique_links.add(absolute_url)\n\n    try:\n        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow(['url'])\n            for link in sorted(unique_links):\n                writer.writerow([link])\n    except IOError as e:\n        print(f\"Error writing to CSV file: {e}\")\n        return 0\n\n    return len(unique_links)", "raw_solution": "```python\nimport requests\nfrom urllib.parse import urljoin, urlparse\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    Scrapes all unique absolute hyperlinks from the specified webpage and saves them to a CSV file.\n\n    Args:\n        url (str): The URL of the webpage to scrape.\n        base_url (str): The base URL to resolve relative links. Defaults to \"https://www.example.com\".\n        csv_file (str): The filename for the CSV output. Defaults to \"scraped_data.csv\".\n\n    Returns:\n        int: The number of unique absolute links scraped.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        return 0\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    hrefs = soup.find_all('a', href=True)\n\n    unique_links = set()\n    for tag in hrefs:\n        href = tag.get('href')\n        absolute_url = urljoin(base_url, href)\n        # Optional: Validate the URL\n        parsed_url = urlparse(absolute_url)\n        if parsed_url.scheme and parsed_url.netloc:\n            unique_links.add(absolute_url)\n\n    try:\n        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow(['url'])\n            for link in sorted(unique_links):\n                writer.writerow([link])\n    except IOError as e:\n        print(f\"Error writing to CSV file: {e}\")\n        return 0\n\n    return len(unique_links)\n\nif __name__ == \"__main__\":\n    # Example usage\n    webpage_url = \"https://www.example.com\"  # Replace with the target URL\n    base = \"https://www.example.com\"        # Replace with the appropriate base URL if needed\n    output_csv = \"scraped_links.csv\"        # Desired CSV filename\n\n    count = task_func(webpage_url, base, output_csv)\n    print(f\"Number of unique absolute links scraped: {count}\")\n    print(f\"Links have been saved to '{output_csv}'.\")\n```"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    Parses an HTML table from a specified URL or local file and stores it into an SQLite database.\n    \n    Args:\n        webpage_url (str): The URL of the webpage or the path to the local HTML file containing the table.\n        database_name (str, optional): The name of the SQLite database file. Defaults to \"my_database.db\".\n    \n    Returns:\n        int: The number of rows in the parsed HTML table. Returns 0 if no table is found or if the table is empty.\n    \n    Raises:\n        requests.RequestException: If there is an issue accessing the URL.\n        sqlite3.DatabaseError: If there is an issue with the SQLite database operations.\n        FileNotFoundError: If the local file does not exist.\n        ValueError: If the HTML content does not contain any tables.\n    \"\"\"\n    # Determine if the input is a URL or a local file path\n    if webpage_url.startswith('http://') or webpage_url.startswith('https://'):\n        # It's a URL; attempt to fetch the content\n        try:\n            response = requests.get(webpage_url)\n            response.raise_for_status()  # Raises HTTPError for bad responses\n            html_content = response.text\n        except requests.RequestException as e:\n            # Propagate RequestException for network-related errors\n            raise e\n    else:\n        # Assume it's a local file path; attempt to read the file\n        try:\n            with open(webpage_url, 'r', encoding='utf-8') as file:\n                html_content = file.read()\n        except FileNotFoundError as e:\n            # Propagate FileNotFoundError if the file does not exist\n            raise e\n        except Exception as e:\n            # Propagate any other exceptions related to file operations\n            raise e\n    \n    # Attempt to parse HTML tables using pandas\n    try:\n        tables = pd.read_html(html_content, flavor='lxml')\n    except ValueError:\n        # No tables found in the HTML content\n        return 0\n    except Exception as e:\n        # Propagate any other exceptions related to parsing\n        raise e\n    \n    # Check if any tables were found\n    if not tables:\n        return 0\n    \n    # Use the first table found\n    df = tables[0]\n    \n    # Check if the table is empty\n    if df.empty:\n        return 0\n    \n    # Get the number of rows in the table\n    num_rows = len(df)\n    \n    # Connect to the SQLite database and replace \"my_table\" with the new data\n    try:\n        with sqlite3.connect(database_name) as conn:\n            df.to_sql('my_table', conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        # Propagate DatabaseError for any SQLite-related issues\n        raise e\n    \n    return num_rows", "raw_solution": "```python\nimport requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    Parses an HTML table from a specified URL or local file and stores it into an SQLite database.\n    \n    Args:\n        webpage_url (str): The URL of the webpage or the path to the local HTML file containing the table.\n        database_name (str, optional): The name of the SQLite database file. Defaults to \"my_database.db\".\n    \n    Returns:\n        int: The number of rows in the parsed HTML table. Returns 0 if no table is found or if the table is empty.\n    \n    Raises:\n        requests.RequestException: If there is an issue accessing the URL.\n        sqlite3.DatabaseError: If there is an issue with the SQLite database operations.\n        FileNotFoundError: If the local file does not exist.\n        ValueError: If the HTML content does not contain any tables.\n    \"\"\"\n    # Determine if the input is a URL or a local file path\n    if webpage_url.startswith('http://') or webpage_url.startswith('https://'):\n        # It's a URL; attempt to fetch the content\n        try:\n            response = requests.get(webpage_url)\n            response.raise_for_status()  # Raises HTTPError for bad responses\n            html_content = response.text\n        except requests.RequestException as e:\n            # Propagate RequestException for network-related errors\n            raise e\n    else:\n        # Assume it's a local file path; attempt to read the file\n        try:\n            with open(webpage_url, 'r', encoding='utf-8') as file:\n                html_content = file.read()\n        except FileNotFoundError as e:\n            # Propagate FileNotFoundError if the file does not exist\n            raise e\n        except Exception as e:\n            # Propagate any other exceptions related to file operations\n            raise e\n    \n    # Attempt to parse HTML tables using pandas\n    try:\n        tables = pd.read_html(html_content, flavor='lxml')\n    except ValueError:\n        # No tables found in the HTML content\n        return 0\n    except Exception as e:\n        # Propagate any other exceptions related to parsing\n        raise e\n    \n    # Check if any tables were found\n    if not tables:\n        return 0\n    \n    # Use the first table found\n    df = tables[0]\n    \n    # Check if the table is empty\n    if df.empty:\n        return 0\n    \n    # Get the number of rows in the table\n    num_rows = len(df)\n    \n    # Connect to the SQLite database and replace \"my_table\" with the new data\n    try:\n        with sqlite3.connect(database_name) as conn:\n            df.to_sql('my_table', conn, if_exists='replace', index=False)\n    except sqlite3.DatabaseError as e:\n        # Propagate DatabaseError for any SQLite-related issues\n        raise e\n    \n    return num_rows\n```"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    Extracts text from an image using OCR and converts its encoding.\n    Falls back to image comments if OCR fails.\n\n    Args:\n        filename (str): Path to the image file.\n        from_encoding (str): Original encoding of the text.\n        to_encoding (str): Target encoding for the text.\n\n    Returns:\n        str: Converted text from OCR or image comments. Empty string if both fail.\n\n    Raises:\n        ValueError: If encoding conversion fails or incorrect encodings are provided.\n    \"\"\"\n    try:\n        # Open the image file\n        img = Image.open(filename)\n    except Exception as e:\n        raise ValueError(f\"Failed to open image file: {e}\") from e\n\n    comment = \"\"\n\n    # Attempt OCR extraction\n    try:\n        ocr_text = pytesseract.image_to_string(img)\n        if ocr_text.strip():\n            try:\n                # Encode to bytes using from_encoding, then decode to to_encoding\n                encoded_bytes = ocr_text.encode(from_encoding)\n                comment = encoded_bytes.decode(to_encoding)\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(f\"Encoding conversion failed during OCR processing: {e}\") from e\n            except Exception as e:\n                raise ValueError(f\"Unexpected error during OCR encoding conversion: {e}\") from e\n    except Exception:\n        # OCR failed, attempt to extract image comment\n        image_comment = img.info.get('comment', '')\n        if image_comment:\n            try:\n                # Encode to bytes using from_encoding, then decode to to_encoding\n                encoded_bytes = image_comment.encode(from_encoding)\n                comment = encoded_bytes.decode(to_encoding)\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(f\"Encoding conversion failed during comment processing: {e}\") from e\n            except Exception as e:\n                raise ValueError(f\"Unexpected error during comment encoding conversion: {e}\") from e\n\n    return comment", "raw_solution": "```python\nfrom PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    Extracts text from an image using OCR and converts its encoding.\n    Falls back to image comments if OCR fails.\n\n    Args:\n        filename (str): Path to the image file.\n        from_encoding (str): Original encoding of the text.\n        to_encoding (str): Target encoding for the text.\n\n    Returns:\n        str: Converted text from OCR or image comments. Empty string if both fail.\n\n    Raises:\n        ValueError: If encoding conversion fails or incorrect encodings are provided.\n    \"\"\"\n    try:\n        # Open the image file\n        img = Image.open(filename)\n    except Exception as e:\n        raise ValueError(f\"Failed to open image file: {e}\") from e\n\n    comment = \"\"\n\n    # Attempt OCR extraction\n    try:\n        ocr_text = pytesseract.image_to_string(img)\n        if ocr_text.strip():\n            try:\n                # Encode to bytes using from_encoding, then decode to to_encoding\n                encoded_bytes = ocr_text.encode(from_encoding)\n                comment = encoded_bytes.decode(to_encoding)\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(f\"Encoding conversion failed during OCR processing: {e}\") from e\n            except Exception as e:\n                raise ValueError(f\"Unexpected error during OCR encoding conversion: {e}\") from e\n    except Exception:\n        # OCR failed, attempt to extract image comment\n        image_comment = img.info.get('comment', '')\n        if image_comment:\n            try:\n                # Encode to bytes using from_encoding, then decode to to_encoding\n                encoded_bytes = image_comment.encode(from_encoding)\n                comment = encoded_bytes.decode(to_encoding)\n            except (UnicodeDecodeError, LookupError) as e:\n                raise ValueError(f\"Encoding conversion failed during comment processing: {e}\") from e\n            except Exception as e:\n                raise ValueError(f\"Unexpected error during comment encoding conversion: {e}\") from e\n\n    return comment\n```"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    Fetches data from a specified REST API URL and processes it for JSON parsing.\n    \n    Parameters:\n        url (str): The API endpoint URL.\n        from_encoding (str, optional): The encoding to decode the response content. If None, encoding will be detected.\n        to_encoding (str): The encoding to re-encode the decoded content. Defaults to \"utf8\".\n    \n    Returns:\n        dict: The JSON-parsed data after re-encoding. Returns an empty dictionary if the content is empty.\n    \n    Raises:\n        ValueError: If unable to detect encoding for non-empty content.\n        requests.RequestException: For network-related errors.\n        json.JSONDecodeError: If the re-encoded content is not valid JSON.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        # Detect encoding if not provided\n        if from_encoding:\n            encoding = from_encoding\n        else:\n            detection = chardet.detect(content)\n            encoding = detection.get('encoding')\n            confidence = detection.get('confidence', 0)\n            if not encoding or confidence < 0.5:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode the content using the detected or provided encoding\n        decoded_text = content.decode(encoding)\n\n        # Re-encode the text to the target encoding\n        re_encoded_bytes = decoded_text.encode(to_encoding)\n\n        # Decode back to string using the target encoding for JSON parsing\n        re_encoded_text = re_encoded_bytes.decode(to_encoding)\n\n        # Parse the JSON data\n        json_data = json.loads(re_encoded_text)\n\n        return json_data\n\n    except requests.RequestException as e:\n        # Handle network-related errors\n        raise e\n    except UnicodeDecodeError as e:\n        # Handle decoding errors\n        raise e\n    except UnicodeEncodeError as e:\n        # Handle encoding errors\n        raise e\n    except json.JSONDecodeError as e:\n        # Handle JSON parsing errors\n        raise e", "raw_solution": "```python\nimport json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    Fetches data from a specified REST API URL and processes it for JSON parsing.\n    \n    Parameters:\n        url (str): The API endpoint URL.\n        from_encoding (str, optional): The encoding to decode the response content. If None, encoding will be detected.\n        to_encoding (str): The encoding to re-encode the decoded content. Defaults to \"utf8\".\n    \n    Returns:\n        dict: The JSON-parsed data after re-encoding. Returns an empty dictionary if the content is empty.\n    \n    Raises:\n        ValueError: If unable to detect encoding for non-empty content.\n        requests.RequestException: For network-related errors.\n        json.JSONDecodeError: If the re-encoded content is not valid JSON.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        content = response.content\n\n        if not content:\n            return {}\n\n        # Detect encoding if not provided\n        if from_encoding:\n            encoding = from_encoding\n        else:\n            detection = chardet.detect(content)\n            encoding = detection.get('encoding')\n            confidence = detection.get('confidence', 0)\n            if not encoding or confidence < 0.5:\n                raise ValueError(\"Unable to detect encoding for non-empty content\")\n\n        # Decode the content using the detected or provided encoding\n        decoded_text = content.decode(encoding)\n\n        # Re-encode the text to the target encoding\n        re_encoded_bytes = decoded_text.encode(to_encoding)\n\n        # Decode back to string using the target encoding for JSON parsing\n        re_encoded_text = re_encoded_bytes.decode(to_encoding)\n\n        # Parse the JSON data\n        json_data = json.loads(re_encoded_text)\n\n        return json_data\n\n    except requests.RequestException as e:\n        # Handle network-related errors\n        raise e\n    except UnicodeDecodeError as e:\n        # Handle decoding errors\n        raise e\n    except UnicodeEncodeError as e:\n        # Handle encoding errors\n        raise e\n    except json.JSONDecodeError as e:\n        # Handle JSON parsing errors\n        raise e\n```"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Processes a CSV file's date-related data.\n\n    Parameters:\n    - csv_file_path (str): Path to the CSV file.\n    - column_name (str): Name of the date column to process.\n    - date_format (str): Format of the date strings in the column.\n\n    Returns:\n    - pd.DataFrame: Processed DataFrame after filtering and sorting.\n\n    Raises:\n    - FileNotFoundError: If the CSV file does not exist at the given path.\n    - ValueError: If the specified column is not present in the CSV file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file '{csv_file_path}' does not exist.\")\n\n    try:\n        # Attempt to read the CSV file\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # Return an empty DataFrame if the CSV is empty\n        return pd.DataFrame()\n\n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' is not present in the CSV file.\")\n\n    try:\n        # Convert the specified column to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    except ValueError as ve:\n        # Raise an error if date conversion fails\n        raise ValueError(f\"Date conversion failed: {ve}\")\n\n    # Get the current datetime\n    current_datetime = datetime.now()\n\n    # Filter rows where the date is on or before the current date\n    filtered_df = df[df[column_name] <= current_datetime]\n\n    # Sort the resulting DataFrame by the date column\n    sorted_df = filtered_df.sort_values(by=column_name).reset_index(drop=True)\n\n    return sorted_df", "raw_solution": "```python\nimport pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Processes a CSV file's date-related data.\n\n    Parameters:\n    - csv_file_path (str): Path to the CSV file.\n    - column_name (str): Name of the date column to process.\n    - date_format (str): Format of the date strings in the column.\n\n    Returns:\n    - pd.DataFrame: Processed DataFrame after filtering and sorting.\n\n    Raises:\n    - FileNotFoundError: If the CSV file does not exist at the given path.\n    - ValueError: If the specified column is not present in the CSV file.\n    \"\"\"\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file '{csv_file_path}' does not exist.\")\n\n    try:\n        # Attempt to read the CSV file\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # Return an empty DataFrame if the CSV is empty\n        return pd.DataFrame()\n\n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column '{column_name}' is not present in the CSV file.\")\n\n    try:\n        # Convert the specified column to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    except ValueError as ve:\n        # Raise an error if date conversion fails\n        raise ValueError(f\"Date conversion failed: {ve}\")\n\n    # Get the current datetime\n    current_datetime = datetime.now()\n\n    # Filter rows where the date is on or before the current date\n    filtered_df = df[df[column_name] <= current_datetime]\n\n    # Sort the resulting DataFrame by the date column\n    sorted_df = filtered_df.sort_values(by=column_name).reset_index(drop=True)\n\n    return sorted_df\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Replace 'data.csv' with your CSV file path and 'date_column' with your date column name\n    csv_path = 'data.csv'\n    date_col = 'date_column'\n    \n    try:\n        result_df = task_func(csv_path, date_col)\n        print(\"Processed DataFrame:\")\n        print(result_df)\n    except FileNotFoundError as fnf_error:\n        print(fnf_error)\n    except ValueError as val_error:\n        print(val_error)\n```"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Set up SSL context\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n\n        # Wrap the client socket with SSL\n        ssl_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        data = ssl_socket.recv(buffer_size)\n        if not data:\n            raise ValueError(\"No data received from the client.\")\n        file_path = data.decode('utf-8').strip()\n\n        # Check if the file exists\n        if os.path.isfile(file_path):\n            # Compute SHA256 hash of the file\n            sha256_hash = hashlib.sha256()\n            with open(file_path, \"rb\") as f:\n                for byte_block in iter(lambda: f.read(4096), b\"\"):\n                    sha256_hash.update(byte_block)\n            hash_str = sha256_hash.hexdigest()\n\n            # Send the hash back to the client\n            ssl_socket.sendall(hash_str.encode('utf-8'))\n\n            return hash_str\n        else:\n            message = 'File not found'\n            ssl_socket.sendall(message.encode('utf-8'))\n            return message\n\n    except Exception as e:\n        error_message = f'Error: {str(e)}'\n        try:\n            ssl_socket.sendall(error_message.encode('utf-8'))\n        except:\n            pass  # If sending fails, there's not much we can do\n        return error_message", "raw_solution": "```python\nimport ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Set up SSL context\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n\n        # Wrap the client socket with SSL\n        ssl_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        data = ssl_socket.recv(buffer_size)\n        if not data:\n            raise ValueError(\"No data received from the client.\")\n        file_path = data.decode('utf-8').strip()\n\n        # Check if the file exists\n        if os.path.isfile(file_path):\n            # Compute SHA256 hash of the file\n            sha256_hash = hashlib.sha256()\n            with open(file_path, \"rb\") as f:\n                for byte_block in iter(lambda: f.read(4096), b\"\"):\n                    sha256_hash.update(byte_block)\n            hash_str = sha256_hash.hexdigest()\n\n            # Send the hash back to the client\n            ssl_socket.sendall(hash_str.encode('utf-8'))\n\n            return hash_str\n        else:\n            message = 'File not found'\n            ssl_socket.sendall(message.encode('utf-8'))\n            return message\n\n    except Exception as e:\n        error_message = f'Error: {str(e)}'\n        try:\n            ssl_socket.sendall(error_message.encode('utf-8'))\n        except:\n            pass  # If sending fails, there's not much we can do\n        return error_message\n```"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\nimport time\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Runs a non-blocking echo server that appends the server's current time to received data\n    and sends it back to the client. Handles exceptional conditions for each socket.\n\n    Args:\n        server_address (str): The hostname or IP address to bind the server.\n        server_port (int): The port number to bind the server.\n        buffer_size (int): The maximum amount of data to be received at once.\n        run_duration (int): The duration in seconds for which the server should run.\n\n    Returns:\n        str: A status message indicating the server's operation and run duration.\n    \"\"\"\n    # Create a TCP/IP socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(False)\n\n    # Bind the socket to the address and port\n    server_address_full = (server_address, server_port)\n    server.bind(server_address_full)\n\n    # Listen for incoming connections\n    server.listen()\n\n    # Sockets from which we expect to read\n    inputs = [server]\n\n    # Sockets to which we expect to write\n    outputs = []\n\n    # Outgoing message queues (socket:Queue)\n    message_queues = {}\n\n    # Record the start time and calculate end time\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    # Flag to indicate server shutdown\n    running = True\n\n    try:\n        while running:\n            current_time = datetime.now()\n            if current_time >= end_time:\n                running = False\n                continue\n\n            # Calculate the timeout for select\n            timeout = (end_time - current_time).total_seconds()\n            if timeout < 0:\n                timeout = 0\n\n            # Use select to wait for I/O readiness\n            readable, writable, exceptional = select.select(inputs, outputs, inputs, timeout)\n\n            for s in readable:\n                if s is server:\n                    # Handle new connection\n                    try:\n                        connection, client_address = s.accept()\n                        print(f\"Connection from {client_address}\")\n                        connection.setblocking(False)\n                        inputs.append(connection)\n                        message_queues[connection] = queue.Queue()\n                    except socket.error as e:\n                        print(f\"Error accepting connection: {e}\")\n                else:\n                    # Handle data from a client\n                    try:\n                        data = s.recv(buffer_size)\n                    except socket.error as e:\n                        print(f\"Error receiving data from {s.getpeername()}: {e}\")\n                        data = None\n\n                    if data:\n                        # Append server's current time to the received data\n                        current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                        response = data.decode() + f\" | Server Time: {current_time_str}\"\n                        message_queues[s].put(response.encode())\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        # No data means the client has disconnected\n                        print(f\"Closing connection to {s.getpeername()} (no data)\")\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in writable:\n                try:\n                    next_msg = message_queues[s].get_nowait()\n                except queue.Empty:\n                    # No messages waiting so stop checking for writability.\n                    outputs.remove(s)\n                else:\n                    try:\n                        sent = s.send(next_msg)\n                        print(f\"Sent {sent} bytes to {s.getpeername()}\")\n                    except socket.error as e:\n                        print(f\"Error sending data to {s.getpeername()}: {e}\")\n                        outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in exceptional:\n                print(f\"Handling exceptional condition for {s.getpeername()}\")\n                inputs.remove(s)\n                if s in outputs:\n                    outputs.remove(s)\n                s.close()\n                del message_queues[s]\n\n    except Exception as e:\n        print(f\"Server encountered an error: {e}\")\n    finally:\n        # Clean up the server socket\n        server.close()\n        for s in inputs:\n            if s is not server:\n                s.close()\n        for s in outputs:\n            s.close()\n        message_queues.clear()\n\n    duration = (datetime.now() - start_time).total_seconds()\n    status_message = f\"Echo server ran successfully for {duration:.2f} seconds.\"\n    return status_message", "raw_solution": "```python\nimport socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\nimport time\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Runs a non-blocking echo server that appends the server's current time to received data\n    and sends it back to the client. Handles exceptional conditions for each socket.\n\n    Args:\n        server_address (str): The hostname or IP address to bind the server.\n        server_port (int): The port number to bind the server.\n        buffer_size (int): The maximum amount of data to be received at once.\n        run_duration (int): The duration in seconds for which the server should run.\n\n    Returns:\n        str: A status message indicating the server's operation and run duration.\n    \"\"\"\n    # Create a TCP/IP socket\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.setblocking(False)\n\n    # Bind the socket to the address and port\n    server_address_full = (server_address, server_port)\n    server.bind(server_address_full)\n\n    # Listen for incoming connections\n    server.listen()\n\n    # Sockets from which we expect to read\n    inputs = [server]\n\n    # Sockets to which we expect to write\n    outputs = []\n\n    # Outgoing message queues (socket:Queue)\n    message_queues = {}\n\n    # Record the start time and calculate end time\n    start_time = datetime.now()\n    end_time = start_time + timedelta(seconds=run_duration)\n\n    # Flag to indicate server shutdown\n    running = True\n\n    try:\n        while running:\n            current_time = datetime.now()\n            if current_time >= end_time:\n                running = False\n                continue\n\n            # Calculate the timeout for select\n            timeout = (end_time - current_time).total_seconds()\n            if timeout < 0:\n                timeout = 0\n\n            # Use select to wait for I/O readiness\n            readable, writable, exceptional = select.select(inputs, outputs, inputs, timeout)\n\n            for s in readable:\n                if s is server:\n                    # Handle new connection\n                    try:\n                        connection, client_address = s.accept()\n                        print(f\"Connection from {client_address}\")\n                        connection.setblocking(False)\n                        inputs.append(connection)\n                        message_queues[connection] = queue.Queue()\n                    except socket.error as e:\n                        print(f\"Error accepting connection: {e}\")\n                else:\n                    # Handle data from a client\n                    try:\n                        data = s.recv(buffer_size)\n                    except socket.error as e:\n                        print(f\"Error receiving data from {s.getpeername()}: {e}\")\n                        data = None\n\n                    if data:\n                        # Append server's current time to the received data\n                        current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                        response = data.decode() + f\" | Server Time: {current_time_str}\"\n                        message_queues[s].put(response.encode())\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        # No data means the client has disconnected\n                        print(f\"Closing connection to {s.getpeername()} (no data)\")\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in writable:\n                try:\n                    next_msg = message_queues[s].get_nowait()\n                except queue.Empty:\n                    # No messages waiting so stop checking for writability.\n                    outputs.remove(s)\n                else:\n                    try:\n                        sent = s.send(next_msg)\n                        print(f\"Sent {sent} bytes to {s.getpeername()}\")\n                    except socket.error as e:\n                        print(f\"Error sending data to {s.getpeername()}: {e}\")\n                        outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in exceptional:\n                print(f\"Handling exceptional condition for {s.getpeername()}\")\n                inputs.remove(s)\n                if s in outputs:\n                    outputs.remove(s)\n                s.close()\n                del message_queues[s]\n\n    except Exception as e:\n        print(f\"Server encountered an error: {e}\")\n    finally:\n        # Clean up the server socket\n        server.close()\n        for s in inputs:\n            if s is not server:\n                s.close()\n        for s in outputs:\n            s.close()\n        message_queues.clear()\n\n    duration = (datetime.now() - start_time).total_seconds()\n    status_message = f\"Echo server ran successfully for {duration:.2f} seconds.\"\n    return status_message\n\nif __name__ == \"__main__\":\n    status = task_func(run_duration=10)\n    print(status)\n```"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    try:\n        # Receive the message from the client\n        message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n        if not message:\n            print(\"No message received.\")\n            return\n\n        print(f\"Received message: {message}\")\n\n        # Prompt for email details\n        sender_email = input(\"Enter sender's email address: \")\n        recipient_email = input(\"Enter recipient's email address: \")\n        password = getpass.getpass(\"Enter sender's email password: \")\n\n        # Create the email message\n        msg = EmailMessage()\n        msg['Subject'] = 'Message from Client'\n        msg['From'] = sender_email\n        msg['To'] = recipient_email\n        msg.set_content(message)\n\n        # Connect to SMTP server and send the email\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n            server.starttls()\n            server.login(sender_email, password)\n            server.send_message(msg)\n\n        print(\"Email sent successfully.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        client_socket.close()", "raw_solution": "```python\nimport smtplib\nfrom email.message import EmailMessage\nimport getpass\nimport socket\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    try:\n        # Receive the message from the client\n        message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n        if not message:\n            print(\"No message received.\")\n            return\n\n        print(f\"Received message: {message}\")\n\n        # Prompt for email details\n        sender_email = input(\"Enter sender's email address: \")\n        recipient_email = input(\"Enter recipient's email address: \")\n        password = getpass.getpass(\"Enter sender's email password: \")\n\n        # Create the email message\n        msg = EmailMessage()\n        msg['Subject'] = 'Message from Client'\n        msg['From'] = sender_email\n        msg['To'] = recipient_email\n        msg.set_content(message)\n\n        # Connect to SMTP server and send the email\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n            server.starttls()\n            server.login(sender_email, password)\n            server.send_message(msg)\n\n        print(\"Email sent successfully.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        client_socket.close()\n\nif __name__ == \"__main__\":\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket:\n        server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n        server_socket.listen(5)\n        print(f\"Server listening on {SERVER_ADDRESS}:{SERVER_PORT}\")\n        while True:\n            client_conn, client_addr = server_socket.accept()\n            print(f\"Connection from {client_addr}\")\n            task_func(client_conn)\n```"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    Processes a CSV file containing text data and generates a histogram of the ten most common words.\n    \n    Parameters:\n        file_path (str): Path to the CSV file.\n        save_path (str, optional): Path to save the histogram image. If not provided, the plot is displayed.\n        \n    Returns:\n        matplotlib.axes.Axes or None: The Axes object of the plot if save_path is not provided.\n                                      None if the plot is saved to a file.\n                                      \n    Raises:\n        FileNotFoundError: If the specified file_path does not exist.\n        Exception: For any other errors that occur during the function execution.\n    \"\"\"\n    try:\n        # Attempt to read the CSV with header\n        try:\n            df = pd.read_csv(file_path)\n            if 'Text' not in df.columns:\n                raise ValueError(\"CSV file must contain a 'Text' column.\")\n            text_data = df['Text'].dropna().astype(str)\n        except pd.errors.EmptyDataError:\n            raise ValueError(\"CSV file is empty.\")\n        except pd.errors.ParserError:\n            # If no header, assume the first column is text\n            df = pd.read_csv(file_path, header=None)\n            if df.empty:\n                raise ValueError(\"CSV file is empty.\")\n            text_data = df.iloc[:, 0].dropna().astype(str)\n\n        if text_data.empty:\n            raise ValueError(\"No text data found in the CSV file.\")\n\n        # Initialize CountVectorizer with predefined stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_counts = vectorizer.fit_transform(text_data)\n\n        # Sum the counts of each word\n        sum_words = word_counts.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n        # Sort the words by frequency\n        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n        # Get the top 10 most common words\n        top_words = words_freq[:10]\n        words, counts = zip(*top_words)\n\n        # Create the histogram\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.bar(words, counts, color='skyblue')\n        ax.set_title('Top 10 Most Common Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        plt.xticks(rotation=45)\n\n        if save_path:\n            plt.tight_layout()\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            plt.tight_layout()\n            return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at path '{file_path}' was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None", "raw_solution": "```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    Processes a CSV file containing text data and generates a histogram of the ten most common words.\n    \n    Parameters:\n        file_path (str): Path to the CSV file.\n        save_path (str, optional): Path to save the histogram image. If not provided, the plot is displayed.\n        \n    Returns:\n        matplotlib.axes.Axes or None: The Axes object of the plot if save_path is not provided.\n                                      None if the plot is saved to a file.\n                                      \n    Raises:\n        FileNotFoundError: If the specified file_path does not exist.\n        Exception: For any other errors that occur during the function execution.\n    \"\"\"\n    try:\n        # Attempt to read the CSV with header\n        try:\n            df = pd.read_csv(file_path)\n            if 'Text' not in df.columns:\n                raise ValueError(\"CSV file must contain a 'Text' column.\")\n            text_data = df['Text'].dropna().astype(str)\n        except pd.errors.EmptyDataError:\n            raise ValueError(\"CSV file is empty.\")\n        except pd.errors.ParserError:\n            # If no header, assume the first column is text\n            df = pd.read_csv(file_path, header=None)\n            if df.empty:\n                raise ValueError(\"CSV file is empty.\")\n            text_data = df.iloc[:, 0].dropna().astype(str)\n\n        if text_data.empty:\n            raise ValueError(\"No text data found in the CSV file.\")\n\n        # Initialize CountVectorizer with predefined stop words\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        word_counts = vectorizer.fit_transform(text_data)\n\n        # Sum the counts of each word\n        sum_words = word_counts.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n        # Sort the words by frequency\n        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n        # Get the top 10 most common words\n        top_words = words_freq[:10]\n        words, counts = zip(*top_words)\n\n        # Create the histogram\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.bar(words, counts, color='skyblue')\n        ax.set_title('Top 10 Most Common Words')\n        ax.set_xlabel('Words')\n        ax.set_ylabel('Frequency')\n        plt.xticks(rotation=45)\n\n        if save_path:\n            plt.tight_layout()\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            plt.tight_layout()\n            return ax\n\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file at path '{file_path}' was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Define default lists\n    default_animals = ['Cat', 'Dog', 'Elephant', 'Giraffe', 'Lion']\n    default_foods = ['Fish', 'Bone', 'Grass', 'Leaves', 'Meat']\n    \n    # Check if animals and foods are provided and not empty\n    animals_provided = animals is not None and len(animals) > 0\n    foods_provided = foods is not None and len(foods) > 0\n    \n    # Handle special cases\n    if not animals_provided and not foods_provided:\n        return pd.DataFrame()\n    \n    # Use provided lists or default lists\n    animals_list = animals if animals_provided else default_animals\n    foods_list = foods if foods_provided else default_foods\n    \n    # If one of the lists was not provided, use the default for the missing one\n    if not animals_provided:\n        animals_list = default_animals\n    if not foods_provided:\n        foods_list = default_foods\n    \n    # Shuffle the animals and foods lists to ensure variety\n    np.random.seed(None)  # Ensure randomness\n    shuffled_animals = animals_list.copy()\n    shuffled_foods = foods_list.copy()\n    np.random.shuffle(shuffled_animals)\n    np.random.shuffle(shuffled_foods)\n    \n    # Create all possible combinations\n    combinations = list(itertools.product(shuffled_animals, shuffled_foods))\n    \n    # Shuffle the combinations\n    np.random.shuffle(combinations)\n    \n    # Create a pivot table from the shuffled combinations\n    df_dict = {}\n    for animal, food in combinations:\n        if animal not in df_dict:\n            df_dict[animal] = {}\n        df_dict[animal][food] = f\"{animal}:{food}\"\n    \n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(df_dict).T  # Transpose to have animals as rows\n    \n    # Reorder the DataFrame rows and columns based on shuffled lists\n    df = df.reindex(shuffled_animals)\n    df = df[shuffled_foods]\n    \n    return df", "raw_solution": "```python\nimport pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Define default lists\n    default_animals = ['Cat', 'Dog', 'Elephant', 'Giraffe', 'Lion']\n    default_foods = ['Fish', 'Bone', 'Grass', 'Leaves', 'Meat']\n    \n    # Check if animals and foods are provided and not empty\n    animals_provided = animals is not None and len(animals) > 0\n    foods_provided = foods is not None and len(foods) > 0\n    \n    # Handle special cases\n    if not animals_provided and not foods_provided:\n        return pd.DataFrame()\n    \n    # Use provided lists or default lists\n    animals_list = animals if animals_provided else default_animals\n    foods_list = foods if foods_provided else default_foods\n    \n    # If one of the lists was not provided, use the default for the missing one\n    if not animals_provided:\n        animals_list = default_animals\n    if not foods_provided:\n        foods_list = default_foods\n    \n    # Shuffle the animals and foods lists to ensure variety\n    np.random.seed(None)  # Ensure randomness\n    shuffled_animals = animals_list.copy()\n    shuffled_foods = foods_list.copy()\n    np.random.shuffle(shuffled_animals)\n    np.random.shuffle(shuffled_foods)\n    \n    # Create all possible combinations\n    combinations = list(itertools.product(shuffled_animals, shuffled_foods))\n    \n    # Shuffle the combinations\n    np.random.shuffle(combinations)\n    \n    # Create a pivot table from the shuffled combinations\n    df_dict = {}\n    for animal, food in combinations:\n        if animal not in df_dict:\n            df_dict[animal] = {}\n        df_dict[animal][food] = f\"{animal}:{food}\"\n    \n    # Convert the dictionary to a DataFrame\n    df = pd.DataFrame(df_dict).T  # Transpose to have animals as rows\n    \n    # Reorder the DataFrame rows and columns based on shuffled lists\n    df = df.reindex(shuffled_animals)\n    df = df[shuffled_foods]\n    \n    return df\n```"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    \"\"\"\n    Calculates the average time difference in seconds between each consecutive pair of timestamps\n    in a given list after converting them to a specified timezone.\n\n    Parameters:\n        time_strings (list of str): List of timestamp strings.\n        timezone (str): The timezone to convert the timestamps to (e.g., 'America/New_York').\n\n    Returns:\n        float: The mean (average) time difference in seconds between each consecutive pair of timestamps.\n               Returns 0.0 if there are less than two timestamps.\n    \"\"\"\n    if len(time_strings) < 2:\n        return 0.0\n\n    try:\n        target_timezone = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n        raise ValueError(f\"Unknown timezone: {timezone}\")\n\n    converted_times = []\n    for ts in time_strings:\n        try:\n            # Attempt to parse the timestamp with timezone information\n            dt = datetime.fromisoformat(ts)\n            if dt.tzinfo is None:\n                # Assume UTC if no timezone info is present\n                dt = pytz.utc.localize(dt)\n            else:\n                dt = dt.astimezone(pytz.utc)\n        except ValueError:\n            raise ValueError(f\"Invalid timestamp format: {ts}\")\n\n        # Convert to the target timezone\n        dt_converted = dt.astimezone(target_timezone)\n        converted_times.append(dt_converted)\n\n    # Calculate time differences in seconds between consecutive pairs\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = abs((converted_times[i] - converted_times[i - 1]).total_seconds())\n        time_diffs.append(diff)\n\n    if not time_diffs:\n        return 0.0\n\n    # Calculate the average time difference using numpy\n    average_diff = float(np.mean(time_diffs))\n    return average_diff", "raw_solution": "```python\nfrom datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    \"\"\"\n    Calculates the average time difference in seconds between each consecutive pair of timestamps\n    in a given list after converting them to a specified timezone.\n\n    Parameters:\n        time_strings (list of str): List of timestamp strings.\n        timezone (str): The timezone to convert the timestamps to (e.g., 'America/New_York').\n\n    Returns:\n        float: The mean (average) time difference in seconds between each consecutive pair of timestamps.\n               Returns 0.0 if there are less than two timestamps.\n    \"\"\"\n    if len(time_strings) < 2:\n        return 0.0\n\n    try:\n        target_timezone = pytz.timezone(timezone)\n    except pytz.UnknownTimeZoneError:\n        raise ValueError(f\"Unknown timezone: {timezone}\")\n\n    converted_times = []\n    for ts in time_strings:\n        try:\n            # Attempt to parse the timestamp with timezone information\n            dt = datetime.fromisoformat(ts)\n            if dt.tzinfo is None:\n                # Assume UTC if no timezone info is present\n                dt = pytz.utc.localize(dt)\n            else:\n                dt = dt.astimezone(pytz.utc)\n        except ValueError:\n            raise ValueError(f\"Invalid timestamp format: {ts}\")\n\n        # Convert to the target timezone\n        dt_converted = dt.astimezone(target_timezone)\n        converted_times.append(dt_converted)\n\n    # Calculate time differences in seconds between consecutive pairs\n    time_diffs = []\n    for i in range(1, len(converted_times)):\n        diff = abs((converted_times[i] - converted_times[i - 1]).total_seconds())\n        time_diffs.append(diff)\n\n    if not time_diffs:\n        return 0.0\n\n    # Calculate the average time difference using numpy\n    average_diff = float(np.mean(time_diffs))\n    return average_diff\n\nif __name__ == \"__main__\":\n    # Sample list of timestamp strings in ISO 8601 format\n    timestamps = [\n        \"2023-10-01T12:00:00+00:00\",\n        \"2023-10-01T12:05:30+00:00\",\n        \"2023-10-01T12:10:15+00:00\",\n        \"2023-10-01T12:20:45+00:00\"\n    ]\n\n    # Specify the target timezone\n    target_tz = \"America/New_York\"\n\n    # Calculate the average time difference\n    avg_diff = task_func(timestamps, target_tz)\n    print(f\"Average time difference: {avg_diff} seconds\")\n```"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split into words\n    words = text.split()\n    \n    # Count word frequencies\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10 = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_10)\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts, color='skyblue')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    ax.tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return top_10, ax", "raw_solution": "```python\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split into words\n    words = text.split()\n    \n    # Count word frequencies\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10 = word_counts.most_common(10)\n    \n    # Prepare data for plotting\n    words, counts = zip(*top_10)\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts, color='skyblue')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Counts')\n    ax.set_title('Top 10 Most Common Words')\n    ax.tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return top_10, ax\n```"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression pattern to extract URLs\n    url_pattern = re.compile(\n        r'(https?://(?:www\\.|(?!www))[^\\s]+)|(www\\.[^\\s]+)'\n    )\n    \n    # Find all matches in the input string\n    matches = url_pattern.findall(myString)\n    \n    # Extract the first valid URL\n    url = None\n    for match in matches:\n        extracted_url = match[0] if match[0] else f'http://{match[1]}'\n        parsed_url = urlparse(extracted_url)\n        if parsed_url.scheme and parsed_url.netloc:\n            url = extracted_url\n            break\n    \n    if not url:\n        return \"No valid URL found in the provided string.\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    soup = BeautifulSoup(response.text, 'html.parser')\n    title_tag = soup.find('title')\n    \n    if title_tag and title_tag.string:\n        return title_tag.string.strip()\n    else:\n        return \"No title tag found in the webpage.\"", "raw_solution": "```python\nimport re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression pattern to extract URLs\n    url_pattern = re.compile(\n        r'(https?://(?:www\\.|(?!www))[^\\s]+)|(www\\.[^\\s]+)'\n    )\n    \n    # Find all matches in the input string\n    matches = url_pattern.findall(myString)\n    \n    # Extract the first valid URL\n    url = None\n    for match in matches:\n        extracted_url = match[0] if match[0] else f'http://{match[1]}'\n        parsed_url = urlparse(extracted_url)\n        if parsed_url.scheme and parsed_url.netloc:\n            url = extracted_url\n            break\n    \n    if not url:\n        return \"No valid URL found in the provided string.\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n    except requests.RequestException:\n        return f\"Unable to fetch the content of the URL: {url}\"\n    \n    soup = BeautifulSoup(response.text, 'html.parser')\n    title_tag = soup.find('title')\n    \n    if title_tag and title_tag.string:\n        return title_tag.string.strip()\n    else:\n        return \"No title tag found in the webpage.\"\n```"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with the specified key,\n    downloads the file from the URL, and saves it with a timestamped filename.\n\n    Parameters:\n        json_data (str): JSON string containing key-value pairs.\n        unknown_key (str): The key whose associated URL will be used for downloading.\n        save_dir (str, optional): Directory to save the downloaded file. Defaults to current working directory.\n\n    Returns:\n        str: The absolute path of the downloaded file.\n    \"\"\"\n    try:\n        # Parse the JSON data\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Invalid JSON data provided.\") from e\n\n    # Check if the unknown_key exists in the JSON data\n    if unknown_key not in data:\n        raise KeyError(f\"The key '{unknown_key}' was not found in the JSON data.\")\n\n    url = data[unknown_key]\n\n    # Validate that the URL is a string\n    if not isinstance(url, str):\n        raise ValueError(f\"The value for key '{unknown_key}' is not a valid URL string.\")\n\n    try:\n        # Download the file from the URL\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Failed to download the file from URL: {url}\") from e\n\n    # Generate timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n\n    # Create filename\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n\n    # Determine the save directory\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n    else:\n        save_dir = os.getcwd()\n\n    # Full path to save the file\n    file_path = os.path.join(save_dir, filename)\n\n    try:\n        # Save the content to the file\n        with open(file_path, 'w', encoding='utf-8') as file:\n            file.write(response.text)\n    except IOError as e:\n        raise IOError(f\"Failed to write the file to: {file_path}\") from e\n\n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)", "raw_solution": "```python\nimport json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses a JSON string to find a URL associated with the specified key,\n    downloads the file from the URL, and saves it with a timestamped filename.\n\n    Parameters:\n        json_data (str): JSON string containing key-value pairs.\n        unknown_key (str): The key whose associated URL will be used for downloading.\n        save_dir (str, optional): Directory to save the downloaded file. Defaults to current working directory.\n\n    Returns:\n        str: The absolute path of the downloaded file.\n    \"\"\"\n    try:\n        # Parse the JSON data\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Invalid JSON data provided.\") from e\n\n    # Check if the unknown_key exists in the JSON data\n    if unknown_key not in data:\n        raise KeyError(f\"The key '{unknown_key}' was not found in the JSON data.\")\n\n    url = data[unknown_key]\n\n    # Validate that the URL is a string\n    if not isinstance(url, str):\n        raise ValueError(f\"The value for key '{unknown_key}' is not a valid URL string.\")\n\n    try:\n        # Download the file from the URL\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise ConnectionError(f\"Failed to download the file from URL: {url}\") from e\n\n    # Generate timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n\n    # Create filename\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n\n    # Determine the save directory\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n    else:\n        save_dir = os.getcwd()\n\n    # Full path to save the file\n    file_path = os.path.join(save_dir, filename)\n\n    try:\n        # Save the content to the file\n        with open(file_path, 'w', encoding='utf-8') as file:\n            file.write(response.text)\n    except IOError as e:\n        raise IOError(f\"Failed to write the file to: {file_path}\") from e\n\n    # Return the absolute path of the saved file\n    return os.path.abspath(file_path)\n```"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\nimport os\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n\n    Args:\n        url (str): The URL or local file path to extract phone numbers from.\n        output_path (str): The file path to save the extracted phone numbers in JSON format.\n\n    Returns:\n        list: A list of strings, each representing a found phone number.\n    \"\"\"\n    # Function to determine if the input is a URL\n    def is_url(input_str):\n        return input_str.startswith('http://') or input_str.startswith('https://')\n\n    try:\n        if is_url(url):\n            # Fetch content from URL\n            response = requests.get(url)\n            response.raise_for_status()\n            content = response.text\n        else:\n            # Read content from local file\n            if not os.path.isfile(url):\n                raise FileNotFoundError(f\"The file {url} does not exist.\")\n            with open(url, 'r', encoding='utf-8') as file:\n                content = file.read()\n\n        # Parse HTML content\n        soup = bs4.BeautifulSoup(content, 'html.parser')\n        text = soup.get_text()\n\n        # Regular expression pattern for phone numbers\n        phone_pattern = re.compile(\n            r'''(\n                (\\+?\\d{1,3}[\\s-]?)?              # country code\n                (\\(?\\d{3}\\)?[\\s-]?)?             # area code\n                \\d{3}[\\s-]?\\d{4}                  # main number\n            )''',\n            re.VERBOSE\n        )\n\n        # Find all matches\n        matches = phone_pattern.findall(text)\n        phone_numbers = set()\n        for match in matches:\n            phone = match[0]\n            # Clean up phone number\n            phone = re.sub(r'\\s+', ' ', phone)\n            phone_numbers.add(phone.strip())\n\n        phone_list = list(phone_numbers)\n\n        # Save to JSON file\n        with open(output_path, 'w', encoding='utf-8') as outfile:\n            json.dump(phone_list, outfile, indent=4)\n\n        return phone_list\n\n    except requests.RequestException as e:\n        print(f\"Error fetching URL: {e}\")\n        return []\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []", "raw_solution": "```python\nimport bs4\nimport requests\nimport re\nimport json\nimport os\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from a given URL or local file and saves them to a specified file in JSON format.\n\n    Args:\n        url (str): The URL or local file path to extract phone numbers from.\n        output_path (str): The file path to save the extracted phone numbers in JSON format.\n\n    Returns:\n        list: A list of strings, each representing a found phone number.\n    \"\"\"\n    # Function to determine if the input is a URL\n    def is_url(input_str):\n        return input_str.startswith('http://') or input_str.startswith('https://')\n\n    try:\n        if is_url(url):\n            # Fetch content from URL\n            response = requests.get(url)\n            response.raise_for_status()\n            content = response.text\n        else:\n            # Read content from local file\n            if not os.path.isfile(url):\n                raise FileNotFoundError(f\"The file {url} does not exist.\")\n            with open(url, 'r', encoding='utf-8') as file:\n                content = file.read()\n\n        # Parse HTML content\n        soup = bs4.BeautifulSoup(content, 'html.parser')\n        text = soup.get_text()\n\n        # Regular expression pattern for phone numbers\n        phone_pattern = re.compile(\n            r'''(\n                (\\+?\\d{1,3}[\\s-]?)?              # country code\n                (\\(?\\d{3}\\)?[\\s-]?)?             # area code\n                \\d{3}[\\s-]?\\d{4}                  # main number\n            )''',\n            re.VERBOSE\n        )\n\n        # Find all matches\n        matches = phone_pattern.findall(text)\n        phone_numbers = set()\n        for match in matches:\n            phone = match[0]\n            # Clean up phone number\n            phone = re.sub(r'\\s+', ' ', phone)\n            phone_numbers.add(phone.strip())\n\n        phone_list = list(phone_numbers)\n\n        # Save to JSON file\n        with open(output_path, 'w', encoding='utf-8') as outfile:\n            json.dump(phone_list, outfile, indent=4)\n\n        return phone_list\n\n    except requests.RequestException as e:\n        print(f\"Error fetching URL: {e}\")\n        return []\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return []\n```"}
